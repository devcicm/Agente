<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/10893401b2d5e766-s.p.ttf" as="font" crossorigin="" type="font/ttf"/><link rel="preload" href="/_next/static/media/10eb923c8d83aed3-s.p.ttf" as="font" crossorigin="" type="font/ttf"/><link rel="preload" as="image" imageSrcSet="/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flmstudio-app-logo.11b4d746.webp&amp;w=32&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flmstudio-app-logo.11b4d746.webp&amp;w=64&amp;q=75 2x" fetchPriority="high"/><link rel="stylesheet" href="/_next/static/css/5558a0fd21778b44.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/5d898acc654ad377.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/77904bbb8e5c2b67.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-77a60de418d8e871.js"/><script src="/_next/static/chunks/fd9d1056-ea999b7dd9a5dd54.js" async=""></script><script src="/_next/static/chunks/5030-ce9d4d0ffc372ee8.js" async=""></script><script src="/_next/static/chunks/main-app-dd53f9e0929c416a.js" async=""></script><script src="/_next/static/chunks/ee560e2c-b093271a256731e1.js" async=""></script><script src="/_next/static/chunks/8e1d74a4-ddce50761baba1e8.js" async=""></script><script src="/_next/static/chunks/f8025e75-e20309addf7bf5f4.js" async=""></script><script src="/_next/static/chunks/9930-a52251cc1096c3b1.js" async=""></script><script src="/_next/static/chunks/2813-6d096989a4051682.js" async=""></script><script src="/_next/static/chunks/1716-15257501f3402ab6.js" async=""></script><script src="/_next/static/chunks/2972-6f5f7e95fba3c98d.js" async=""></script><script src="/_next/static/chunks/1229-bfdd3210479325c8.js" async=""></script><script src="/_next/static/chunks/5878-ba606658793f4e54.js" async=""></script><script src="/_next/static/chunks/602-f8d92996d4060178.js" async=""></script><script src="/_next/static/chunks/6890-680ccec6f8ba7871.js" async=""></script><script src="/_next/static/chunks/7772-31f69a9dcf45ceee.js" async=""></script><script src="/_next/static/chunks/1597-04f9481876dbbe2a.js" async=""></script><script src="/_next/static/chunks/2130-a91b687142678a30.js" async=""></script><script src="/_next/static/chunks/9962-4fbb58d4dc9d3e27.js" async=""></script><script src="/_next/static/chunks/app/(static)/docs/%5B...slug%5D/page-750ba986e1beb3f3.js" async=""></script><script src="/_next/static/chunks/0e762574-26ab547f20f731a1.js" async=""></script><script src="/_next/static/chunks/9c4e2130-5b8fdd2a06a75632.js" async=""></script><script src="/_next/static/chunks/eec3d76d-692d954fdc526554.js" async=""></script><script src="/_next/static/chunks/3d47b92a-50c49e0880f014d1.js" async=""></script><script src="/_next/static/chunks/94730671-6f37d42c2422b345.js" async=""></script><script src="/_next/static/chunks/578c2090-ecc970c206c04bd8.js" async=""></script><script src="/_next/static/chunks/1687-bad5337c161b89bf.js" async=""></script><script src="/_next/static/chunks/1762-6246519a44fd2148.js" async=""></script><script src="/_next/static/chunks/1346-7cdf57d39b4f88ef.js" async=""></script><script src="/_next/static/chunks/2183-5f3f4936ef3f46d1.js" async=""></script><script src="/_next/static/chunks/6234-46ca85d7201402af.js" async=""></script><script src="/_next/static/chunks/app/(static)/docs/layout-b7a00b0618400251.js" async=""></script><script src="/_next/static/chunks/8782-7f5165b200ddbee7.js" async=""></script><script src="/_next/static/chunks/1526-c6ca2b6e8924795d.js" async=""></script><script src="/_next/static/chunks/8801-cdec8854fe39743a.js" async=""></script><script src="/_next/static/chunks/app/(dynamic)/layout-4a5b0d5600a00d5a.js" async=""></script><script src="/_next/static/chunks/app/global-error-bc0866584b5d17be.js" async=""></script><script src="/_next/static/chunks/app/(static)/docs/%5B...slug%5D/layout-147aeca92859f018.js" async=""></script><script src="/_next/static/chunks/3809-cb26ddec5be1b9d8.js" async=""></script><script src="/_next/static/chunks/4662-5275219a57c51235.js" async=""></script><script src="/_next/static/chunks/9133-789d0b90d9d9c0ea.js" async=""></script><script src="/_next/static/chunks/app/layout-dba5af340b8e8926.js" async=""></script><script src="/_next/static/chunks/6097-a44cd9b7466c245f.js" async=""></script><script src="/_next/static/chunks/app/not-found-d596f4f02073e882.js" async=""></script><link rel="preload" href="https://plausible.io/js/script.file-downloads.outbound-links.tagged-events.js" as="script"/><title>Welcome to LM Studio Docs! | LM Studio Docs</title><meta name="description" content="Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM Studio."/><meta name="keywords" content="local ai,local llm,gpt-oss,on-device ai,run local ai,LM Studio,Llama,Gemma,Qwen,DeepSeek,llama.cpp,mlx"/><meta name="robots" content="index, follow"/><link rel="canonical" href="https://lmstudio.ai/docs/app"/><meta property="og:title" content="Welcome to LM Studio Docs! | LM Studio Docs"/><meta property="og:description" content="Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM Studio."/><meta property="og:url" content="https://lmstudio.ai/docs/app"/><meta property="og:site_name" content="LM Studio - Docs"/><meta property="og:image" content="https://lmstudio.ai/api/og?title=Welcome%20to%20LM%20Studio%20Docs!&amp;from=docs/app&amp;description=Learn%20how%20to%20run%20Llama,%20DeepSeek,%20Qwen,%20Phi,%20and%20other%20LLMs%20locally%20with%20LM%20Studio."/><meta property="og:image:type" content="image/png"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image:alt" content="LM Studio: Welcome to LM Studio Docs!"/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:creator" content="@lmstudio"/><meta name="twitter:title" content="Welcome to LM Studio Docs! | LM Studio Docs"/><meta name="twitter:description" content="Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM Studio."/><meta name="twitter:image" content="https://lmstudio.ai/api/og?title=Welcome%20to%20LM%20Studio%20Docs!&amp;description=Learn%20how%20to%20run%20Llama,%20DeepSeek,%20Qwen,%20Phi,%20and%20other%20LLMs%20locally%20with%20LM%20Studio.&amp;from=docs/app"/><link rel="icon" href="/_next/static/media/android-chrome-192x192.3a60873f.png"/><meta name="next-size-adjust"/><link rel="preconnect" href="https://plausible.io"/><link rel="preconnect" href="https://static.cloudflareinsights.com"/><script id="ld-org" type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","@id":"https://lmstudio.ai/#organization","name":"LM Studio","url":"https://lmstudio.ai","logo":{"@type":"ImageObject","url":"https://lmstudio.ai/assets/android-chrome-192x192.png","width":192,"height":192},"sameAs":["https://twitter.com/lmstudio","https://github.com/lmstudio-ai"],"foundingDate":"2023","contactPoint":[{"@type":"ContactPoint","contactType":"customer support","email":"support@lmstudio.ai","url":"https://lmstudio.ai/support","availableLanguage":["en"]}]}</script><script id="ld-website" type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","@id":"https://lmstudio.ai/#website","url":"https://lmstudio.ai","name":"LM Studio","publisher":{"@id":"https://lmstudio.ai/#organization"},"potentialAction":{"@type":"SearchAction","target":"https://lmstudio.ai/search?keyword={search_term_string}","query-input":"required name=search_term_string"}}</script><script id="ld-software" type="application/ld+json">{"@context":"https://schema.org","@type":"SoftwareApplication","@id":"https://lmstudio.ai/#software","name":"LM Studio","applicationCategory":"DeveloperApplication","operatingSystem":"macOS, Windows, Linux","downloadUrl":"https://lmstudio.ai/download","publisher":{"@id":"https://lmstudio.ai/#organization"},"offers":{"@type":"Offer","price":"0","priceCurrency":"USD"}}</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class=""><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><section aria-label="Notifications alt+T" tabindex="-1" aria-live="polite" aria-relevant="additions text" aria-atomic="false"></section><div class="flex min-h-screen w-full flex-col"><div dir="ltr" class="relative overflow-x-hidden overflow-y-hidden flex w-full flex-grow flex-col max-h-screen" style="position:relative;--radix-scroll-area-corner-width:0px;--radix-scroll-area-corner-height:0px"><style>[data-radix-scroll-area-viewport]{scrollbar-width:none;-ms-overflow-style:none;-webkit-overflow-scrolling:touch;}[data-radix-scroll-area-viewport]::-webkit-scrollbar{display:none}</style><div data-radix-scroll-area-viewport="" class="h-full rounded-[inherit] flex w-full flex-grow flex-col [&amp;&gt;div]:!flex [&amp;&gt;div]:!flex-grow [&amp;&gt;div]:!flex-col" style="overflow-x:hidden" tabindex="-1" id="root-scroll-area"><div style="min-width:100%;display:table"><div class="relative flex-none" style="height:65px"><div class="w-full fixed z-[48]"><nav class="flex w-full flex-row items-center justify-between border-b border-border-subtle px-3 fixed z-[48] text-navbar-foreground dark:drop-shadow flex-none bg-navbar pt-0.5" style="height:65px" id="fixed-header-1"><div class="flex min-w-0 flex-1 items-center justify-start gap-3"><div class="flex flex-row items-center justify-center gap-1 p-0"><a class="whitespace-nowrap rounded p-2 px-2 opacity-100 transition-all duration-[400ms] ease-in-out bg-blue-400/0 py-5" href="/"><div class="z-0 flex w-fit flex-row items-center justify-center gap-2"><img alt="LM Studio" fetchPriority="high" width="30" height="30" decoding="async" data-nimg="1" class="h-[30px] w-[30px]" style="color:transparent" srcSet="/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flmstudio-app-logo.11b4d746.webp&amp;w=32&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flmstudio-app-logo.11b4d746.webp&amp;w=64&amp;q=75 2x" src="/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flmstudio-app-logo.11b4d746.webp&amp;w=64&amp;q=75"/><span class="w-full whitespace-nowrap text-lg font-semibold">LM Studio</span></div></a></div><div class="hidden items-center gap-4 pl-2 md:flex"><span><a class="whitespace-nowrap rounded p-2 px-2 opacity-100 transition-all duration-[400ms] ease-in-out" href="/models">Models</a></span><span><a class="whitespace-nowrap rounded p-2 px-2 opacity-100 transition-all duration-[400ms] ease-in-out" href="/docs/developer">Docs</a></span><span class="hidden xl:block"><a class="whitespace-nowrap rounded p-2 px-2 opacity-100 transition-all duration-[400ms] ease-in-out" href="/work">Enterprise</a></span></div></div><ul class="hidden min-w-0 flex-1 flex-row items-center justify-end gap-1 text-sm font-medium md:flex md:gap-3"><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out border-accent-border bg-accent text-accent-foreground hover:bg-accent/80 hover:text-accent-foreground mx-1 rounded-md px-3 py-2" href="/download"><li class="flex flex-row items-center gap-1.5"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"></path><polyline points="7 10 12 15 17 10"></polyline><line x1="12" y1="15" x2="12" y2="3"></line></svg>Download</li></a><div class="flex items-center gap-3"><div class="my-auto h-[24px] w-[1px] bg-foreground/50"></div><a target="_self" rel="noopener noreferrer" class="opacity-100 text-foreground min-w-[52px] p-2 no-underline hover:opacity-80" title="" href="/login">Login</a></div></ul><button class="relative flex h-12 w-12 flex-col items-center justify-center p-2 md:hidden" aria-label="Toggle navigation menu"><span class="absolute block h-0.5 w-[50%] bg-navbar-foreground transition-all duration-300 ease-out translate-y-[-6px]"></span><span class="absolute block h-0.5 w-[50%] bg-navbar-foreground transition-all duration-300 ease-out opacity-100"></span><span class="absolute block h-0.5 w-[50%] bg-navbar-foreground transition-all duration-300 ease-out translate-y-[6px]"></span></button><div class="fixed left-0 z-50 w-full overflow-y-auto bg-navbar p-8 py-4 text-navbar-foreground pointer-events-none max-h-0 opacity-0" style="height:0px;top:65px"><div class="flex flex-col justify-start bg-navbar-background font-medium"><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out flex w-full items-center bg-navbar-background justify-start rounded-none py-2 gap-2 px-2" href="/home"><p class="">Home</p></a><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out flex w-full items-center bg-navbar-background justify-start rounded-none py-2 gap-2 px-2" href="/models"><p class="">Models</p></a><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out flex w-full items-center bg-navbar-background justify-start rounded-none py-2 gap-2 px-2" href="/work"><p class="">Use at Work</p></a><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out flex w-full items-center bg-navbar-background justify-start rounded-none py-2 gap-2 px-2" href="/docs"><p class="">Docs</p></a><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out flex w-full items-center bg-navbar-background justify-start rounded-none py-2 gap-2 px-2" href="/work"><p class="">Enterprise</p></a><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out flex w-full items-center bg-navbar-background justify-start rounded-none py-2 gap-2 px-2" href="/download"><p class="">Download</p></a><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out flex w-full items-center bg-navbar-background justify-start rounded-none py-2 gap-2 px-2" href="/login"><p class="">Login</p></a></div><hr class="mx-auto my-4 max-w-[400px]"/><div class="flex flex-col justify-start bg-navbar-background font-normal text-foreground/60"><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out w-full bg-navbar-background justify-start rounded-none py-2 px-2 flex flex-row items-center gap-1" href="/careers"><p class="">Careers</p><span class="ml-2 rounded-md bg-accent/10 px-2 py-1 text-xs font-medium text-accent dark:bg-lm-purple/10 dark:text-lm-purple">We&#x27;re Hiring!</span></a><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out flex w-full items-center bg-navbar-background justify-start rounded-none py-2 gap-2 px-2" href="/app-privacy"><p class="">Privacy Policy</p></a><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out flex w-full items-center bg-navbar-background justify-start rounded-none py-2 gap-2 px-2" href="/app-terms"><p class="">Terms of Use</p></a><div class="flex items-center justify-start gap-0 pt-8"><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out flex items-center bg-navbar-background justify-start rounded-none py-2 gap-2 px-2 w-fit" href="https://discord.gg/lmstudio"><button class="border inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 transition-all duration-[250ms] ease-in-out border-input bg-background hover:bg-accent hover:text-accent-foreground rounded-md px-3 h-8 w-8 !p-0"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></button></a><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out flex items-center bg-navbar-background justify-start rounded-none py-2 gap-2 px-2 w-fit" href="https://github.com/lmstudio-ai"><button class="border inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 transition-all duration-[250ms] ease-in-out border-input bg-background hover:bg-accent hover:text-accent-foreground rounded-md px-3 h-8 w-8 !p-0"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></button></a><a class="whitespace-nowrap p-2 opacity-100 transition-all duration-[400ms] ease-in-out flex items-center bg-navbar-background justify-start rounded-none py-2 gap-2 px-2 w-fit" href="https://twitter.com/lmstudio"><button class="border inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 transition-all duration-[250ms] ease-in-out border-input bg-background hover:bg-accent hover:text-accent-foreground rounded-md px-3 h-8 w-8 !p-0"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></button></a></div></div></div></nav></div></div><div class="flex min-h-0 min-h-screen w-full flex-auto flex-grow flex-col"><div class="__variable_2f8338 __variable_06d220 docs-typography"><div id="fixed-header-2" class="fixed z-40 hidden w-full flex-grow items-center gap-2 border-b border-border-subtle bg-navbar-secondary lg:flex lg:h-[44px]"><div class="max-w-full mx-auto hidden flex-grow items-center justify-between gap-4 lg:flex"><div class="flex flex-none items-center justify-start gap-6 px-5"><a target="_self" rel="noopener noreferrer" class="no-underline flex items-center justify-start gap-2 text-left text-sm font-medium opacity-100 hover:text-foreground w-fit hover:bg-transparent text-foreground/100 border border-border/70 relative py-2 px-2 top-[2px] rounded-none border-x-0 dark:bg-transparent border-b-[3px] border-t-0 border-t-transparent border-b-lm-blue bg-transparent" title="" href="/docs/app">App</a><a target="_self" rel="noopener noreferrer" class="no-underline flex items-center justify-start gap-2 text-left text-sm font-medium opacity-100 text-foreground/70 hover:text-foreground border relative top-[2px] w-fit px-2 py-2 hover:bg-transparent rounded-none border-x-0 border-b-[3px] border-t-0 border-transparent bg-transparent dark:bg-transparent" title="" href="/docs/developer">Developer Docs</a><a target="_self" rel="noopener noreferrer" class="no-underline flex items-center justify-start gap-2 text-left text-sm font-medium opacity-100 text-foreground/70 hover:text-foreground border relative top-[2px] w-fit px-2 py-2 hover:bg-transparent rounded-none border-x-0 border-b-[3px] border-t-0 border-transparent bg-transparent dark:bg-transparent" title="" href="/docs/typescript">lmstudio-js</a><a target="_self" rel="noopener noreferrer" class="no-underline flex items-center justify-start gap-2 text-left text-sm font-medium opacity-100 text-foreground/70 hover:text-foreground border relative top-[2px] w-fit px-2 py-2 hover:bg-transparent rounded-none border-x-0 border-b-[3px] border-t-0 border-transparent bg-transparent dark:bg-transparent" title="" href="/docs/python">lmstudio-python</a><a target="_self" rel="noopener noreferrer" class="no-underline flex items-center justify-start gap-2 text-left text-sm font-medium opacity-100 text-foreground/70 hover:text-foreground border relative top-[2px] w-fit px-2 py-2 hover:bg-transparent rounded-none border-x-0 border-b-[3px] border-t-0 border-transparent bg-transparent dark:bg-transparent" title="" href="/docs/cli">CLI</a></div></div></div><div class="hidden w-full flex-none lg:flex" style="height:43px"></div><div class="relative flex h-full flex-grow bg-documentation docs-no-line-numbers"><div class="flex h-full w-full items-center justify-center"><div class="flex h-full w-full flex-row items-start justify-start gap-0 overflow-hidden flex-wrap lg:flex-nowrap max-w-full"><div id="fixed-header-2" class="fixed z-40 flex w-full flex-grow items-center gap-2 border-b border-border-subtle bg-navbar-secondary px-3 lg:hidden" style="height:65px"><div class="flex flex-grow items-center gap-2 lg:hidden"><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 transition-all duration-[250ms] ease-in-out text-primary-foreground hover:bg-secondary/50 border pointer-events-auto z-40 h-10 w-10 flex-none bg-background p-2 text-xl"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="h-full w-full" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M4 4m0 2a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v12a2 2 0 0 1 -2 2h-12a2 2 0 0 1 -2 -2z"></path><path d="M15 4v16"></path><path d="M9 10l2 2l-2 2"></path></svg></button><p class="text-base font-normal leading-8 title">Documentation</p><div class="flex-grow"></div><button class="border inline-flex items-center justify-center whitespace-nowrap rounded-md font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 transition-all duration-[250ms] ease-in-out text-foreground/70 hover:text-foreground/80 border-none h-10 w-10 text-xl"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line></svg></button></div></div><div class="w-full flex-none lg:hidden" style="height:65px"></div><div class="flex h-full flex-col lg:w-[320px] lg:max-w-[320px] z-20 flex-none bg-documentation lg:fixed"><div class="flex bg-documentation py-5 pb-2 lg:hidden"><div class="bg-background w-[350px] max-w-[calc(100vw-70px)] fixed left-0 z-50 p-4 pr-1 transform transition-transform duration-300 -translate-x-full lg:hidden" style="height:calc(100vh - 130px);top:130px"><div class="h-full w-full overflow-y-scroll pr-3 pb-10 md:pb-0 overscroll-y-contain"><div class="flex h-full flex-col gap-4"><div class="flex flex-col gap-1 py-3"><a target="_self" rel="noopener noreferrer" class="no-underline flex w-full items-center justify-start gap-2 rounded-md px-3 py-1 text-left text-sm font-medium opacity-100 hover:text-foreground hover:bg-foreground/10 text-foreground/100 border border-border/70 dark:bg-foreground/10 bg-foreground/5" title="" href="/docs/app"><div class="rounded-md border p-0.5"><img alt="lmstudio icon" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="text-lm-blue" style="color:transparent" src="/_next/static/media/mcp-install.b2052392.svg"/></div>App</a><a target="_self" rel="noopener noreferrer" class="no-underline flex w-full items-center justify-start gap-2 rounded-md px-3 py-1 text-left text-sm font-medium opacity-100 text-foreground/70 hover:text-foreground hover:bg-foreground/10 border border-transparent" title="" href="/docs/developer"><div class="rounded-md border p-0.5"><img alt="developer logo" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="rounded-sm" style="color:transparent" srcSet="/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fdev-logo.7a6f42a1.jpg&amp;w=32&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fdev-logo.7a6f42a1.jpg&amp;w=48&amp;q=75 2x" src="/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fdev-logo.7a6f42a1.jpg&amp;w=48&amp;q=75"/></div>Developer Docs</a><a target="_self" rel="noopener noreferrer" class="no-underline flex w-full items-center justify-start gap-2 rounded-md px-3 py-1 text-left text-sm font-medium opacity-100 text-foreground/70 hover:text-foreground hover:bg-foreground/10 border border-transparent" title="" href="/docs/typescript"><div class="rounded-md border p-0.5"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="text-lm-blue" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M19.24 3H4.76A1.76 1.76 0 0 0 3 4.76v14.48A1.76 1.76 0 0 0 4.76 21h14.48A1.76 1.76 0 0 0 21 19.24V4.76A1.76 1.76 0 0 0 19.24 3zm-5.8 10h-2.25v6.44H9.4V13H7.15v-1.46h6.29zm5.8 5.28a1.71 1.71 0 0 1-.67.74 3 3 0 0 1-1 .39 5.81 5.81 0 0 1-1.2.12 7 7 0 0 1-1.23-.11 4.52 4.52 0 0 1-1-.33v-1.71l-.06-.06h.06v.07a3.41 3.41 0 0 0 1 .54 3.06 3.06 0 0 0 1.13.2 2.58 2.58 0 0 0 .6-.06 1.47 1.47 0 0 0 .42-.17.75.75 0 0 0 .25-.25.69.69 0 0 0-.06-.74 1.24 1.24 0 0 0-.35-.33 3.12 3.12 0 0 0-.53-.3l-.67-.28a3.57 3.57 0 0 1-1.37-1 2 2 0 0 1-.46-1.33 2.16 2.16 0 0 1 .24-1.06 2.09 2.09 0 0 1 .66-.71 2.88 2.88 0 0 1 1-.42 5.11 5.11 0 0 1 1.19-.13 7 7 0 0 1 1.09.07 4.53 4.53 0 0 1 .88.23v1.65a2.42 2.42 0 0 0-.42-.24 3.58 3.58 0 0 0-.49-.17 3 3 0 0 0-.49-.1 2.45 2.45 0 0 0-.46 0 2.29 2.29 0 0 0-.56.06 1.54 1.54 0 0 0-.43.16.78.78 0 0 0-.26.25.63.63 0 0 0-.09.33.62.62 0 0 0 .1.35 1.19 1.19 0 0 0 .3.29 2.15 2.15 0 0 0 .46.28l.63.28a6.56 6.56 0 0 1 .84.42 2.65 2.65 0 0 1 .64.49 1.79 1.79 0 0 1 .42.63 2.48 2.48 0 0 1 .14.85 2.68 2.68 0 0 1-.25 1.08z"></path></svg></div>lmstudio-js</a><a target="_self" rel="noopener noreferrer" class="no-underline flex w-full items-center justify-start gap-2 rounded-md px-3 py-1 text-left text-sm font-medium opacity-100 text-foreground/70 hover:text-foreground hover:bg-foreground/10 border border-transparent" title="" href="/docs/python"><div class="rounded-md border p-0.5"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="text-lm-yellow" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z"></path></svg></div>lmstudio-python</a><a target="_self" rel="noopener noreferrer" class="no-underline flex w-full items-center justify-start gap-2 rounded-md px-3 py-1 text-left text-sm font-medium opacity-100 text-foreground/70 hover:text-foreground hover:bg-foreground/10 border border-transparent" title="" href="/docs/cli"><div class="rounded-md border p-0.5"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-lm-green" height="20" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M5 7l5 5l-5 5"></path><path d="M12 19l7 0"></path></svg></div>CLI</a></div><div class="max-h-full overflow-y-auto"><div class="pb-12"><div><div class="mt-0 p-0 last:mb-20 pl-1 pt-4 first:pt-0"><div class="flex flex-col gap-0"><div><a id="app" class="rounded p-1.5 px-2 flex text-[15px] hover:border-lm-purple/80 bg-accent text-foreground hover:bg-accent hover:text-accent-foreground transition-all duration-300 ease-in-out" href="/docs/app"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body [&amp;_p]:text-[15px] foreground text-accent-foreground hover:text-accent-foreground [&amp;&gt;code]:!text-accent-foreground"><p>Welcome</p></div></div></a></div><div><a id="app/system-requirements" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/system-requirements"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body [&amp;_p]:text-[15px]"><p>System Requirements</p></div></div></a></div><div><a id="app/offline" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/offline"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body [&amp;_p]:text-[15px]"><p>Offline Operation</p></div></div></a></div></div></div><div class="mt-0 p-0 pl-1 pt-4 first:pt-0"><div class="w-full items-center text-left gap-1 justify-start rounded p-1.5 px-2 flex hover:text-foreground hover:border-lm-purple/80 hover:bg-transparent text-foreground pr-2 text-xs font-bold uppercase opacity-100"><p class="">Getting Started</p></div><div class="flex flex-col gap-0"><div><a id="app/basics" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/basics"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Overview</p></div></div></a></div><div><a id="app/basics/chat" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/basics/chat"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Manage chats</p></div></div></a></div><div><a id="app/basics/download-model" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/basics/download-model"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Download an LLM</p></div></div></a></div><div><a id="app/basics/rag" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/basics/rag"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Chat with Documents</p></div></div></a></div></div></div><div class="mt-0 p-0 pl-1 pt-4 first:pt-0"><div class="w-full items-center text-left gap-1 justify-start rounded p-1.5 px-2 flex hover:text-foreground hover:border-lm-purple/80 hover:bg-transparent text-foreground pr-2 text-xs font-bold uppercase opacity-100"><p class="">Model Context Protocol (MCP)</p></div><div class="flex flex-col gap-0"><div><a id="app/mcp" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/mcp"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Use MCP Servers</p></div></div></a></div><div><a id="app/mcp/deeplink" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/mcp/deeplink"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p><code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">Add to LM Studio</code> Button</p></div></div></a></div></div></div><div class="mt-0 p-0 pl-1 pt-4 first:pt-0"><div class="w-full items-center text-left gap-1 justify-start rounded p-1.5 px-2 flex hover:text-foreground hover:border-lm-purple/80 hover:bg-transparent text-foreground pr-2 text-xs font-bold uppercase opacity-100"><p class="">Models (model.yaml)</p></div><div class="flex flex-col gap-0"><div><a id="app/modelyaml" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/modelyaml"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Introduction to <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">model.yaml</code></p></div></div></a></div><div><a id="app/modelyaml/publish" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/modelyaml/publish"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Publish a <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">model.yaml</code></p></div></div></a></div></div></div><div class="mt-0 p-0 pl-1 pt-4 first:pt-0"><div class="w-full items-center text-left gap-1 justify-start rounded p-1.5 px-2 flex hover:text-foreground hover:border-lm-purple/80 hover:bg-transparent text-foreground pr-2 text-xs font-bold uppercase opacity-100"><p class="">Presets</p></div><div class="flex flex-col gap-0"><div><a id="app/presets" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/presets"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Overview</p></div></div></a></div><div><a id="app/presets/import" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/presets/import"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Importing and Sharing</p></div></div></a></div><div><a id="app/presets/publish" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/presets/publish"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Publish a Preset</p></div></div></a></div><div><a id="app/presets/pull" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/presets/pull"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Pull Updates</p></div></div></a></div><div><a id="app/presets/push" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/presets/push"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Push New Revisions</p></div></div></a></div></div></div><div class="mt-0 p-0 pl-1 pt-4 first:pt-0"><div class="w-full items-center text-left gap-1 justify-start rounded p-1.5 px-2 flex hover:text-foreground hover:border-lm-purple/80 hover:bg-transparent text-foreground pr-2 text-xs font-bold uppercase opacity-100"><p class="">Advanced</p></div><div class="flex flex-col gap-0"><div><a id="app/advanced/speculative-decoding" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/advanced/speculative-decoding"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Speculative Decoding</p></div></div></a></div><div><a id="app/advanced/import-model" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/advanced/import-model"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Import Models</p></div></div></a></div><div><a id="app/advanced/per-model" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/advanced/per-model"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Per-model Defaults</p></div></div></a></div><div><a id="app/advanced/prompt-template" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/advanced/prompt-template"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Prompt Template</p></div></div></a></div></div></div><div class="mt-0 p-0 pl-1 pt-4 first:pt-0"><div class="w-full items-center text-left gap-1 justify-start rounded p-1.5 px-2 flex hover:text-foreground hover:border-lm-purple/80 hover:bg-transparent text-foreground pr-2 text-xs font-bold uppercase opacity-100"><p class="">User Interface</p></div><div class="flex flex-col gap-0"><div><a id="app/user-interface/languages" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/user-interface/languages"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Languages</p></div></div></a></div><div><a id="app/user-interface/modes" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/user-interface/modes"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>UI Modes</p></div></div></a></div><div><a id="app/user-interface/themes" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/user-interface/themes"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Color Themes</p></div></div></a></div></div></div></div></div></div></div></div></div><div class="fixed left-0 z-40 w-full bg-[rgba(0,0,0,0.2)] transition-opacity duration-300 hidden opacity-0 lg:hidden" style="height:calc(100vh - 130px);top:130px"></div></div><div class="relative hidden h-full flex-grow-0 py-2 pt-5 lg:flex bg-navbar-secondary border-r border-border-subtle" style="max-height:calc(100vh - 108px)"><div class="flex flex-grow flex-col px-4 pt-0"><div class="mb-3 hidden px-0.5 lg:block"><button class="inline-flex whitespace-nowrap text-sm font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 transition-all duration-[250ms] ease-in-out hover:bg-secondary/50 hover:text-secondary-foreground rounded-md px-3 h-10 w-full min-w-0 items-center justify-start gap-2 border border-border/70 opacity-70 hover:opacity-100 cursor-default"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="opacity-60" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line></svg><span class="flex-1 text-left opacity-60">Search Docs</span><span class="flex items-center gap-0.5"><span class="flex h-6 w-6 items-center justify-center rounded border border-border-subtle bg-background p-0.5 px-1 text-center text-xs font-semibold text-foreground">âŒ˜</span><span class="flex h-6 w-6 items-center justify-center rounded border border-border-subtle bg-background p-0.5 px-1 text-center text-xs font-semibold text-foreground">K</span></span></button></div><div class="flex h-full flex-col gap-4"><div class="max-h-full overflow-y-auto"><div class="pb-12"><div><div class="mt-0 p-0 last:mb-20 pl-1 pt-4 first:pt-0"><div class="flex flex-col gap-0"><div><a id="app" class="rounded p-1.5 px-2 flex text-[15px] hover:border-lm-purple/80 bg-accent text-foreground hover:bg-accent hover:text-accent-foreground transition-all duration-300 ease-in-out" href="/docs/app"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body [&amp;_p]:text-[15px] foreground text-accent-foreground hover:text-accent-foreground [&amp;&gt;code]:!text-accent-foreground"><p>Welcome</p></div></div></a></div><div><a id="app/system-requirements" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/system-requirements"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body [&amp;_p]:text-[15px]"><p>System Requirements</p></div></div></a></div><div><a id="app/offline" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/offline"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body [&amp;_p]:text-[15px]"><p>Offline Operation</p></div></div></a></div></div></div><div class="mt-0 p-0 pl-1 pt-4 first:pt-0"><div class="w-full items-center text-left gap-1 justify-start rounded p-1.5 px-2 flex hover:text-foreground hover:border-lm-purple/80 hover:bg-transparent text-foreground pr-2 text-xs font-bold uppercase opacity-100"><p class="">Getting Started</p></div><div class="flex flex-col gap-0"><div><a id="app/basics" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/basics"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Overview</p></div></div></a></div><div><a id="app/basics/chat" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/basics/chat"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Manage chats</p></div></div></a></div><div><a id="app/basics/download-model" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/basics/download-model"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Download an LLM</p></div></div></a></div><div><a id="app/basics/rag" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/basics/rag"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Chat with Documents</p></div></div></a></div></div></div><div class="mt-0 p-0 pl-1 pt-4 first:pt-0"><div class="w-full items-center text-left gap-1 justify-start rounded p-1.5 px-2 flex hover:text-foreground hover:border-lm-purple/80 hover:bg-transparent text-foreground pr-2 text-xs font-bold uppercase opacity-100"><p class="">Model Context Protocol (MCP)</p></div><div class="flex flex-col gap-0"><div><a id="app/mcp" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/mcp"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Use MCP Servers</p></div></div></a></div><div><a id="app/mcp/deeplink" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/mcp/deeplink"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p><code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">Add to LM Studio</code> Button</p></div></div></a></div></div></div><div class="mt-0 p-0 pl-1 pt-4 first:pt-0"><div class="w-full items-center text-left gap-1 justify-start rounded p-1.5 px-2 flex hover:text-foreground hover:border-lm-purple/80 hover:bg-transparent text-foreground pr-2 text-xs font-bold uppercase opacity-100"><p class="">Models (model.yaml)</p></div><div class="flex flex-col gap-0"><div><a id="app/modelyaml" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/modelyaml"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Introduction to <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">model.yaml</code></p></div></div></a></div><div><a id="app/modelyaml/publish" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/modelyaml/publish"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Publish a <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">model.yaml</code></p></div></div></a></div></div></div><div class="mt-0 p-0 pl-1 pt-4 first:pt-0"><div class="w-full items-center text-left gap-1 justify-start rounded p-1.5 px-2 flex hover:text-foreground hover:border-lm-purple/80 hover:bg-transparent text-foreground pr-2 text-xs font-bold uppercase opacity-100"><p class="">Presets</p></div><div class="flex flex-col gap-0"><div><a id="app/presets" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/presets"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Overview</p></div></div></a></div><div><a id="app/presets/import" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/presets/import"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Importing and Sharing</p></div></div></a></div><div><a id="app/presets/publish" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/presets/publish"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Publish a Preset</p></div></div></a></div><div><a id="app/presets/pull" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/presets/pull"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Pull Updates</p></div></div></a></div><div><a id="app/presets/push" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/presets/push"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Push New Revisions</p></div></div></a></div></div></div><div class="mt-0 p-0 pl-1 pt-4 first:pt-0"><div class="w-full items-center text-left gap-1 justify-start rounded p-1.5 px-2 flex hover:text-foreground hover:border-lm-purple/80 hover:bg-transparent text-foreground pr-2 text-xs font-bold uppercase opacity-100"><p class="">Advanced</p></div><div class="flex flex-col gap-0"><div><a id="app/advanced/speculative-decoding" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/advanced/speculative-decoding"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Speculative Decoding</p></div></div></a></div><div><a id="app/advanced/import-model" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/advanced/import-model"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Import Models</p></div></div></a></div><div><a id="app/advanced/per-model" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/advanced/per-model"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Per-model Defaults</p></div></div></a></div><div><a id="app/advanced/prompt-template" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/advanced/prompt-template"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Prompt Template</p></div></div></a></div></div></div><div class="mt-0 p-0 pl-1 pt-4 first:pt-0"><div class="w-full items-center text-left gap-1 justify-start rounded p-1.5 px-2 flex hover:text-foreground hover:border-lm-purple/80 hover:bg-transparent text-foreground pr-2 text-xs font-bold uppercase opacity-100"><p class="">User Interface</p></div><div class="flex flex-col gap-0"><div><a id="app/user-interface/languages" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/user-interface/languages"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Languages</p></div></div></a></div><div><a id="app/user-interface/modes" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/user-interface/modes"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>UI Modes</p></div></div></a></div><div><a id="app/user-interface/themes" class="rounded p-1.5 px-2 flex text-foreground text-[15px] hover:text-foreground hover:border-lm-purple/80 hover:bg-secondary/60 opacity-[90%] hover:opacity-100 transition-all duration-300 ease-in-out" href="/docs/app/user-interface/themes"><div class="flex w-full items-center justify-between gap-2"><div class="markdown-body text-foreground hover:text-foreground [&amp;_p]:text-[15px]"><p>Color Themes</p></div></div></a></div></div></div></div></div></div></div></div></div></div><div class="h-full flex-none lg:min-w-[320px]"></div><script type="application/ld+json">{"@context":"https://schema.org","@type":"TechArticle","@id":"https://lmstudio.ai/docs/app#article","mainEntityOfPage":"https://lmstudio.ai/docs/app","headline":"Welcome to LM Studio Docs!","description":"Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM Studio.","image":["https://lmstudio.ai/api/og?title=Welcome%20to%20LM%20Studio%20Docs!&description=Learn%20how%20to%20run%20Llama%2C%20DeepSeek%2C%20Qwen%2C%20Phi%2C%20and%20other%20LLMs%20locally%20with%20LM%20Studio.&from=docs/app"],"inLanguage":"en","publisher":{"@id":"https://lmstudio.ai/#organization"}}</script><div class="flex w-full flex-row items-center justify-center lg:min-w-0 lg:flex-1"><div class="z-10 flex h-full w-full items-start overflow-y-hidden p-0 px-5 md:pl-[20px] justify-center"><div class="flex h-full w-full flex-col justify-start pl-0 z-10 max-w-[950px] flex-none gap-0 pb-[150px] pt-[20px] xl:px-[30px]"><div class="flex w-full flex-col gap-1.5 py-5 md:py-5 md:pb-10"><div class="flex w-full flex-col gap-1.5"><div class="flex w-full flex-row items-center justify-between gap-3"><div class="min-w-0 flex-1"><div class="markdown-body text-foreground [&amp;&gt;h1]:font-medium"><h1 id="welcome-to-lm-studio-docs" node="[object Object]" class="flex w-full flex-row items-center justify-start gap-2"><span>Welcome to LM Studio Docs!</span><a href="#welcome-to-lm-studio-docs" class="text-sm text-foreground/60" aria-label="Link to this section"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-xl transition duration-200 ease-in-out opacity-0 hover:opacity-100" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1></div></div><div class="mb-3"><button class="border inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 transition-all duration-[250ms] ease-in-out bg-secondary/40 text-secondary-foreground hover:bg-secondary/70 border-none active:bg-secondary/50 h-8 rounded-md px-3"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="mr-2" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect><path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path></svg><span class="hidden md:flex">Copy as Markdown</span><span class="flex md:hidden">Copy</span></button></div></div></div><div class="markdown-body text-xl font-[400] opacity-80"><p class="text-foreground/90 text-xl font-[400] opacity-80">Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM Studio.</p></div></div><div class="markdown-body "><p class="text-foreground/90">To get LM Studio, head over to the <a href="/download" node="[object Object]" class="" rel="noopener noreferrer">Downloads page</a> and download an installer for your operating system.</p>
<p class="text-foreground/90">LM Studio is available for macOS, Windows, and Linux.</p>
<h2 id="what-can-i-do-with-lm-studio" node="[object Object]" class="flex w-full flex-row items-center justify-start gap-2"><span class="">What can I do with LM Studio?</span><a href="#what-can-i-do-with-lm-studio" class="text-sm text-foreground/60" aria-label="Link to this section" title="Link to &#x27;What can I do with LM Studio?&#x27;"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-xl transition duration-200 ease-in-out opacity-0 hover:opacity-100" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul node="[object Object]" class="list-decimal">
<li>Download and run local LLMs like gpt-oss or Llama, Qwen</li>
<li>Use a simple and flexible chat interface</li>
<li>Connect MCP servers and use them with local models</li>
<li>Search &amp; download functionality (via Hugging Face ðŸ¤—)</li>
<li>Serve local models on OpenAI-like endpoints, locally and on the network</li>
<li>Manage your local models, prompts, and configurations</li>
</ul>
<h2 id="system-requirements" node="[object Object]" class="flex w-full flex-row items-center justify-start gap-2"><span class="">System requirements</span><a href="#system-requirements" class="text-sm text-foreground/60" aria-label="Link to this section" title="Link to &#x27;System requirements&#x27;"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-xl transition duration-200 ease-in-out opacity-0 hover:opacity-100" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p class="text-foreground/90">LM Studio generally supports Apple Silicon Macs, x64/ARM64 Windows PCs, and x64 Linux PCs.</p>
<p class="text-foreground/90">Consult the <a href="app/system-requirements" node="[object Object]" class="" rel="noopener noreferrer">System Requirements</a> page for more detailed information.</p>
<h2 id="run-llamacpp-gguf-or-mlx-models" node="[object Object]" class="flex w-full flex-row items-center justify-start gap-2"><span class="">Run llama.cpp (GGUF) or MLX models</span><a href="#run-llamacpp-gguf-or-mlx-models" class="text-sm text-foreground/60" aria-label="Link to this section" title="Link to &#x27;Run llama.cpp (GGUF) or MLX models&#x27;"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-xl transition duration-200 ease-in-out opacity-0 hover:opacity-100" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p class="text-foreground/90">LM Studio supports running LLMs on Mac, Windows, and Linux using <a href="https://github.com/ggerganov/llama.cpp" node="[object Object]" class="" target="_blank" rel="noopener noreferrer"><code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">llama.cpp</code></a>.</p>
<p class="text-foreground/90">On Apple Silicon Macs, LM Studio also supports running LLMs using Apple&#x27;s <a href="https://github.com/ml-explore/mlx" node="[object Object]" class="" target="_blank" rel="noopener noreferrer"><code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">MLX</code></a>.</p>
<p class="text-foreground/90">To install or manage LM Runtimes, press <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">âŒ˜</code> <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">Shift</code> <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">R</code> on Mac or <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">Ctrl</code> <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">Shift</code> <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">R</code> on Windows/Linux.</p>
<h2 id="lm-studio-as-an-mcp-client" node="[object Object]" class="flex w-full flex-row items-center justify-start gap-2"><span class="">LM Studio as an MCP client</span><a href="#lm-studio-as-an-mcp-client" class="text-sm text-foreground/60" aria-label="Link to this section" title="Link to &#x27;LM Studio as an MCP client&#x27;"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-xl transition duration-200 ease-in-out opacity-0 hover:opacity-100" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p class="text-foreground/90">You can install MCP servers in LM Studio and use them with your local models.</p>
<p class="text-foreground/90">See the docs for more: <a href="/docs/app/plugins/mcp" node="[object Object]" class="" rel="noopener noreferrer">Use MCP server</a>.</p>
<p class="text-foreground/90">If you&#x27;re develping an MCP server, check out <a href="/docs/app/plugins/mcp/deeplink" node="[object Object]" class="" rel="noopener noreferrer">Add to LM Studio Button</a>.</p>
<h2 id="run-an-llm-like-gpt-oss-llama-qwen-mistral-or-deepseek-r1-on-your-computer" node="[object Object]" class="flex w-full flex-row items-center justify-start gap-2"><span class="">Run an LLM like <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">gpt-oss</code>, <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">Llama</code>, <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">Qwen</code>, <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">Mistral</code>, or <code class="whitespace-nowrap" dir="ltr" style="white-space:pre-wrap;word-break:break-word">DeepSeek R1</code> on your computer</span><a href="#run-an-llm-like-gpt-oss-llama-qwen-mistral-or-deepseek-r1-on-your-computer" class="text-sm text-foreground/60" aria-label="Link to this section" title="Link to &#x27;Run an LLM like ,[object Object],, ,[object Object],, ,[object Object],, ,[object Object],, or ,[object Object], on your computer&#x27;"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-xl transition duration-200 ease-in-out opacity-0 hover:opacity-100" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p class="text-foreground/90">To run an LLM on your computer you first need to download the model weights.</p>
<p class="text-foreground/90">You can do this right within LM Studio! See <a href="app/basics/download-model" node="[object Object]" class="" rel="noopener noreferrer">Download an LLM</a> for guidance.</p>
<h2 id="chat-with-documents-entirely-offline-on-your-computer" node="[object Object]" class="flex w-full flex-row items-center justify-start gap-2"><span class="">Chat with documents entirely offline on your computer</span><a href="#chat-with-documents-entirely-offline-on-your-computer" class="text-sm text-foreground/60" aria-label="Link to this section" title="Link to &#x27;Chat with documents entirely offline on your computer&#x27;"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-xl transition duration-200 ease-in-out opacity-0 hover:opacity-100" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p class="text-foreground/90">You can attach documents to your chat messages and interact with them entirely offline, also known as &quot;RAG&quot;.</p>
<p class="text-foreground/90">Read more about how to use this feature in the <a href="app/basics/rag" node="[object Object]" class="" rel="noopener noreferrer">Chat with Documents</a> guide.</p>
<h2 id="use-lm-studios-api-from-your-own-apps-and-scripts" node="[object Object]" class="flex w-full flex-row items-center justify-start gap-2"><span class="">Use LM Studio&#x27;s API from your own apps and scripts</span><a href="#use-lm-studios-api-from-your-own-apps-and-scripts" class="text-sm text-foreground/60" aria-label="Link to this section" title="Link to &#x27;Use LM Studio&#x27;s API from your own apps and scripts&#x27;"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-xl transition duration-200 ease-in-out opacity-0 hover:opacity-100" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p class="text-foreground/90">LM Studio provides a REST API that you can use to interact with your local models from your own apps and scripts.</p>
<ul node="[object Object]" class="list-disc">
<li><a href="api/openai-api" node="[object Object]" class="" rel="noopener noreferrer">OpenAI Compatibility API</a></li>
<li><a href="api/rest-api" node="[object Object]" class="" rel="noopener noreferrer">LM Studio REST API (beta)</a></li>
</ul>
<br/>
<h2 id="community" node="[object Object]" class="flex w-full flex-row items-center justify-start gap-2"><span class="">Community</span><a href="#community" class="text-sm text-foreground/60" aria-label="Link to this section" title="Link to &#x27;Community&#x27;"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-xl transition duration-200 ease-in-out opacity-0 hover:opacity-100" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p class="text-foreground/90">Join the LM Studio community on <a href="https://discord.gg/aPQfnNkxGC" node="[object Object]" class="" target="_blank" rel="noopener noreferrer">Discord</a> to ask questions, share knowledge, and get help from other users and the LM Studio team.</p></div><div class="mt-12 flex w-full flex-col items-center justify-center gap-5"><div class="h-7 w-full"></div><div class="w-full rounded border border-border/10 p-2 text-center"><div class="markdown-body text-xs"><p class="text-foreground/90 text-xs">This page&#x27;s source is available on <a href="https://github.com/lmstudio-ai/docs/blob/main/0_app/0_root/index.md" node="[object Object]" class="" target="_blank" rel="noopener noreferrer">GitHub</a></p></div></div></div></div></div></div><div class="relative w-[0px] flex-none xl:w-[350px]"><div class="absolute right-0 hidden w-full flex-grow flex-col gap-2 lg:flex"><div class="fixed flex w-[300px] flex-grow flex-col gap-2 overflow-y-auto p-5 pt-6" style="height:calc(100vh - 65px)"><p class="text-sm opacity-80">On this page</p><div class="flex w-full flex-col gap-3"></div><div class="flex flex-col gap-2" style="margin-left:0px"><p class="text-sm text-foreground/50 cursor-pointer hover:text-lm-blue">What can I do with LM Studio?</p></div><div class="flex flex-col gap-2" style="margin-left:0px"><p class="text-sm text-foreground/50 cursor-pointer hover:text-lm-blue">System requirements</p></div><div class="flex flex-col gap-2" style="margin-left:0px"><p class="text-sm text-foreground/50 cursor-pointer hover:text-lm-blue">Run llama.cpp (GGUF) or MLX models</p></div><div class="flex flex-col gap-2" style="margin-left:0px"><p class="text-sm text-foreground/50 cursor-pointer hover:text-lm-blue">LM Studio as an MCP client</p></div><div class="flex flex-col gap-2" style="margin-left:0px"><p class="text-sm text-foreground/50 cursor-pointer hover:text-lm-blue">Run an LLM like gpt-oss, Llama, Qwen, Mistral, or DeepSeek R1 on your computer</p></div><div class="flex flex-col gap-2" style="margin-left:0px"><p class="text-sm text-foreground/50 cursor-pointer hover:text-lm-blue">Chat with documents entirely offline on your computer</p></div><div class="flex flex-col gap-2" style="margin-left:0px"><p class="text-sm text-foreground/50 cursor-pointer hover:text-lm-blue">Use LM Studio&#x27;s API from your own apps and scripts</p></div><div class="flex flex-col gap-2" style="margin-left:0px"><p class="text-sm text-foreground/50 cursor-pointer hover:text-lm-blue">Community</p></div><div class="mt-8 flex items-center gap-3 border-t border-border/20 pt-5 text-xs opacity-80"><a href="https://github.com/lmstudio-ai/docs/blob/main/0_app/0_root/index.md" target="_blank" rel="noreferrer noopener" class="flex items-center gap-2 hover:text-foreground hover:underline" aria-label="View source on GitHub" title="View source on GitHub"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="h-4 w-4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><span>Page Source</span></a><a href="https://github.com/lmstudio-ai/docs/edit/main/0_app/0_root/index.md" target="_blank" rel="noreferrer noopener" class="flex items-center gap-2 hover:text-foreground hover:underline" aria-label="Edit this page" title="Edit this page"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M17 3a2.828 2.828 0 1 1 4 4L7.5 20.5 2 22l1.5-5.5L17 3z"></path></svg><span>Edit on GitHub</span></a></div></div></div></div></div></div></div></div></div></div></div></div></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><script src="/_next/static/chunks/webpack-77a60de418d8e871.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/5558a0fd21778b44.css\",\"style\"]\n2:HL[\"/_next/static/media/10893401b2d5e766-s.p.ttf\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/ttf\"}]\n3:HL[\"/_next/static/media/10eb923c8d83aed3-s.p.ttf\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/ttf\"}]\n4:HL[\"/_next/static/css/5d898acc654ad377.css\",\"style\"]\n5:HL[\"/_next/static/css/77904bbb8e5c2b67.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"6:I[12846,[],\"\"]\n9:I[58459,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"6890\",\"static/chunks/6890-680ccec6f8ba7871.js\",\"7772\",\"static/chunks/7772-31f69a9dcf45ceee.js\",\"1597\",\"static/chunks/1597-04f9481876dbbe2a.js\",\"2130\",\"static/chunks/2130-a91b687142678a30.js\",\"9962\",\"static/chunks/9962-4fbb58d4dc9d3e27.js\",\"1113\",\"static/chunks/app/(static)/docs/%5B...slug%5D/page-750ba986e1beb3f3.js\"],\"CopyMarkdownButton\"]\nf:I[2423,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"1779\",\"static/chunks/0e762574-26ab547f20f731a1.js\",\"5706\",\"static/chunks/9c4e2130-5b8fdd2a06a75632.js\",\"6305\",\"static/chunks/eec3d76d-692d954fdc526554.js\",\"614\",\"static/chunks/3d47b92a-50c49e0880f014d1.js\",\"522\",\"static/chunks/94730671-6f37d42c2422b345.js\",\"3217\",\"static/chunks/578c2090-ecc970c206c04bd8.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"6890\",\"static/chunks/6890-680ccec6f8ba7871.js\",\"1687\",\"static/chunks/1687-bad5337c161b89bf.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"1346\",\"static/chunks/1346-7cdf57d39b4f88ef.js\",\"2183\",\"static/chunks/2183-5f3f4936ef3f46d1.js\",\"2130\",\"static/chunks/2130-a91b687142678a30.js\",\"6234\",\"static/chunks/6234-46ca85d7201402af.js\",\"6165\",\"static/chunks/app/(static)/docs/l"])</script><script>self.__next_f.push([1,"ayout-b7a00b0618400251.js\"],\"DocsSecondaryHeaderDesktop\"]\n70:I[2423,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"1779\",\"static/chunks/0e762574-26ab547f20f731a1.js\",\"5706\",\"static/chunks/9c4e2130-5b8fdd2a06a75632.js\",\"6305\",\"static/chunks/eec3d76d-692d954fdc526554.js\",\"614\",\"static/chunks/3d47b92a-50c49e0880f014d1.js\",\"522\",\"static/chunks/94730671-6f37d42c2422b345.js\",\"3217\",\"static/chunks/578c2090-ecc970c206c04bd8.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"6890\",\"static/chunks/6890-680ccec6f8ba7871.js\",\"1687\",\"static/chunks/1687-bad5337c161b89bf.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"1346\",\"static/chunks/1346-7cdf57d39b4f88ef.js\",\"2183\",\"static/chunks/2183-5f3f4936ef3f46d1.js\",\"2130\",\"static/chunks/2130-a91b687142678a30.js\",\"6234\",\"static/chunks/6234-46ca85d7201402af.js\",\"6165\",\"static/chunks/app/(static)/docs/layout-b7a00b0618400251.js\"],\"DocumentationSidebarSection\"]\n1ee:I[4707,[],\"\"]\n1ef:I[36423,[],\"\"]\n1f0:I[62382,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"1779\",\"static/chunks/0e762574-26ab547f20f731a1.js\",\"5706\",\"static/chunks/9c4e2130-5b8fdd2a06a75632.js\",\"6305\",\"static/chunks/eec3d76d-692d954fdc526554.js\",\"614\",\"static/chunks/3d47b92a-50c49e0880f014d1.js\",\"522\",\"static/chunks/94730671-6f37d42c2422b345.js\",\"3217\",\"static/chunks/578c2090-ecc970c206c04bd8.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210"])</script><script>self.__next_f.push([1,"479325c8.js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"8782\",\"static/chunks/8782-7f5165b200ddbee7.js\",\"1687\",\"static/chunks/1687-bad5337c161b89bf.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"1346\",\"static/chunks/1346-7cdf57d39b4f88ef.js\",\"1526\",\"static/chunks/1526-c6ca2b6e8924795d.js\",\"2130\",\"static/chunks/2130-a91b687142678a30.js\",\"8801\",\"static/chunks/8801-cdec8854fe39743a.js\",\"6234\",\"static/chunks/6234-46ca85d7201402af.js\",\"326\",\"static/chunks/app/(dynamic)/layout-4a5b0d5600a00d5a.js\"],\"ProfileProvider\"]\n1f1:I[57246,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"1779\",\"static/chunks/0e762574-26ab547f20f731a1.js\",\"5706\",\"static/chunks/9c4e2130-5b8fdd2a06a75632.js\",\"6305\",\"static/chunks/eec3d76d-692d954fdc526554.js\",\"614\",\"static/chunks/3d47b92a-50c49e0880f014d1.js\",\"522\",\"static/chunks/94730671-6f37d42c2422b345.js\",\"3217\",\"static/chunks/578c2090-ecc970c206c04bd8.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"8782\",\"static/chunks/8782-7f5165b200ddbee7.js\",\"1687\",\"static/chunks/1687-bad5337c161b89bf.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"1346\",\"static/chunks/1346-7cdf57d39b4f88ef.js\",\"1526\",\"static/chunks/1526-c6ca2b6e8924795d.js\",\"2130\",\"static/chunks/2130-a91b687142678a30.js\",\"8801\",\"static/chunks/8801-cdec8854fe39743a.js\",\"6234\",\"static/chunks/6234-46ca85d7201402af.js\",\"326\",\"static/chunks/app/(dynamic)/layout-4a5b0d5600a00d5a.js\"],\"UserOrganizationsProvider\"]\n1f2:I[14438,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"1779\",\"static/"])</script><script>self.__next_f.push([1,"chunks/0e762574-26ab547f20f731a1.js\",\"5706\",\"static/chunks/9c4e2130-5b8fdd2a06a75632.js\",\"6305\",\"static/chunks/eec3d76d-692d954fdc526554.js\",\"614\",\"static/chunks/3d47b92a-50c49e0880f014d1.js\",\"522\",\"static/chunks/94730671-6f37d42c2422b345.js\",\"3217\",\"static/chunks/578c2090-ecc970c206c04bd8.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"8782\",\"static/chunks/8782-7f5165b200ddbee7.js\",\"1687\",\"static/chunks/1687-bad5337c161b89bf.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"1346\",\"static/chunks/1346-7cdf57d39b4f88ef.js\",\"1526\",\"static/chunks/1526-c6ca2b6e8924795d.js\",\"2130\",\"static/chunks/2130-a91b687142678a30.js\",\"8801\",\"static/chunks/8801-cdec8854fe39743a.js\",\"6234\",\"static/chunks/6234-46ca85d7201402af.js\",\"326\",\"static/chunks/app/(dynamic)/layout-4a5b0d5600a00d5a.js\"],\"Toaster\"]\n1f3:I[46504,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"1779\",\"static/chunks/0e762574-26ab547f20f731a1.js\",\"5706\",\"static/chunks/9c4e2130-5b8fdd2a06a75632.js\",\"6305\",\"static/chunks/eec3d76d-692d954fdc526554.js\",\"614\",\"static/chunks/3d47b92a-50c49e0880f014d1.js\",\"522\",\"static/chunks/94730671-6f37d42c2422b345.js\",\"3217\",\"static/chunks/578c2090-ecc970c206c04bd8.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"8782\",\"static/chunks/8782-7f5165b200ddbee7.js\",\"1687\",\"static/chunks/1687-bad5337c161b89bf.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"13"])</script><script>self.__next_f.push([1,"46\",\"static/chunks/1346-7cdf57d39b4f88ef.js\",\"1526\",\"static/chunks/1526-c6ca2b6e8924795d.js\",\"2130\",\"static/chunks/2130-a91b687142678a30.js\",\"8801\",\"static/chunks/8801-cdec8854fe39743a.js\",\"6234\",\"static/chunks/6234-46ca85d7201402af.js\",\"326\",\"static/chunks/app/(dynamic)/layout-4a5b0d5600a00d5a.js\"],\"PostNavToastListener\"]\n1f4:I[80118,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"1779\",\"static/chunks/0e762574-26ab547f20f731a1.js\",\"5706\",\"static/chunks/9c4e2130-5b8fdd2a06a75632.js\",\"6305\",\"static/chunks/eec3d76d-692d954fdc526554.js\",\"614\",\"static/chunks/3d47b92a-50c49e0880f014d1.js\",\"522\",\"static/chunks/94730671-6f37d42c2422b345.js\",\"3217\",\"static/chunks/578c2090-ecc970c206c04bd8.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"8782\",\"static/chunks/8782-7f5165b200ddbee7.js\",\"1687\",\"static/chunks/1687-bad5337c161b89bf.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"1346\",\"static/chunks/1346-7cdf57d39b4f88ef.js\",\"1526\",\"static/chunks/1526-c6ca2b6e8924795d.js\",\"2130\",\"static/chunks/2130-a91b687142678a30.js\",\"8801\",\"static/chunks/8801-cdec8854fe39743a.js\",\"6234\",\"static/chunks/6234-46ca85d7201402af.js\",\"326\",\"static/chunks/app/(dynamic)/layout-4a5b0d5600a00d5a.js\"],\"FeaturesProvider\"]\n1f5:I[97961,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"1779\",\"static/chunks/0e762574-26ab547f20f731a1.js\",\"5706\",\"static/chunks/9c4e2130-5b8fdd2a06a75632.js\",\"6305\",\"static/chunks/eec3d76d-692d954fdc526554.js\",\"614\",\"static/chunks/3d47b92a-50c49e0880f014d1.js\",\"522\",\"static/chunks/94730671-6f37d42c2422b345.js\",\"3217\",\"static/"])</script><script>self.__next_f.push([1,"chunks/578c2090-ecc970c206c04bd8.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"8782\",\"static/chunks/8782-7f5165b200ddbee7.js\",\"1687\",\"static/chunks/1687-bad5337c161b89bf.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"1346\",\"static/chunks/1346-7cdf57d39b4f88ef.js\",\"1526\",\"static/chunks/1526-c6ca2b6e8924795d.js\",\"2130\",\"static/chunks/2130-a91b687142678a30.js\",\"8801\",\"static/chunks/8801-cdec8854fe39743a.js\",\"6234\",\"static/chunks/6234-46ca85d7201402af.js\",\"326\",\"static/chunks/app/(dynamic)/layout-4a5b0d5600a00d5a.js\"],\"AppComponent\"]\n1f8:I[21667,[\"6470\",\"static/chunks/app/global-error-bc0866584b5d17be.js\"],\"default\"]\na:T96d,"])</script><script>self.__next_f.push([1,"\nTo get LM Studio, head over to the [Downloads page](/download) and download an installer for your operating system.\n\nLM Studio is available for macOS, Windows, and Linux.\n\n## What can I do with LM Studio?\n\n1. Download and run local LLMs like gpt-oss or Llama, Qwen\n2. Use a simple and flexible chat interface\n3. Connect MCP servers and use them with local models\n4. Search \u0026 download functionality (via Hugging Face ðŸ¤—)\n5. Serve local models on OpenAI-like endpoints, locally and on the network\n6. Manage your local models, prompts, and configurations\n\n## System requirements\n\nLM Studio generally supports Apple Silicon Macs, x64/ARM64 Windows PCs, and x64 Linux PCs.\n\nConsult the [System Requirements](app/system-requirements) page for more detailed information.\n\n## Run llama.cpp (GGUF) or MLX models\n\nLM Studio supports running LLMs on Mac, Windows, and Linux using [`llama.cpp`](https://github.com/ggerganov/llama.cpp).\n\nOn Apple Silicon Macs, LM Studio also supports running LLMs using Apple's [`MLX`](https://github.com/ml-explore/mlx).\n\nTo install or manage LM Runtimes, press `âŒ˜` `Shift` `R` on Mac or `Ctrl` `Shift` `R` on Windows/Linux.\n\n## LM Studio as an MCP client\n\nYou can install MCP servers in LM Studio and use them with your local models.\n\nSee the docs for more: [Use MCP server](/docs/app/plugins/mcp).\n\nIf you're develping an MCP server, check out [Add to LM Studio Button](/docs/app/plugins/mcp/deeplink).\n\n## Run an LLM like `gpt-oss`, `Llama`, `Qwen`, `Mistral`, or `DeepSeek R1` on your computer\n\nTo run an LLM on your computer you first need to download the model weights.\n\nYou can do this right within LM Studio! See [Download an LLM](app/basics/download-model) for guidance.\n\n## Chat with documents entirely offline on your computer\n\nYou can attach documents to your chat messages and interact with them entirely offline, also known as \"RAG\".\n\nRead more about how to use this feature in the [Chat with Documents](app/basics/rag) guide.\n\n## Use LM Studio's API from your own apps and scripts\n\nLM Studio provides a REST API that you can use to interact with your local models from your own apps and scripts.\n\n- [OpenAI Compatibility API](api/openai-api)\n- [LM Studio REST API (beta)](api/rest-api)\n\n\u003cbr /\u003e\n\n## Community\n\nJoin the LM Studio community on [Discord](https://discord.gg/aPQfnNkxGC) to ask questions, share knowledge, and get help from other users and the LM Studio team.\n"])</script><script>self.__next_f.push([1,"10:T96d,"])</script><script>self.__next_f.push([1,"\nTo get LM Studio, head over to the [Downloads page](/download) and download an installer for your operating system.\n\nLM Studio is available for macOS, Windows, and Linux.\n\n## What can I do with LM Studio?\n\n1. Download and run local LLMs like gpt-oss or Llama, Qwen\n2. Use a simple and flexible chat interface\n3. Connect MCP servers and use them with local models\n4. Search \u0026 download functionality (via Hugging Face ðŸ¤—)\n5. Serve local models on OpenAI-like endpoints, locally and on the network\n6. Manage your local models, prompts, and configurations\n\n## System requirements\n\nLM Studio generally supports Apple Silicon Macs, x64/ARM64 Windows PCs, and x64 Linux PCs.\n\nConsult the [System Requirements](app/system-requirements) page for more detailed information.\n\n## Run llama.cpp (GGUF) or MLX models\n\nLM Studio supports running LLMs on Mac, Windows, and Linux using [`llama.cpp`](https://github.com/ggerganov/llama.cpp).\n\nOn Apple Silicon Macs, LM Studio also supports running LLMs using Apple's [`MLX`](https://github.com/ml-explore/mlx).\n\nTo install or manage LM Runtimes, press `âŒ˜` `Shift` `R` on Mac or `Ctrl` `Shift` `R` on Windows/Linux.\n\n## LM Studio as an MCP client\n\nYou can install MCP servers in LM Studio and use them with your local models.\n\nSee the docs for more: [Use MCP server](/docs/app/plugins/mcp).\n\nIf you're develping an MCP server, check out [Add to LM Studio Button](/docs/app/plugins/mcp/deeplink).\n\n## Run an LLM like `gpt-oss`, `Llama`, `Qwen`, `Mistral`, or `DeepSeek R1` on your computer\n\nTo run an LLM on your computer you first need to download the model weights.\n\nYou can do this right within LM Studio! See [Download an LLM](app/basics/download-model) for guidance.\n\n## Chat with documents entirely offline on your computer\n\nYou can attach documents to your chat messages and interact with them entirely offline, also known as \"RAG\".\n\nRead more about how to use this feature in the [Chat with Documents](app/basics/rag) guide.\n\n## Use LM Studio's API from your own apps and scripts\n\nLM Studio provides a REST API that you can use to interact with your local models from your own apps and scripts.\n\n- [OpenAI Compatibility API](api/openai-api)\n- [LM Studio REST API (beta)](api/rest-api)\n\n\u003cbr /\u003e\n\n## Community\n\nJoin the LM Studio community on [Discord](https://discord.gg/aPQfnNkxGC) to ask questions, share knowledge, and get help from other users and the LM Studio team.\n"])</script><script>self.__next_f.push([1,"11:T45c,\n## macOS\n\n- Chip: Apple Silicon (M1/M2/M3/M4).\n- macOS 13.4 or newer is required.\n  - For MLX models, macOS 14.0 or newer is required.\n- 16GB+ RAM recommended.\n  - You may still be able to use LM Studio on 8GB Macs, but stick to smaller models and modest context sizes.\n- Intel-based Macs are currently not supported. Chime in [here](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/9) if you are interested in this.\n\n## Windows\n\nLM Studio is supported on both x64 and ARM (Snapdragon X Elite) based systems.\n\n- CPU: AVX2 instruction set support is required (for x64)\n- RAM: LLMs can consume a lot of RAM. At least 16GB of RAM is recommended.\n- GPU: at least 4GB of dedicated VRAM is recommended.\n\n## Linux\n\nLM Studio is supported on both x64 and ARM64 (aarch64) based systems.\n\n- LM Studio for Linux is distributed as an AppImage.\n- Ubuntu 20.04 or newer is required\n- Ubuntu versions newer than 22 are not well tested. Let us know if you're running into issues by opening a bug [here](https://github.com/lmstudio-ai/lmstudio-bug-tracker).\n- CPU:\n  - On x64, LM Studio ships with AVX2 support by default\n12:Tb7f,"])</script><script>self.__next_f.push([1,"\n```lms_notice\nIn general, LM Studio does not require the internet in order to work. This includes core functions like chatting with models, chatting with documents, or running a local server, none of which require the internet.\n```\n\n### Operations that do NOT require connectivity\n\n#### Using downloaded LLMs\n\nOnce you have an LLM onto your machine, the model will run locally and you should be good to go entirely offline. Nothing you enter into LM Studio when chatting with LLMs leaves your device.\n\n#### Chatting with documents (RAG)\n\nWhen you drag and drop a document into LM Studio to chat with it or perform RAG, that document stays on your machine. All document processing is done locally, and nothing you upload into LM Studio leaves the application.\n\n#### Running a local server\n\nLM Studio can be used as a server to provide LLM inferencing on localhost or the local network. Requests to LM Studio use OpenAI endpoints and return OpenAI-like response objects, but stay local.\n\n### Operations that require connectivity\n\nSeveral operations, described below, rely on internet connectivity. Once you get an LLM onto your machine, you should be good to go entirely offline.\n\n#### Searching for models\n\nWhen you search for models in the Discover tab, LM Studio makes network requests (e.g. to huggingface.co). Search will not work without internet connection.\n\n#### Downloading new models\n\nIn order to download models you need a stable (and decently fast) internet connection. You can also 'sideload' models (use models that were procured outside the app). See instructions for [sideloading models](/docs/advanced/sideload).\n\n#### Discover tab's model catalog\n\nAny given version of LM Studio ships with an initial model catalog built-in. The entries in the catalog are typically the state of the online catalog near the moment we cut the release. However, in order to show stats and download options for each model, we need to make network requests (e.g. to huggingface.co).\n\n#### Downloading runtimes\n\n[LM Runtimes](advanced/lm-runtimes) are individually packaged software libraries, or LLM engines, that allow running certain formats of models (e.g. `llama.cpp`). As of LM Studio 0.3.0 (read the [announcement](https://lmstudio.ai/blog/lmstudio-v0.3.0)) it's easy to download and even hot-swap runtimes without a full LM Studio update. To check for available runtimes, and to download them, we need to make network requests.\n\n#### Checking for app updates\n\nOn macOS and Windows, LM Studio has a built-in app updater that's capable. The linux in-app updater [is in the works](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/89). When you open LM Studio, the app updater will make a network request to check if there are any new updates available. If there's a new version, the app will show you a notification to update now or later.\nWithout internet connectivity you will not be able to update the app via the in-app updater.\n"])</script><script>self.__next_f.push([1,"13:T795,\nDouble check computer meets the minimum [system requirements](/docs/system-requirements).\n\n```lms_info\nYou might sometimes see terms such as `open-source models` or `open-weights models`. Different models might be released under different licenses and varying degrees of 'openness'. In order to run a model locally, you need to be able to get access to its \"weights\", often distributed as one or more files that end with `.gguf`, `.safetensors` etc.\n```\n\n\u003chr\u003e\n\n## Getting up and running\n\nFirst, **install the latest version of LM Studio**. You can get it from [here](/download).\n\nOnce you're all set up, you need to **download your first LLM**.\n\n### 1. Download an LLM to your computer\n\nHead over to the Discover tab to download models. Pick one of the curated options or search for models by search query (e.g. `\"Llama\"`). See more in-depth information about downloading models [here](/docs/basics/download-models).\n\n\u003cimg src=\"/assets/docs/discover.png\" style=\"width: 500px; margin-top:30px\" data-caption=\"The Discover tab in LM Studio\" /\u003e\n\n### 2. Load a model to memory\n\nHead over to the **Chat** tab, and\n\n1. Open the model loader\n2. Select one of the models you downloaded (or [sideloaded](/docs/advanced/sideload)).\n3. Optionally, choose load configuration parameters.\n\n\u003cimg src=\"/assets/docs/loader.png\" data-caption=\"Quickly open the model loader with `cmd` + `L` on macOS or `ctrl` + `L` on Windows/Linux\" /\u003e\n\n##### What does loading a model mean?\n\nLoading a model typically means allocating memory to be able to accommodate the model's weights and other parameters in your computer's RAM.\n\n### 3. Chat!\n\nOnce the model is loaded, you can start a back-and-forth conversation with the model in the Chat tab.\n\n\u003cimg src=\"/assets/docs/chat.png\" data-caption=\"LM Studio on macOS\" /\u003e\n\n\u003chr\u003e\n\n### Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n14:T61d,\nLM Studio has a ChatGPT-like interface for chatting with local LLMs. You can create many d"])</script><script>self.__next_f.push([1,"ifferent conversation threads and manage them in folders.\n\n\u003chr\u003e\n\n### Create a new chat\n\nYou can create a new chat by clicking the \"+\" button or by using a keyboard shortcut: `âŒ˜` + `N` on Mac, or `ctrl` + `N` on Windows / Linux.\n\n### Create a folder\n\nCreate a new folder by clicking the new folder button or by pressing: `âŒ˜` + `shift` + `N` on Mac, or `ctrl` + `shift` + `N` on Windows / Linux.\n\n### Drag and drop\n\nYou can drag and drop chats in and out of folders, and even drag folders into folders!\n\n### Duplicate chats\n\nYou can duplicate a whole chat conversation by clicking the `â€¢â€¢â€¢` menu and selecting \"Duplicate\". If the chat has any files in it, they will be duplicated too.\n\n## FAQ\n\n#### Where are chats stored in the file system?\n\nRight-click on a chat and choose \"Reveal in Finder\" / \"Show in File Explorer\".\nConversations are stored in JSON format. It is NOT recommended to edit them manually, nor to rely on their structure.\n\n#### Does the model learn from chats?\n\nThe model doesn't 'learn' from chats. The model only 'knows' the content that is present in the chat or is provided to it via configuration options such as the \"system prompt\".\n\n## Conversations folder filesystem path\n\nMac / Linux:\n\n```shell\n~/.lmstudio/conversations/\n```\n\nWindows:\n\n```ps\n%USERPROFILE%\\.lmstudio\\conversations\n```\n\n\u003chr\u003e\n\n### Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n15:T62e,\nLM Studio comes with a built-in model downloader that let's you download any supported model from [Hugging Face](https://huggingface.co).\n\n\u003cimg src=\"/assets/docs/discover.png\" style=\"width: 500px; margin-top:30px\" data-caption=\"Download models from the Discover tab in LM Studio\" /\u003e\n\n\u003chr\u003e\n\n### Searching for models\n\nYou can search for models by keyword (e.g. `llama`, `gemma`, `lmstudio`), or by providing a specific `user/model` string. You can even insert full Hugging Face URLs into the search bar!\n\n###### Pro tip: you can jump to the Discover tab from anywhere"])</script><script>self.__next_f.push([1," by pressing `âŒ˜` + `2` on Mac, or `ctrl` + `2` on Windows / Linux.\n\n### Which download option to choose?\n\nYou will often see several options for any given model named things like `Q3_K_S`, `Q_8` etc. These are all copies of the same model, provided in varying degrees of fidelity. The `Q` represents a technique called \"Quantization\", which roughly means compressing model files in size, while giving up some degree of quality.\n\nChoose a 4-bit option or higher if your machine is capable enough for running it.\n\n\u003cimg src=\"/assets/docs/search.png\" style=\"\" data-caption=\"Hugging Face search results in LM Studio\" /\u003e\n\n\u003chr\u003e\n\n`Advanced`\n\n### Changing the models directory\n\nYou can change the models directory by heading to My Models\n\n\u003cimg src=\"/assets/docs/change-models-dir.png\" style=\"width:80%\" data-caption=\"Manage your models directory in the My Models tab\"\u003e\n\n\u003chr\u003e\n\n### Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n16:T61a,\nYou can attach document files (`.docx`, `.pdf`, `.txt`) to chat sessions in LM Studio.\n\nThis will provide additional context to LLMs you chat with through the app.\n\n\u003chr\u003e\n\n## Terminology\n\n- **Retrieval**: Identifying relevant portion of a long source document\n- **Query**: The input to the retrieval operation\n- **RAG**: Retrieval-Augmented Generation\\*\n- **Context**: the 'working memory' of an LLM. Has a maximum size\n\n###### \\* In this context, 'Generation' means the output of the LLM.\n###### Context sizes are measured in \"tokens\". One token is often about 3/4 of a word.\n\n## RAG vs. Full document 'in context'\n\nIf the document is short enough (i.e., if it fits in the model's context), LM Studio will add the file contents to the conversation in full. This is particularly useful for models that support longer context sizes such as Meta's Llama 3.1 and Mistral Nemo.\n\nIf the document is very long, LM Studio will opt into using \"Retrieval Augmented Generation\", frequently referred to as \"RAG\". RAG means attempting t"])</script><script>self.__next_f.push([1,"o fish out relevant bits of a very long document (or several documents) and providing them to the model for reference. This technique sometimes works really well, but sometimes it requires some tuning and experimentation.\n\n## Tip for successful RAG\n\nprovide as much context in your query as possible. Mention terms, ideas, and words you expect to be in the relevant source material. This will often increase the chance the system will provide useful context to the LLM. As always, experimentation is the best way to find what works best.\n17:Tb37,"])</script><script>self.__next_f.push([1,"\nStarting LM Studio 0.3.17, LM Studio acts as an **Model Context Protocol (MCP) Host**. This means you can connect MCP servers to the app and make them available to your models.\n\n### Be cautious\n\nNever install MCPs from untrusted sources.\n\n```lms_warning\nSome MCP servers can run arbitrary code, access your local files, and use your network connection. Always be cautious when installing and using MCP servers. If you don't trust the source, don't install it.\n```\n\n# Use MCP servers in LM Studio\n\nStarting 0.3.17 (b10), LM Studio supports both local and remote MCP servers. You can add MCPs by editing the app's `mcp.json` file or via the [\"Add to LM Studio\" Button](mcp/deeplink), when available. LM Studio currently follows Cursor's `mcp.json` notation.\n\n## Install new servers: `mcp.json`\n\nSwitch to the \"Program\" tab in the right hand sidebar. Click `Install \u003e Edit mcp.json`.\n\n\u003cimg src=\"/assets/docs/install-mcp.png\"  data-caption=\"\" style=\"width: 80%;\" className=\"\" /\u003e\n\nThis will open the `mcp.json` file in the in-app editor. You can add MCP servers by editing this file.\n\n\u003cimg src=\"/assets/docs/mcp-editor.png\"  data-caption=\"Edit mcp.json using the in-app editor\" style=\"width: 100%;\" className=\"\" /\u003e\n\n### Example MCP to try: Hugging Face MCP Server\n\nThis MCP server provides access to functions like model and dataset search.\n\n\u003cdiv className=\"w-fit\"\u003e\n  \u003ca style=\"background: rgb(255,255,255)\" href=\"https://lmstudio.ai/install-mcp?name=hf-mcp-server\u0026config=eyJ1cmwiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvL21jcCIsImhlYWRlcnMiOnsiQXV0aG9yaXphdGlvbiI6IkJlYXJlciA8WU9VUl9IRl9UT0tFTj4ifX0%3D\"\u003e\n    \u003cLightVariant\u003e\n      \u003cimg src=\"https://files.lmstudio.ai/deeplink/mcp-install-light.svg\" alt=\"Add MCP Server hf-mcp-server to LM Studio\" /\u003e\n    \u003c/LightVariant\u003e\n    \u003cDarkVariant\u003e\n      \u003cimg src=\"https://files.lmstudio.ai/deeplink/mcp-install-dark.svg\" alt=\"Add MCP Server hf-mcp-server to LM Studio\" /\u003e\n    \u003c/DarkVariant\u003e\n  \u003c/a\u003e\n\u003c/div\u003e\n\n```json\n{\n  \"mcpServers\": {\n    \"hf-mcp-server\": {\n      \"url\": \"https://huggingface.co/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer \u003cYOUR_HF_TOKEN\u003e\"\n      }\n    }\n  }\n}\n```\n\n###### You will need to replace `\u003cYOUR_HF_TOKEN\u003e` with your actual Hugging Face token. Learn more [here](https://huggingface.co/docs/hub/en/security-tokens).\n\nUse the [deeplink button](mcp/deeplink), or copy the JSON snippet above and paste it into your `mcp.json` file.\n\n---\n\n## Gotchas and Troubleshooting\n\n- Never install MCP servers from untrusted sources. Some MCPs can have far reaching access to your system.\n\n- Some MCP servers were designed to be used with Claude, ChatGPT, Gemini and might use excessive amounts of tokens.\n\n  - Watch out for this. It may quickly bog down your local model and trigger frequent context overflows.\n\n- When adding MCP servers manually, copy only the content after `\"mcpServers\": {` and before the closing `}`.\n"])</script><script>self.__next_f.push([1,"18:T4c0,\nYou can install MCP servers in LM Studio with one click using a deeplink.\n\nStarting with version 0.3.17 (10), LM Studio can act as an MCP host. Learn more about it [here](../mcp).\n\n---\n\n# Generate your own MCP install link\n\nEnter your MCP JSON entry to generate a deeplink for the `Add to LM Studio` button.\n\n```lms_mcp_deep_link_generator\n\n```\n\n## Try an example\n\nTry to copy and paste the following into the link generator above.\n\n```json\n{\n  \"hf-mcp-server\": {\n    \"url\": \"https://huggingface.co/mcp\",\n    \"headers\": {\n      \"Authorization\": \"Bearer \u003cYOUR_HF_TOKEN\u003e\"\n    }\n  }\n}\n```\n\n### Deeplink format\n\n```bash\nlmstudio://add_mcp?name=hf-mcp-server\u0026config=eyJ1cmwiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvL21jcCIsImhlYWRlcnMiOnsiQXV0aG9yaXphdGlvbiI6IkJlYXJlciA8WU9VUl9IRl9UT0tFTj4ifX0%3D\n```\n\n#### Parameters\n\n```lms_params\n- name: \"lmstudio://\"\n  type: \"protocol\"\n  description: \"The protocol scheme to open LM Studio\"\n- name: \"add_mcp\"\n  type: \"path\"\n  description: \"The action to install an MCP server\"\n- name: \"name\"\n  type: \"query parameter\"\n  description: \"The name of the MCP server to install\"\n- name: \"config\"\n  type: \"query parameter\"\n  description: \"Base64 encoded JSON configuration for the MCP server\"\n```\n19:T11ab,"])</script><script>self.__next_f.push([1,"\n`Draft`\n\n[`model.yaml`](https://modelyaml.org) describes a model and all of its variants in a single portable file. Models in LM Studio's [model catalog](https://lmstudio.ai/models) are all implemented using model.yaml.\n\nThis allows abstracting away the underlying format (GGUF, MLX, etc) and presenting a single entry point for a given model. Furthermore, the model.yaml file supports baking in additional metadata, load and inference options, and even custom logic (e.g. enable/disable thinking).\n\n**You can clone existing model.yaml files on the LM Studio Hub and even [publish your own](./modelyaml/publish)!**\n\n## Core fields\n\n### `model`\n\nThe canonical identifier in the form `publisher/model`.\n\n```yaml\nmodel: qwen/qwen3-8b\n```\n\n### `base`\n\nPoints to the \"concrete\" model files or other virtual models. Each entry uses a unique `key` and one or more `sources` from which the file can be fetched.\n\nThe snippet below demonstrates a case where the model (`qwen/qwen3-8b`) can resolve to one of 3 different concrete models.\n\n```yaml\nmodel: qwen/qwen3-8b\nbase:\n  - key: lmstudio-community/qwen3-8b-gguf\n    sources:\n      - type: huggingface\n        user: lmstudio-community\n        repo: Qwen3-8B-GGUF\n  - key: lmstudio-community/qwen3-8b-mlx-4bit\n    sources:\n      - type: huggingface\n        user: lmstudio-community\n        repo: Qwen3-8B-MLX-4bit\n  - key: lmstudio-community/qwen3-8b-mlx-8bit\n    sources:\n      - type: huggingface\n        user: lmstudio-community\n        repo: Qwen3-8B-MLX-8bit\n```\n\nConcrete model files refer to the actual weights.\n\n### `metadataOverrides`\n\nOverrides the base model's metadata. This is useful for presentation purposes, for example in LM Studio's model catalog or in app model search. It is not used for any functional changes to the model.\n\n```yaml\nmetadataOverrides:\n  domain: llm\n  architectures:\n    - qwen3\n  compatibilityTypes:\n    - gguf\n    - safetensors\n  paramsStrings:\n    - 8B\n  minMemoryUsageBytes: 4600000000\n  contextLengths:\n    - 40960\n  vision: false\n  reasoning: true\n  trainedForToolUse: true\n```\n\n### `config`\n\nUse this to \"bake in\" default runtime settings (such as sampling parameters) and even load time options.\nThis works similarly to [Per Model Defaults](/docs/app/advanced/per-model).\n\n- `operation:` inference time parameters\n- `load:` load time parameters\n\n```yaml\nconfig:\n  operation:\n    fields:\n      - key: llm.prediction.topKSampling\n        value: 20\n      - key: llm.prediction.temperature\n        value: 0.7\n  load:\n    fields:\n      - key: llm.load.contextLength\n        value: 42690\n```\n\n### `customFields`\n\nDefine model-specific custom fields.\n\n```yaml\ncustomFields:\n  - key: enableThinking\n    displayName: Enable Thinking\n    description: Controls whether the model will think before replying\n    type: boolean\n    defaultValue: true\n    effects:\n      - type: setJinjaVariable\n        variable: enable_thinking\n```\n\nIn order for the above example to work, the jinja template needs to have a variable named `enable_thinking`.\n\n## Complete example\n\nTaken from https://lmstudio.ai/models/qwen/qwen3-8b\n\n```yaml\n# model.yaml is an open standard for defining cross-platform, composable AI models\n# Learn more at https://modelyaml.org\nmodel: qwen/qwen3-8b\nbase:\n  - key: lmstudio-community/qwen3-8b-gguf\n    sources:\n      - type: huggingface\n        user: lmstudio-community\n        repo: Qwen3-8B-GGUF\n  - key: lmstudio-community/qwen3-8b-mlx-4bit\n    sources:\n      - type: huggingface\n        user: lmstudio-community\n        repo: Qwen3-8B-MLX-4bit\n  - key: lmstudio-community/qwen3-8b-mlx-8bit\n    sources:\n      - type: huggingface\n        user: lmstudio-community\n        repo: Qwen3-8B-MLX-8bit\nmetadataOverrides:\n  domain: llm\n  architectures:\n    - qwen3\n  compatibilityTypes:\n    - gguf\n    - safetensors\n  paramsStrings:\n    - 8B\n  minMemoryUsageBytes: 4600000000\n  contextLengths:\n    - 40960\n  vision: false\n  reasoning: true\n  trainedForToolUse: true\nconfig:\n  operation:\n    fields:\n      - key: llm.prediction.topKSampling\n        value: 20\n      - key: llm.prediction.minPSampling\n        value:\n          checked: true\n          value: 0\ncustomFields:\n  - key: enableThinking\n    displayName: Enable Thinking\n    description: Controls whether the model will think before replying\n    type: boolean\n    defaultValue: true\n    effects:\n      - type: setJinjaVariable\n        variable: enable_thinking\n```\n\nThe [GitHub specification](https://github.com/modelyaml/modelyaml) contains further details and the latest schema.\n"])</script><script>self.__next_f.push([1,"1a:T783,\nShare portable models by uploading a [`model.yaml`](./) to your page on the LM Studio Hub.\n\nAfter you publish a model.yaml to the LM Studio Hub, it will be available for other users to download with `lms get`.\n\n###### Note: `model.yaml` refers to metadata only. This means it does not include the actual model weights.\n\n# Quickstart\n\nThe easiest way to get started is by cloning an existing model, modifying it, and then running `lms push`.\n\nFor example, you can clone the Qwen 3 8B model:\n\n```shell\nlms clone qwen/qwen3-8b\n```\n\nThis will result in a local copy `model.yaml`, `README` and other metadata files. Importantly, this does NOT download the model weights.\n\n```lms_terminal\n$ ls\nREADME.md     manifest.json    model.yaml    thumbnail.png\n```\n\n## Change the publisher to your user\n\nThe first part in the `model:` field should be the username of the publisher. Change it to a username of a user or organization for which you have write access.\n\n```diff\n- model: qwen/qwen3-8b\n+ model: your-user-here/qwen3-8b\nbase:\n  - key: lmstudio-community/qwen3-8b-gguf\n    sources:\n# ... the rest of the file\n```\n\n## Sign in\n\nAuthenticate with the Hub from the command line:\n\n```shell\nlms login\n```\n\nThe CLI will print an authentication URL. After you approve access, the session token is saved locally so you can publish models.\n\n## Publish your model\n\nRun the push command in the directory containing `model.yaml`:\n\n```shell\nlms push\n```\n\nThe command packages the file, uploads it, and prints a revision number for the new version.\n\n### Override metadata at publish time\n\nUse `--overrides` to tweak fields without editing the file:\n\n```shell\nlms push --overrides '{\"description\": \"Qwen 3 8B model\"}'\n```\n\n## Downloading a model and using it in LM Studio\n\nAfter publishing, the model appears under your user or organization profile on the LM Studio Hub.\n\nIt can then be downloaded with:\n\n```shell\nlms get my-user/my-model\n```\n1b:Tac3,"])</script><script>self.__next_f.push([1,"\nPresets are a way to bundle together a system prompt and other parameters into a single configuration that can be easily reused across different chats.\n\nNew in 0.3.15: You can [import](/docs/app/presets/import) Presets from file or URL, and even [publish](/docs/app/presets/publish) your own Presets to share with others on to the LM Studio Hub.\n\u003chr\u003e\n\n## Saving, resetting, and deselecting Presets\n\nBelow is the anatomy of the Preset manager:\n\n\u003cimg src=\"/assets/docs/preset-widget-anatomy.png\" style=\"width:70%\" data-caption=\"The anatomy of the Preset manager in the settings sidebar.\"\u003e\n\n## Importing, Publishing, and Updating Downloaded Presets\n\nPresets are JSON files. You can share them by sending around the JSON, or you can share them by publishing them to the LM Studio Hub.\nYou can also import Presets from other users by URL. See the [Import](/docs/app/presets/import) and [Publish](/docs/app/presets/publish) sections for more details.\n\n## Example: Build your own Prompt Library\n\nYou can create your own prompt library by using Presets.\n\n\u003cvideo autoplay loop muted playsinline style=\"width:60vh;\" data-caption=\"Save collections of parameters as a Preset for easy reuse.\" class=\"border border-border\"\u003e\n  \u003csource src=\"https://files.lmstudio.ai/presets.mp4\" type=\"video/mp4\"\u003e\n  Your browser does not support the video tag.\n\u003c/video\u003e\n\nIn addition to system prompts, every parameter under the Advanced Configuration sidebar can be recorded in a named Preset.\n\nFor example, you might want to always use a certain Temperature, Top P, or Max Tokens for a particular use case. You can save these settings as a Preset (with or without a system prompt) and easily switch between them.\n\n#### The Use Case for Presets\n\n- Save your system prompts, inference parameters as a named `Preset`.\n- Easily switch between different use cases, such as reasoning, creative writing, multi-turn conversations, or brainstorming.\n\n## Where Presets are stored\n\nPresets are stored in the following directory:\n\n#### macOS or Linux\n\n```xml\n~/.lmstudio/config-presets\n```\n\n#### Windows\n\n```xml\n%USERPROFILE%\\.lmstudio\\config-presets\n```\n\n### Migration from LM Studio 0.2.\\* Presets\n\n- Presets you've saved in LM Studio 0.2.\\* are automatically readable in 0.3.3 with no migration step needed.\n- If you save **new changes** in a **legacy preset**, it'll be **copied** to a new format upon save.\n  - The old files are NOT deleted.\n- Notable difference: Load parameters are not included in the new preset format.\n  - Favor editing the model's default config in My Models. See [how to do it here](/docs/configuration/per-model).\n\n\u003chr\u003e\n\n### Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n"])</script><script>self.__next_f.push([1,"1c:T798,\nYou can import preset by file or URL. This is useful for sharing presets with others, or for importing presets from other users.\n\n\u003chr\u003e\n\n# Import Presets\n\nFirst, click the presets dropdown in the sidebar. You will see a list of your presets along with 2 buttons: `+ New Preset` and `Import`.\n\nClick the `Import` button to import a preset.\n\n\u003cimg src=\"/assets/docs/preset-import-button.png\" data-caption=\"Import Presets\" /\u003e\n\n## Import Presets from File\n\nOnce you click the Import button, you can select the source of the preset you want to import. You can either import from a file or from a URL.\n\u003cimg src=\"/assets/docs/import-preset-from-file.png\" data-caption=\"Import one or more Presets from file\" /\u003e\n\n## Import Presets from URL\n\nPresets that are [published](/docs/app/presets/publish) to the LM Studio Hub can be imported by providing their URL.\n\nImporting public presets does not require logging in within LM Studio.\n\n\u003cimg src=\"/assets/docs/import-preset-from-url.png\" data-caption=\"Import Presets by URL\" /\u003e\n\n### Using `lms` CLI\nYou can also use the CLI to import presets from URL. This is useful for sharing presets with others.\n\n```\nlms get {author}/{preset-name}\n```\n\nExample:\n```bash\nlms get neil/qwen3-thinking\n```\n\n\n### Find your config-presets directory\n\nLM Studio manages config presets on disk. Presets are local and private by default. You or others can choose to share them by sharing the file.\n\nClick on the `â€¢â€¢â€¢` button in the Preset dropdown and select \"Reveal in Finder\" (or \"Show in Explorer\" on Windows).\n\u003cimg src=\"/assets/docs/preset-reveal-in-finder.png\" data-caption=\"Reveal Preset in your local file system\" /\u003e\n\nThis will download the preset file and automatically surface it in the preset dropdown in the app. \n\n### Where Hub shared presets are stored\nPresets you share, and ones you download from the LM Studio Hub are saved in `~/.lmstudio/hub` on macOS and Linux, or `%USERPROFILE%\\.lmstudio\\hub` on Windows. 1d:T52d,\n`Feature In Preview`\n\nStarting LM Studio 0.3.15, you can publish your Presets to the LM"])</script><script>self.__next_f.push([1," Studio community. This allows you to share your Presets with others and import Presets from other users.\n\nThis feature is early and we would love to hear your feedback. Please report bugs and feedback to bugs@lmstudio.ai.\n\n---\n\n## Step 1: Click the Publish Button\n\nIdentify the Preset you want to publish in the Preset dropdown. Click the `â€¢â€¢â€¢` button and select \"Publish\" from the menu.\n\n\u003cimg src=\"/assets/docs/preset-publish-new.png\" data-caption=\"Click the Publish button to publish your Preset to the LM Studio Hub.\" /\u003e\n\n## Step 2: Set the Preset Details\n\nYou will be prompted to set the details of your Preset. This includes the name (slug) and optional description. \n\nCommunity presets are public and can be used by anyone on the internet!\n\n\u003cimg src=\"/assets/docs/preset-publish-details.png\" data-caption=\"Set the details of your Preset before publishing.\" /\u003e\n\n#### Privacy and Terms\nFor good measure, visit the [Privacy Policy](https://lmstudio.ai/hub-privacy) and [Terms of Service](https://lmstudio.ai/hub-terms) to understand what's suitable to share on the Hub, and how data is handled. Community presets are public and visible to everyone. Make sure you agree to what these documents say before publishing your Preset.1e:T138b,"])</script><script>self.__next_f.push([1,"\n`Advanced`\n\nSpeculative decoding is a technique that can substantially increase the generation speed of large language models (LLMs) without reducing response quality.\n\n\u003chr\u003e\n\n## What is Speculative Decoding\n\nSpeculative decoding relies on the collaboration of two models:\n\n- A larger, \"main\" model\n- A smaller, faster \"draft\" model\n\nDuring generation, the draft model rapidly proposes potential tokens (subwords), which the main model can verify faster than it would take it to generate them from scratch. To maintain quality, the main model only accepts tokens that match what it would have generated. After the last accepted draft token, the main model always generates one additional token.\n\nFor a model to be used as a draft model, it must have the same \"vocabulary\" as the main model.\n\n## How to enable Speculative Decoding\n\nOn `Power User` mode or higher, load a model, then select a `Draft Model` within the `Speculative Decoding` section of the chat sidebar:\n\n\u003cimg src=\"/assets/docs/speculative-decoding-setting.png\" style=\"width:80%; margin-top: 20px; border: 1px solid rgba(0,0,0,0.2);\" data-caption=\"The Speculative Decoding section of the chat sidebar\"\u003e\n\n### Finding compatible draft models\n\nYou might see the following when you open the dropdown:\n\n\u003cimg src=\"/assets/docs/speculative-decoding-no-compatible.png\" style=\"width:40%; margin-top: 20px; border: 1px solid rgba(0,0,0,0.2);\" data-caption=\"No compatible draft models\"\u003e\n\nTry to download a lower parameter variant of the model you have loaded, if it exists. If no smaller versions of your model exist, find a pairing that does.\n\nFor example:\n\n\u003ccenter style=\"margin: 20px;\"\u003e\n\n|          Main Model          |          Draft Model          |\n| :--------------------------: | :---------------------------: |\n|    Llama 3.1 8B Instruct     |     Llama 3.2 1B Instruct     |\n|    Qwen 2.5 14B Instruct     |    Qwen 2.5 0.5B Instruct     |\n| DeepSeek R1 Distill Qwen 32B | DeepSeek R1 Distill Qwen 1.5B |\n\n\u003c/center\u003e\n\nOnce you have both a main and draft model loaded, simply begin chatting to enable speculative decoding.\n\n## Key factors affecting performance\n\nSpeculative decoding speed-up is generally dependent on two things:\n\n1. How small and fast the _draft model_ is compared with the _main model_\n2. How often the draft model is able to make \"good\" suggestions\n\nIn simple terms, you want to choose a draft model that's much smaller than the main model. And some prompts will work better than others.\n\n### An important trade-off\n\nRunning a draft model alongside a main model to enable speculative decoding requires more **computation and resources** than running the main model on its own.\n\nThe key to faster generation of the main model is choosing a draft model that's both small and capable enough.\n\nHere are general guidelines for the **maximum** draft model size you should select based on main model size (in parameters):\n\n\u003ccenter style=\"margin: 20px;\"\u003e\n\n| Main Model Size | Max Draft Model Size to Expect Speed-Ups |\n| :-------------: | :--------------------------------------: |\n|       3B        |                    -                     |\n|       7B        |                    1B                    |\n|       14B       |                    3B                    |\n|       32B       |                    7B                    |\n\n\u003c/center\u003e\n\nGenerally, the larger the size difference is between the main model and the draft model, the greater the speed-up.\n\nNote: if the draft model is not fast enough or effective enough at making \"good\" suggestions to the main model, the generation speed will not increase, and could actually decrease.\n\n### Prompt dependent\n\nOne thing you will likely notice when using speculative decoding is that the generation speed is not consistent across all prompts.\n\nThe reason that the speed-up is not consistent across all prompts is because for some prompts, the draft model is less likely to make \"good\" suggestions to the main model.\n\nHere are some extreme examples that illustrate this concept:\n\n#### 1. Discrete Example: Mathematical Question\n\nPrompt: \"What is the quadratic equation formula?\"\n\nIn this case, both a 70B model and a 0.5B model are both very likely to give the standard formula `x = (-b Â± âˆš(bÂ² - 4ac))/(2a)`. So if the draft model suggested this formula as the next tokens, the target model would likely accept it, making this an ideal case for speculative decoding to work efficiently.\n\n#### 2. Creative Example: Story Generation\n\nPrompt: \"Write a story that begins: 'The door creaked open...'\"\n\nIn this case, the smaller model's draft tokens are likely be rejected more often by the larger model, as each next word could branch into countless valid possibilities.\n\nWhile \"4\" is the only reasonable answer to \"2+2\", this story could continue with \"revealing a monster\", \"as the wind howled\", \"and Sarah froze\", or hundreds of other perfectly valid continuations, making the smaller model's specific word predictions much less likely to match the larger\nmodel's choices.\n"])</script><script>self.__next_f.push([1,"1f:T4bf,\nYou can use compatible models you've downloaded outside of LM Studio by placing them in the expected directory structure.\n\n\u003chr\u003e\n\n### Use `lms import` (experimental)\n\nTo import a `GGUF` model you've downloaded outside of LM Studio, run the following command in your terminal:\n\n```bash\nlms import \u003cpath/to/model.gguf\u003e\n```\n\n###### Follow the interactive prompt to complete the import process.\n\n### LM Studio's expected models directory structure\n\n\u003cimg src=\"/assets/docs/reveal-models-dir.png\" style=\"width:80%\" data-caption=\"Manage your models directory in the My Models tab\"\u003e\n\nLM Studio aims to preserves the directory structure of models downloaded from Hugging Face. The expected directory structure is as follows:\n\n```xml\n~/.lmstudio/models/\nâ””â”€â”€ publisher/\n    â””â”€â”€ model/\n        â””â”€â”€ model-file.gguf\n```\n\nFor example, if you have a model named `ocelot-v1` published by `infra-ai`, the structure would look like this:\n\n```xml\n~/.lmstudio/models/\nâ””â”€â”€ infra-ai/\n    â””â”€â”€ ocelot-v1/\n        â””â”€â”€ ocelot-v1-instruct-q4_0.gguf\n```\n\n\u003chr\u003e\n\n### Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n20:T7f7,\n`Advanced`\n\nYou can set default load settings for each model in LM Studio.\n\nWhen the model is loaded anywhere in the app (including through [`lms load`](/docs/cli#load-a-model-with-options)) these settings will be used.\n\n\u003chr\u003e\n\n### Setting default parameters for a model\n\nHead to the My Models tab and click on the gear âš™ï¸ icon to edit the model's default parameters.\n\n\u003cimg src=\"/assets/docs/model-settings-gear.png\" style=\"width:80%\" data-caption=\"Click on the gear icon to edit the default load settings for a model.\"\u003e\n\nThis will open a dialog where you can set the default parameters for the model.\n\n\u003cvideo autoplay loop muted playsinline style=\"width:50%\" data-caption=\"You can set the default parameters for a model in this dialog.\"\u003e\n  \u003csource src=\"https://files.lmstudio.ai/default-params.mp4\" type=\"video/m"])</script><script>self.__next_f.push([1,"p4\"\u003e\n  Your browser does not support the video tag.\n\u003c/video\u003e\n\nNext time you load the model, these settings will be used.\n\n\n```lms_protip\n#### Reasons to set default load parameters (not required, totally optional)\n\n- Set a particular GPU offload settings for a given model\n- Set a particular context size for a given model\n- Whether or not to utilize Flash Attention for a given model\n\n```\n\n\n\n\n## Advanced Topics\n\n### Changing load settings before loading a model\n\nWhen you load a model, you can optionally change the default load settings.\n\n\u003cimg src=\"/assets/docs/load-model.png\" style=\"width:80%\" data-caption=\"You can change the load settings before loading a model.\"\u003e\n\n### Saving your changes as the default settings for a model\n\nIf you make changes to load settings when you load a model, you can save them as the default settings for that model.\n\n\u003cimg src=\"/assets/docs/save-load-changes.png\" style=\"width:80%\" data-caption=\"If you make changes to load settings when you load a model, you can save them as the default settings for that model.\"\u003e\n\n\n\u003chr\u003e\n\n### Community\nChat with other LM Studio power users, discuss configs, models, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n21:T6aa,\n`Advanced`\n\nBy default, LM Studio will automatically configure the prompt template based on the model file's metadata. \n\nHowever, you can customize the prompt template for any model.\n\n\u003chr\u003e\n\n\n### Overriding the Prompt Template for a Specific Model\n\nHead over to the My Models tab and click on the gear âš™ï¸ icon to edit the model's default parameters.\n###### Pro tip: you can jump to the My Models tab from anywhere by pressing `âŒ˜` + `3` on Mac, or `ctrl` + `3` on Windows / Linux.\n\n### Customize the Prompt Template\n\n###### ðŸ’¡ In most cases you don't need to change the prompt template\n\nWhen a model doesn't come with a prompt template information, LM Studio will surface the `Prompt Template` config box in the **ðŸ§ª Advanced Configuration** sidebar.\n\n\u003cimg src=\"/assets/docs/prompt-template.png\" style=\"width:"])</script><script>self.__next_f.push([1,"80%\" data-caption=\"The Prompt Template config box in the chat sidebar\"\u003e\n\nYou can make this config box always show up by right clicking the sidebar and selecting **Always Show Prompt Template**.\n\n### Prompt template options\n\n#### Jinja Template\nYou can express the prompt template in Jinja.\n\n###### ðŸ’¡ [Jinja](https://en.wikipedia.org/wiki/Jinja_(template_engine)) is a templating engine used to encode the prompt template in several popular LLM model file formats.\n\n#### Manual\n\nYou can also express the prompt template manually by specifying message role prefixes and suffixes.\n\n\u003chr\u003e\n\n#### Reasons you might want to edit the prompt template:\n1. The model's metadata is incorrect, incomplete, or LM Studio doesn't recognize it\n2. The model does not have a prompt template in its metadata (e.g. custom or older models)\n3. You want to customize the prompt template for a specific use case22:T1070,"])</script><script>self.__next_f.push([1,"\nLM Studio is available in `English`, `Spanish`, `Japanese`, `Chinese`, `German`, `Norwegian`, `Turkish`, `Russian`, `Korean`, `Polish`, `Vietnamese`, `Czech`, `Ukrainian`, `Portuguese (BR,PT)` and many more languages thanks to incredible  community localizers.\n\n\u003chr\u003e\n\n### Selecting a Language\n\nYou can choose a language in the Settings tab.\n\nUse the dropdown menu under Preferences \u003e Language.\n\n```lms_protip\nYou can jump to Settings from anywhere in the app by pressing `cmd` + `,` on macOS or `ctrl` + `,` on Windows/Linux.\n```\n\n###### To get to the Settings page, you need to be on [Power User mode](/docs/modes) or higher.\n\n\u003chr\u003e\n\n#### Big thank you to community localizers ðŸ™\n\n- Spanish [@xtianpaiva](https://github.com/xtianpaiva), [@AlexisGross](https://github.com/AlexisGross), [@Tonband](https://github.com/Tonband)\n- Norwegian [@Exlo84](https://github.com/Exlo84)\n- German [@marcelMaier](https://github.com/marcelMaier), [@Goekdeniz-Guelmez](https://github.com/Goekdeniz-Guelmez)\n- Romanian (ro) [@alexandrughinea](https://github.com/alexandrughinea)\n- Turkish (tr) [@progesor](https://github.com/progesor), [@nossbar](https://github.com/nossbar)\n- Russian [@shelomitsky](https://github.com/shelomitsky), [@mlatysh](https://github.com/mlatysh), [@Adjacentai](https://github.com/Adjacentai), [@HostFly](https://github.com/HostFly), [@MotyaDev](https://github.com/MotyaDev), [@Autumn-Whisper](https://github.com/Autumn-Whisper), [@seropheem](https://github.com/seropheem)\n- Korean [@williamjeong2](https://github.com/williamjeong2)\n- Polish [@danieltechdev](https://github.com/danieltechdev)\n- Czech [@ladislavsulc](https://github.com/ladislavsulc)\n- Vietnamese [@trinhvanminh](https://github.com/trinhvanminh), [@godkyo98](https://github.com/godkyo98)\n- Portuguese (BR) [@Sm1g00l](https://github.com/Sm1g00l), [@altiereslima](https://github.com/altiereslima)\n- Portuguese (PT) [@catarino](https://github.com/catarino)\n- Chinese (zh-CN) [@neotan](https://github.com/neotan), [@SweetDream0256](https://github.com/SweetDream0256), [@enKl03B](https://github.com/enKl03B), [@evansrrr](https://github.com/evansrrr), [@xkonglong](https://github.com/xkonglong), [@shadow01a](https://github.com/shadow01a)\n- Chinese (zh-HK), (zh-TW) [@neotan](https://github.com/neotan), [ceshizhuanyong895](https://github.com/ceshizhuanyong895), [@BrassaiKao](https://github.com/BrassaiKao)\n- Chinese (zh-Hant) [@kywarai](https://github.com/kywarai), [ceshizhuanyong895](https://github.com/ceshizhuanyong895)\n- Ukrainian (uk) [@hmelenok](https://github.com/hmelenok)\n- Japanese (ja) [@digitalsp](https://github.com/digitalsp)\n- Dutch (nl) [@alaaf11](https://github.com/alaaf11)\n- Italian (it) [@fralapo](https://github.com/fralapo), [@Bl4ck-D0g](https://github.com/Bl4ck-D0g), [@nikypalma](https://github.com/nikypalma)\n- Indonesian (id) [@dwirx](https://github.com/dwirx)\n- Greek (gr) [@ilikecatgirls](https://github.com/ilikecatgirls)\n- Swedish (sv) [@reinew](https://github.com/reinew)\n- Catalan (ca) [@Gopro3010](https://github.com/Gopro3010)\n- French [@Plexi09](https://github.com/Plexi09)\n- Finnish (fi) [@divergentti](https://github.com/divergentti)\n- Bengali (bn) [@AbiruzzamanMolla](https://github.com/AbiruzzamanMolla)\n- Malayalam (ml) [@prasanthc41m](https://github.com/prasanthc41m)\n- Thai (th) [@gnoparus](https://github.com/gnoparus)\n- Bosnian (bs) [@0haris0](https://github.com/0haris0)\n- Bulgarian (bg) [@DenisZekiria](https://github.com/DenisZekiria)\n- Hindi (hi) [@suhailtajshaik](https://github.com/suhailtajshaik)\n- Hungarian (hu) [@Mekemoka](https://github.com/Mekemoka)\n- Persian (Farsi) (fa) [@mohammad007kh](https://github.com/mohammad007kh), [@darwindev](https://github.com/darwindev)\n- Arabic (ar) [@haqbany](https://github.com/haqbany)\n\nStill under development (due to lack of RTL support in LM Studio)\n\n- Hebrew: [@NHLOCAL](https://github.com/NHLOCAL)\n\n#### Contributing to LM Studio localization\n\nIf you want to improve existing translations or contribute new ones, you're more than welcome to jump in.\n\nLM Studio strings are maintained in https://github.com/lmstudio-ai/localization.\n\nSee instructions for contributing [here](https://github.com/lmstudio-ai/localization/blob/main/README.md).\n"])</script><script>self.__next_f.push([1,"23:T7c1,\n```lms_hstack\n## Get to know the stack\n\n- TypeScript SDK: [lmstudio-js](/docs/typescript)\n- Python SDK: [lmstudio-python](/docs/python)\n- OpenAIâ€‘compatible: [Chat, Responses, Embeddings](/docs/developer/openai-compat)\n- LM Studio CLI: [`lms`](/docs/cli)\n\n:::split:::\n\n## What you can build\n\n- Chat and text generation with streaming\n- Structured output (JSON schema)\n- Tool calling and local agents\n- Embeddings and tokenization\n- Model management (JIT load, TTL, autoâ€‘evict)\n```\n\n## Super quick start\n\n### TypeScript (`lmstudio-js`)\n\n```bash\nnpm install @lmstudio/sdk\n```\n\n```ts\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\nconst model = await client.llm.model(\"openai/gpt-oss-20b\");\nconst result = await model.respond(\"Who are you, and what can you do?\");\n\nconsole.info(result.content);\n```\n\nFull docs: [lmstudio-js](/docs/typescript), Source: [GitHub](https://github.com/lmstudio-ai/lmstudio-js)\n\n### Python (`lmstudio-python`)\n\n```bash\npip install lmstudio\n```\n\n```python\nimport lmstudio as lms\n\nwith lms.Client() as client:\n    model = client.llm.model(\"openai/gpt-oss-20b\")\n    result = model.respond(\"Who are you, and what can you do?\")\n    print(result)\n```\n\nFull docs: [lmstudio-python](/docs/python), Source: [GitHub](https://github.com/lmstudio-ai/lmstudio-python)\n\n### Try a minimal HTTP request (OpenAIâ€‘compatible)\n\n```bash\nlms server start --port 1234\n```\n\n```bash\ncurl http://localhost:1234/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai/gpt-oss-20b\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Who are you, and what can you do?\"}]\n  }'\n```\n\nFull docs: [OpenAIâ€‘compatible endpoints](/docs/developer/openai-compat)\n\n## Helpful links\n\n- API Changelog: [/docs/developer/api-changelog](/docs/developer/api-changelog)\n- Local server basics: [/docs/developer/core](/docs/developer/core)\n- CLI reference: [/docs/cli](/docs/cli)\n- Community: [Discord](https://discord.gg/lmstudio)\n24:T243a,"])</script><script>self.__next_f.push([1,"\n---\n\n###### LM Studio 0.3.29 â€¢Â 2025â€‘10â€‘06\n\n### OpenAI `/v1/responses` and variant listing\n\n- New OpenAIâ€‘compatible endpoint: `POST /v1/responses`.\n  - Stateful interactions via `previous_response_id`.\n  - Custom tool calling and Remote MCP support (optâ€‘in).\n  - Reasoning support with `reasoning.effort` for `openai/gptâ€‘ossâ€‘20b`.\n  - Streaming via SSE when `stream: true`.\n- CLI: `lms ls --variants` lists all variants for multiâ€‘variant models.\n- Docs: [/docs/developer/openai-compat](/docs/developer/openai-compat). Full release notes: [/blog/lmstudio-v0.3.29](/blog/lmstudio-v0.3.29).\n\n---\n\n###### LM Studio 0.3.27 â€¢Â 2025â€‘09â€‘24\n\n### CLI: model resource estimates, status, and interrupts\n\n- New: `lms load --estimate-only \u003cmodel\u003e` prints estimated GPU and total memory before loading. Honors `--context-length` and `--gpu`, and uses an improved estimator that now accounts for flash attention and vision models.\n- `lms chat`: press `Ctrl+C` to interrupt an ongoing prediction.\n- `lms ps --json` now reports each model's generation status and the number of queued prediction requests.\n- CLI color contrast improved for light mode.\n- See docs: [/docs/cli/local-models/load](/docs/cli/local-models/load). Full release notes: [/blog/lmstudio-v0.3.27](/blog/lmstudio-v0.3.27).\n\n---\n\n###### LM Studio 0.3.26 â€¢Â 2025â€‘09â€‘15\n\n### CLI log streaming: server + model\n\n- `lms log stream` now supports multiple sources and filters.\n  - `--source server` streams HTTP server logs (startup, endpoints, status)\n  - `--source model --filter input,output` streams formatted user input and model output\n  - Append `--json` for machineâ€‘readable logs; `--stats` adds tokens/sec and related metrics (model source)\n- See usage and examples: [/docs/cli/serve/log-stream](/docs/cli/serve/log-stream). Full release notes: [/blog/lmstudio-v0.3.26](/blog/lmstudio-v0.3.26).\n\n---\n\n###### LM Studio 0.3.25 â€¢Â 2025â€‘09â€‘04\n\n### New model support (API)\n\n- Added support for NVIDIA Nemotronâ€‘Nanoâ€‘v2 with toolâ€‘calling via the OpenAIâ€‘compatible endpoints [â€¡](/blog/lmstudio-v0.3.25).\n- Added support for Google EmbeddingGemma for the `/v1/embeddings` endpoint [â€¡](/blog/lmstudio-v0.3.25).\n\n---\n\n###### LM Studio 0.3.24 â€¢Â 2025â€‘08â€‘28\n\n### Seedâ€‘OSS toolâ€‘calling and template fixes\n\n- Added support for ByteDance/Seedâ€‘OSS including toolâ€‘calling and promptâ€‘template compatibility fixes in the OpenAIâ€‘compatible API [â€¡](/blog/lmstudio-v0.3.24).\n- Fixed cases where tool calls were not parsed for certain prompt templates [â€¡](/blog/lmstudio-v0.3.24).\n\n---\n\n###### LM Studio 0.3.23 â€¢Â 2025â€‘08â€‘12\n\n### Reasoning content and toolâ€‘calling reliability\n\n- For `gptâ€‘oss` on `POST /v1/chat/completions`, reasoning content moves out of `message.content` and into `choices.message.reasoning` (nonâ€‘streaming) and `choices.delta.reasoning` (streaming), aligning with `o3â€‘mini` [â€¡](/blog/lmstudio-v0.3.23).\n- Tool names are normalized (e.g., snake_case) before being provided to the model to improve toolâ€‘calling reliability [â€¡](/blog/lmstudio-v0.3.23).\n- Fixed errors for certain toolsâ€‘containing requests to `POST /v1/chat/completions` (e.g., \"reading 'properties'\") and nonâ€‘streaming toolâ€‘call failures [â€¡](/blog/lmstudio-v0.3.23).\n\n---\n\n###### LM Studio 0.3.19 â€¢Â 2025â€‘07â€‘21\n\n### Bug fixes for streaming and tool calls\n\n- Corrected usage statistics returned by OpenAIâ€‘compatible streaming responses [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.19#:~:text=,OpenAI%20streaming%20responses%20were%20incorrect).\n- Improved handling of parallel tool calls via the streaming API [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.19#:~:text=,API%20were%20not%20handled%20correctly).\n- Fixed parsing of correct tool calls for certain Mistral models [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.19#:~:text=,Ryzen%20AI%20PRO%20300%20series).\n\n---\n\n###### LM Studio 0.3.18 â€¢Â 2025â€‘07â€‘10\n\n### Streaming options and toolâ€‘calling improvements\n\n- Added support for the `stream_options` object on OpenAIâ€‘compatible endpoints. Setting `stream_options.include_usage` to `true` returns prompt and completion token usage during streaming [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=%2A%20Added%20support%20for%20%60,to%20support%20more%20prompt%20templates).\n- Errors returned from streaming endpoints now follow the correct format expected by OpenAI clients [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=,with%20proper%20chat%20templates).\n- Toolâ€‘calling support added for MistralÂ v13 tokenizer models, using proper chat templates [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=,with%20proper%20chat%20templates).\n- The `response_format.type` field now accepts `\"text\"` in chatâ€‘completion requests [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=,that%20are%20split%20across%20multiple).\n- Fixed bugs where parallel tool calls split across multiple chunks were dropped and where rootâ€‘level `$defs` in tool definitions were stripped [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=,being%20stripped%20in%20tool%20definitions).\n\n---\n\n###### LM Studio 0.3.17 â€¢Â 2025â€‘06â€‘25\n\n### Toolâ€‘calling reliability and tokenâ€‘count updates\n\n- Token counts now include the system prompt and tool definitions [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.17#:~:text=,have%20a%20URL%20in%20the). This makes usage reporting more accurate for both the UI and the API.\n- Toolâ€‘call argument tokens are streamed as they are generated [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.17#:~:text=Build%206), improving responsiveness when using streamed function calls.\n- Various fixes improve MCP and toolâ€‘calling reliability, including correct handling of tools that omit a `parameters` object and preventing hangs when an MCP server reloads [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.17#:~:text=,tool%20calls%20would%20hang%20indefinitely).\n\n---\n\n###### LM Studio 0.3.16 â€¢Â 2025â€‘05â€‘23\n\n### Model capabilities in `GETÂ /models`\n\n- The OpenAIâ€‘compatible REST API (`/api/v0`) now returns a `capabilities` array in the `GETÂ /models` response. Each model lists its supported capabilities (e.g. `\"tool_use\"`) [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.16#:~:text=,response) so clients can programmatically discover toolâ€‘enabled models.\n- Fixed a streaming bug where an empty function name string was appended after the first packet of streamed tool calls [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.16#:~:text=%2A%20Bugfix%3A%20%5BOpenAI,packet%20of%20streamed%20function%20calls).\n\n---\n\n###### [ðŸ‘¾ LM Studio 0.3.15](/blog/lmstudio-v0.3.15) â€¢ 2025-04-24\n\n### Improved Tool Use API Support\n\nOpenAI-like REST API now supports the `tool_choice` parameter:\n\n```json\n{\n  \"tool_choice\": \"auto\" // or \"none\", \"required\"\n}\n```\n\n- `\"tool_choice\": \"none\"` â€” Model will not call tools\n- `\"tool_choice\": \"auto\"` â€” Model decides\n- `\"tool_choice\": \"required\"` â€” Model must call tools (llama.cpp only)\n\nChunked responses now set `\"finish_reason\": \"tool_calls\"` when appropriate.\n\n---\n\n###### [ðŸ‘¾ LM Studio 0.3.14](/blog/lmstudio-v0.3.14) â€¢ 2025-03-27\n\n### [API/SDK] Preset Support\n\nRESTful API and SDKs support specifying presets in requests.\n\n_(example needed)_\n\n###### [ðŸ‘¾ LM Studio 0.3.10](/blog/lmstudio-v0.3.10) â€¢ 2025-02-18\n\n### Speculative Decoding API\n\nEnable speculative decoding in API requests with `\"draft_model\"`:\n\n```json\n{\n  \"model\": \"deepseek-r1-distill-qwen-7b\",\n  \"draft_model\": \"deepseek-r1-distill-qwen-0.5b\",\n  \"messages\": [ ... ]\n}\n```\n\nResponses now include a `stats` object for speculative decoding:\n\n```json\n\"stats\": {\n  \"tokens_per_second\": ...,\n  \"draft_model\": \"...\",\n  \"total_draft_tokens_count\": ...,\n  \"accepted_draft_tokens_count\": ...,\n  \"rejected_draft_tokens_count\": ...,\n  \"ignored_draft_tokens_count\": ...\n}\n```\n\n---\n\n###### [ðŸ‘¾ LM Studio 0.3.9](blog/lmstudio-v0.3.9) â€¢ 2025-01-30\n\n### Idle TTL and Auto Evict\n\nSet a TTL (in seconds) for models loaded via API requests (docs article: [Idle TTL and Auto-Evict](/docs/developer/core/ttl-and-auto-evict))\n\n```diff\ncurl http://localhost:1234/api/v0/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"deepseek-r1-distill-qwen-7b\",\n    \"messages\": [ ... ]\n+   \"ttl\": 300,\n}'\n```\n\nWith `lms`:\n\n```\nlms load --ttl \u003cseconds\u003e\n```\n\n### Separate `reasoning_content` in Chat Completion responses\n\nFor DeepSeek R1 models, get reasoning content in a separate field. See more [here](/blog/lmstudio-v0.3.9#separate-reasoningcontent-in-chat-completion-responses).\n\nTurn this on in App Settings \u003e Developer.\n\n---\n\n###### [ðŸ‘¾ LM Studio 0.3.6](blog/lmstudio-v0.3.6) â€¢ 2025-01-06\n\n### Tool and Function Calling API\n\nUse any LLM that supports Tool Use and Function Calling through the OpenAI-like API.\n\nDocs: [Tool Use and Function Calling](/docs/developer/core/tools).\n\n---\n\n###### [ðŸ‘¾ LM Studio 0.3.5](blog/lmstudio-v0.3.5) â€¢ 2024-10-22\n\n### Introducing `lms get`: download models from the terminal\n\nYou can now download models directly from the terminal using a keyword\n\n```bash\nlms get deepseek-r1\n```\n\nor a full Hugging Face URL\n\n```bash\nlms get \u003chugging face url\u003e\n```\n\nTo filter for MLX models only, add `--mlx` to the command.\n\n```bash\nlms get deepseek-r1 --mlx\n```\n"])</script><script>self.__next_f.push([1,"25:T1170,"])</script><script>self.__next_f.push([1,"\n## Background\n\n- `JIT loading` makes it easy to use your LM Studio models in other apps: you don't need to manually load the model first before being able to use it. However, this also means that models can stay loaded in memory even when they're not being used. `[Default: enabled]`\n\n- (New) `Idle TTL` (technically: Time-To-Live) defines how long a model can stay loaded in memory without receiving any requests. When the TTL expires, the model is automatically unloaded from memory. You can set a TTL using the `ttl` field in your request payload. `[Default: 60 minutes]`\n\n- (New) `Auto-Evict` is a feature that unloads previously JIT loaded models before loading new ones. This enables easy switching between models from client apps without having to manually unload them first. You can enable or disable this feature in Developer tab \u003e Server Settings. `[Default: enabled]`\n\n## Idle TTL\n\n**Use case**: imagine you're using an app like [Zed](https://github.com/zed-industries/zed/blob/main/crates/lmstudio/src/lmstudio.rs#L340), [Cline](https://github.com/cline/cline/blob/main/src/api/providers/lmstudio.ts), or [Continue.dev](https://docs.continue.dev/customize/model-providers/more/lmstudio) to interact with LLMs served by LM Studio. These apps leverage JIT to load models on-demand the first time you use them.\n\n**Problem**: When you're not actively using a model, you might don't want it to remain loaded in memory.\n\n**Solution**: Set a TTL for models loaded via API requests. The idle timer resets every time the model receives a request, so it won't disappear while you use it. A model is considered idle if it's not doing any work. When the idle TTL expires, the model is automatically unloaded from memory.\n\n### Set App-default Idle TTL\n\nBy default, JIT-loaded models have a TTL of 60 minutes. You can configure a default TTL value for any model loaded via JIT like so:\n\n\u003cimg src=\"/assets/docs/app-default-ttl.png\" style=\"width: 500px; \" data-caption=\"Set a default TTL value. Will be used for all JIT loaded models unless specified otherwise in the request payload\" /\u003e\n\n### Set per-model TTL-model in API requests\n\nWhen JIT loading is enabled, the **first request** to a model will load it into memory. You can specify a TTL for that model in the request payload.\n\nThis works for requests targeting both the [OpenAI compatibility API](/docs/developer/openai-api) and the [LM Studio's REST API](/docs/developer/rest):\n\n```diff\ncurl http://localhost:1234/api/v0/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"deepseek-r1-distill-qwen-7b\",\n+   \"ttl\": 300,\n    \"messages\": [ ... ]\n}'\n```\n\n###### This will set a TTL of 5 minutes (300 seconds) for this model if it is JIT loaded.\n\n### Set TTL for models loaded with `lms`\n\nBy default, models loaded with `lms load` do not have a TTL, and will remain loaded in memory until you manually unload them.\n\nYou can set a TTL for a model loaded with `lms` like so:\n\n```bash\nlms load \u003cmodel\u003e --ttl 3600\n```\n\n###### Load a `\u003cmodel\u003e` with a TTL of 1 hour (3600 seconds)\n\n### Specify TTL when loading models in the server tab\n\nYou can also set a TTL when loading a model in the server tab like so\n\n\u003cimg src=\"/assets/docs/ttl-server-model.png\" style=\"width: 100%;\" data-caption=\"Set a TTL value when loading a model in the server tab\" /\u003e\n\n## Configure Auto-Evict for JIT loaded models\n\nWith this setting, you can ensure new models loaded via JIT automatically unload previously loaded models first.\n\nThis is useful when you want to switch between models from another app without worrying about memory building up with unused models.\n\n\u003cimg src=\"/assets/docs/auto-evict-and-ttl.png\" style=\"width: 500px; margin-top:30px\" data-caption=\"Enable or disable Auto-Evict for JIT loaded models in the Developer tab \u003e Server Settings\" /\u003e\n\n**When Auto-Evict is ON** (default):\n\n- At most `1` model is kept loaded in memory at a time (when loaded via JIT)\n- Non-JIT loaded models are not affected\n\n**When Auto-Evict is OFF**:\n\n- Switching models from an external app will keep previous models loaded in memory\n- Models will remain loaded until either:\n  - Their TTL expires\n  - You manually unload them\n\nThis feature works in tandem with TTL to provide better memory management for your workflow.\n\n### Nomenclature\n\n`TTL`: Time-To-Live, is a term borrowed from networking protocols and cache systems. It defines how long a resource can remain allocated before it's considered stale and evicted.\n"])</script><script>self.__next_f.push([1,"26:T95e,"])</script><script>self.__next_f.push([1,"\nLM Studio can be run as a service without the GUI. This is useful for running LM Studio on a server or in the background on your local machine. This works on Mac, Windows, and Linux machines with a graphical user interface.\n\n## Run LM Studio as a service\n\nRunning LM Studio as a service consists of several new features intended to make it more efficient to use LM Studio as a developer tool.\n\n1. The ability to run LM Studio without the GUI\n2. The ability to start the LM Studio LLM server on machine login, headlessly\n3. On-demand model loading\n\n## Run the LLM service on machine login\n\nTo enable this, head to app settings (`Cmd` / `Ctrl` + `,`) and check the box to run the LLM server on login.\n\n\u003cimg src=\"/assets/docs/headless-settings.png\" style=\"\" data-caption=\"Enable the LLM server to start on machine login\" /\u003e\n\nWhen this setting is enabled, exiting the app will minimize it to the system tray, and the LLM server will continue to run in the background.\n\n## Just-In-Time (JIT) model loading for OpenAI endpoints\n\nUseful when utilizing LM Studio as an LLM service with other frontends or applications.\n\n\u003cimg src=\"/assets/docs/jit-loading.png\" style=\"\" data-caption=\"Load models on demand\" /\u003e\n\n#### When JIT loading is ON:\n\n- Call to `/v1/models` will return all downloaded models, not only the ones loaded into memory\n- Calls to inference endpoints will load the model into memory if it's not already loaded\n\n#### When JIT loading is OFF:\n\n- Call to `/v1/models` will return only the models loaded into memory\n- You have to first load the model into memory before being able to use it\n\n##### What about auto unloading?\n\nAs of LM Studio 0.3.5, auto unloading is not yet in place. Models that are loaded via JIT loading will remain in memory until you unload them.\nWe expect to implement more sophisticated memory management in the near future. Let us know if you have any feedback or suggestions.\n\n## Auto Server Start\n\nYour last server state will be saved and restored on app or service launch.\n\nTo achieve this programmatically, you can use the following command:\n\n```bash\nlms server start\n```\n\n### Community\n\nChat with other LM Studio developers, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n\nPlease report bugs and issues in the [lmstudio-bug-tracker](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues) GitHub repository.\n"])</script><script>self.__next_f.push([1,"27:T411,\nYou can serve local LLMs from LM Studio's Developer tab, either on `localhost` or on the network.\n\nLM Studio's APIs can be used through [REST API](/docs/developer/rest), client libraries like [lmstudio-js](/docs/typescript) and [lmstudio-python](/docs/python), and [OpenAI compatibility endpoints](/docs/developer/openai-compat)\n\n\u003cimg src=\"/assets/docs/server.png\" style=\"\" data-caption=\"Load and serve LLMs from LM Studio\" /\u003e\n\n### Running the server\n\nTo run the server, go to the Developer tab in LM Studio, and toggle the \"Start server\" switch to start the API server.\n\n\u003cimg src=\"/assets/docs/server-start.png\" style=\"\" data-caption=\"Start the LM Studio API Server\" /\u003e\n\n\n\nAlternatively, you can use `lms` ([LM Studio's CLI](/docs/cli)) to start the server from your terminal:\n\n```bash\nlms server start\n```\n\n\n### API options\n\n- [LM Studio REST API](/docs/developer/rest)\n- [TypeScript SDK](/docs/typescript) - `lmstudio-js`\n- [Python SDK](/docs/python) - `lmstudio-python`\n- [OpenAI compatibility endpoints](/docs/developer/openai-compat)\n28:T61a,\nYou can configure server settings, such as the port number, whether to allow other API clients to access the server and MCP features.\n\n\u003cimg src=\"/assets/docs/server-config.png\" style=\"\" data-caption=\"Configure LM Studio API Server settings\" /\u003e\n\n\n### Settings information\n\n```lms_params\n- name: Server Port\n  type: Integer\n  optional: false\n  description: Port number on which the LM Studio API server listens for incoming connections.\n  unstyledName: true\n- name: Serve on Local Network\n  type: Switch\n  description: Allow other devices on the same local network to access the API server. Learn more in the [Serve on Local Network](/docs/developer/core/server/serve-on-network) section.\n  unstyledName: true\n- name: Allow Per Request Remote MCPs\n  type: Switch\n  description: Enable sending requests to remote MCP (Model Control Protocol) servers on a per-request basis.\n  unstyledName: true\n- name: Enable CORS\n  type: Switch\n  description: Enable Cross-Origin Resource Sharing (CORS) to a"])</script><script>self.__next_f.push([1,"llow applications from different origins to access the API.\n  unstyledName: true\n- name: Just in Time Model Loading\n  type: Switch\n  description: Load models dynamically at request time to save memory.\n  unstyledName: true\n- name: Auto Unload Unused JIT Models\n  type: Switch\n  description: Automatically unload JIT-loaded models from memory when they are no longer in use.\n  unstyledName: true\n- name: Only Keep Last JIT Loaded Model\n  type: Switch\n  description: Keep only the most recently used JIT-loaded model in memory to minimize RAM usag\n  unstyledName: true\n```\n29:T19a1,"])</script><script>self.__next_f.push([1,"\n##### Requires [LM Studio 0.3.6](/download) or newer.\n\nLM Studio now has its own REST API, in addition to OpenAI compatibility mode ([learn more](/docs/developer/openai-compat)).\n\nThe REST API includes enhanced stats such as Token / Second and Time To First Token (TTFT), as well as rich information about models such as loaded vs unloaded, max context, quantization, and more.\n\n#### Supported API Endpoints\n\n- [`GET /api/v0/models`](#get-apiv0models) - List available models\n- [`GET /api/v0/models/{model}`](#get-apiv0modelsmodel) - Get info about a specific model\n- [`POST /api/v0/chat/completions`](#post-apiv0chatcompletions) - Chat Completions (messages -\u003e assistant response)\n- [`POST /api/v0/completions`](#post-apiv0completions) - Text Completions (prompt -\u003e completion)\n- [`POST /api/v0/embeddings`](#post-apiv0embeddings) - Text Embeddings (text -\u003e embedding)\n\n---\n\n### Start the REST API server\n\nTo start the server, run the following command:\n\n```bash\nlms server start\n```\n\n```lms_protip\nYou can run LM Studio as a service and get the server to auto-start on boot without launching the GUI. [Learn about Headless Mode](/docs/developer/core/headless).\n```\n\n## Endpoints\n\n### `GET /api/v0/models`\n\nList all loaded and downloaded models\n\n**Example request**\n\n```bash\ncurl http://localhost:1234/api/v0/models\n```\n\n**Response format**\n\n```json\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"qwen2-vl-7b-instruct\",\n      \"object\": \"model\",\n      \"type\": \"vlm\",\n      \"publisher\": \"mlx-community\",\n      \"arch\": \"qwen2_vl\",\n      \"compatibility_type\": \"mlx\",\n      \"quantization\": \"4bit\",\n      \"state\": \"not-loaded\",\n      \"max_context_length\": 32768\n    },\n    {\n      \"id\": \"meta-llama-3.1-8b-instruct\",\n      \"object\": \"model\",\n      \"type\": \"llm\",\n      \"publisher\": \"lmstudio-community\",\n      \"arch\": \"llama\",\n      \"compatibility_type\": \"gguf\",\n      \"quantization\": \"Q4_K_M\",\n      \"state\": \"not-loaded\",\n      \"max_context_length\": 131072\n    },\n    {\n      \"id\": \"text-embedding-nomic-embed-text-v1.5\",\n      \"object\": \"model\",\n      \"type\": \"embeddings\",\n      \"publisher\": \"nomic-ai\",\n      \"arch\": \"nomic-bert\",\n      \"compatibility_type\": \"gguf\",\n      \"quantization\": \"Q4_0\",\n      \"state\": \"not-loaded\",\n      \"max_context_length\": 2048\n    }\n  ]\n}\n```\n\n---\n\n### `GET /api/v0/models/{model}`\n\nGet info about one specific model\n\n**Example request**\n\n```bash\ncurl http://localhost:1234/api/v0/models/qwen2-vl-7b-instruct\n```\n\n**Response format**\n\n```json\n{\n  \"id\": \"qwen2-vl-7b-instruct\",\n  \"object\": \"model\",\n  \"type\": \"vlm\",\n  \"publisher\": \"mlx-community\",\n  \"arch\": \"qwen2_vl\",\n  \"compatibility_type\": \"mlx\",\n  \"quantization\": \"4bit\",\n  \"state\": \"not-loaded\",\n  \"max_context_length\": 32768\n}\n```\n\n---\n\n### `POST /api/v0/chat/completions`\n\nChat Completions API. You provide a messages array and receive the next assistant response in the chat.\n\n**Example request**\n\n```bash\ncurl http://localhost:1234/api/v0/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"granite-3.0-2b-instruct\",\n    \"messages\": [\n      { \"role\": \"system\", \"content\": \"Always answer in rhymes.\" },\n      { \"role\": \"user\", \"content\": \"Introduce yourself.\" }\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": -1,\n    \"stream\": false\n  }'\n```\n\n**Response format**\n\n```json\n{\n  \"id\": \"chatcmpl-i3gkjwthhw96whukek9tz\",\n  \"object\": \"chat.completion\",\n  \"created\": 1731990317,\n  \"model\": \"granite-3.0-2b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"logprobs\": null,\n      \"finish_reason\": \"stop\",\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Greetings, I'm a helpful AI, here to assist,\\nIn providing answers, with no distress.\\nI'll keep it short and sweet, in rhyme you'll find,\\nA friendly companion, all day long you'll bind.\"\n      }\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 24,\n    \"completion_tokens\": 53,\n    \"total_tokens\": 77\n  },\n  \"stats\": {\n    \"tokens_per_second\": 51.43709529007664,\n    \"time_to_first_token\": 0.111,\n    \"generation_time\": 0.954,\n    \"stop_reason\": \"eosFound\"\n  },\n  \"model_info\": {\n    \"arch\": \"granite\",\n    \"quant\": \"Q4_K_M\",\n    \"format\": \"gguf\",\n    \"context_length\": 4096\n  },\n  \"runtime\": {\n    \"name\": \"llama.cpp-mac-arm64-apple-metal-advsimd\",\n    \"version\": \"1.3.0\",\n    \"supported_formats\": [\"gguf\"]\n  }\n}\n```\n\n---\n\n### `POST /api/v0/completions`\n\nText Completions API. You provide a prompt and receive a completion.\n\n**Example request**\n\n```bash\ncurl http://localhost:1234/api/v0/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"granite-3.0-2b-instruct\",\n    \"prompt\": \"the meaning of life is\",\n    \"temperature\": 0.7,\n    \"max_tokens\": 10,\n    \"stream\": false,\n    \"stop\": \"\\n\"\n  }'\n```\n\n**Response format**\n\n```json\n{\n  \"id\": \"cmpl-p9rtxv6fky2v9k8jrd8cc\",\n  \"object\": \"text_completion\",\n  \"created\": 1731990488,\n  \"model\": \"granite-3.0-2b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"text\": \" to find your purpose, and once you have\",\n      \"logprobs\": null,\n      \"finish_reason\": \"length\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"completion_tokens\": 9,\n    \"total_tokens\": 14\n  },\n  \"stats\": {\n    \"tokens_per_second\": 57.69230769230769,\n    \"time_to_first_token\": 0.299,\n    \"generation_time\": 0.156,\n    \"stop_reason\": \"maxPredictedTokensReached\"\n  },\n  \"model_info\": {\n    \"arch\": \"granite\",\n    \"quant\": \"Q4_K_M\",\n    \"format\": \"gguf\",\n    \"context_length\": 4096\n  },\n  \"runtime\": {\n    \"name\": \"llama.cpp-mac-arm64-apple-metal-advsimd\",\n    \"version\": \"1.3.0\",\n    \"supported_formats\": [\"gguf\"]\n  }\n}\n```\n\n---\n\n### `POST /api/v0/embeddings`\n\nText Embeddings API. You provide a text and a representation of the text as an embedding vector is returned.\n\n**Example request**\n\n```bash\ncurl http://localhost:1234/api/v0/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-nomic-embed-text-v1.5\",\n    \"input\": \"Some text to embed\"\n  }\n```\n\n**Example response**\n\n```json\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"embedding\": [\n        -0.016731496900320053,\n        0.028460891917347908,\n        -0.1407836228609085,\n        ... (truncated for brevity) ...,\n        0.02505224384367466,\n        -0.0037634256295859814,\n        -0.04341062530875206\n      ],\n      \"index\": 0\n    }\n  ],\n  \"model\": \"text-embedding-nomic-embed-text-v1.5@q4_k_m\",\n  \"usage\": {\n    \"prompt_tokens\": 0,\n    \"total_tokens\": 0\n  }\n}\n```\n\n---\n\nPlease report bugs by opening an issue on [Github](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues).\n"])</script><script>self.__next_f.push([1,"2a:T90a,"])</script><script>self.__next_f.push([1,"\n### Supported endpoints\n\n\u003ctable class=\"flexible-cols\"\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eEndpoint\u003c/th\u003e\n      \u003cth\u003eMethod\u003c/th\u003e\n      \u003cth\u003eDocs\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode\u003e/v1/models\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003capimethod method=\"GET\" /\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003ca href=\"/docs/developer/openai-compat/models\"\u003eModels\u003c/a\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode\u003e/v1/responses\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003capimethod method=\"POST\" /\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003ca href=\"/docs/developer/openai-compat/responses\"\u003eResponses\u003c/a\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode\u003e/v1/chat/completions\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003capimethod method=\"POST\" /\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003ca href=\"/docs/developer/openai-compat/chat-completions\"\u003eChat Completions\u003c/a\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode\u003e/v1/embeddings\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003capimethod method=\"POST\" /\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003ca href=\"/docs/developer/openai-compat/embeddings\"\u003eEmbeddings\u003c/a\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode\u003e/v1/completions\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003capimethod method=\"POST\" /\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003ca href=\"/docs/developer/openai-compat/completions\"\u003eCompletions\u003c/a\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003chr\u003e\n\n## Set the `base url` to point to LM Studio\n\nYou can reuse existing OpenAI clients (in Python, JS, C#, etc) by switching up the \"base URL\" property to point to your LM Studio instead of OpenAI's servers.\n\nNote: The following examples assume the server port is `1234`\n\n### Python Example\n\n```diff\nfrom openai import OpenAI\n\nclient = OpenAI(\n+    base_url=\"http://localhost:1234/v1\"\n)\n\n# ... the rest of your code ...\n```\n\n### Typescript Example\n\n```diff\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n+  baseUrl: \"http://localhost:1234/v1\"\n});\n\n// ... the rest of your code ...\n```\n\n### cURL Example\n\n```diff\n- curl https://api.openai.com/v1/chat/completions \\\n+ curl http://localhost:1234/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n-     \"model\": \"gpt-4o-mini\",\n+     \"model\": \"use the model identifier from LM Studio here\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n---\n\nOther OpenAI client libraries should have similar options to set the base URL.\n\nIf you're running into trouble, hop onto our [Discord](https://discord.gg/lmstudio) and enter the `#ðŸ”¨-developers` channel.\n"])</script><script>self.__next_f.push([1,"2b:T122d,"])</script><script>self.__next_f.push([1,"\nYou can enforce a particular response format from an LLM by providing a JSON schema to the `/v1/chat/completions` endpoint, via LM Studio's REST API (or via any OpenAI client).\n\n\u003chr\u003e\n\n### Start LM Studio as a server\n\nTo use LM Studio programmatically from your own code, run LM Studio as a local server.\n\nYou can turn on the server from the \"Developer\" tab in LM Studio, or via the `lms` CLI:\n\n```\nlms server start\n```\n\n###### Install `lms` by running `npx lmstudio install-cli`\n\nThis will allow you to interact with LM Studio via the REST API. For an intro to LM Studio's REST API, see [REST API Overview](/docs/developer/rest).\n\n### Structured Output\n\nThe API supports structured JSON outputs through the `/v1/chat/completions` endpoint when given a [JSON schema](https://json-schema.org/overview/what-is-jsonschema). Doing this will cause the LLM to respond in valid JSON conforming to the schema provided.\n\nIt follows the same format as OpenAI's recently announced [Structured Output](https://platform.openai.com/docs/guides/structured-outputs) API and is expected to work via the OpenAI client SDKs.\n\n**Example using `curl`**\n\nThis example demonstrates a structured output request using the `curl` utility.\n\nTo run this example on Mac or Linux, use any terminal. On Windows, use [Git Bash](https://git-scm.com/download/win).\n\n```bash\ncurl http://localhost:1234/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"{{model}}\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful jokester.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Tell me a joke.\"\n      }\n    ],\n    \"response_format\": {\n      \"type\": \"json_schema\",\n      \"json_schema\": {\n        \"name\": \"joke_response\",\n        \"strict\": \"true\",\n        \"schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"joke\": {\n              \"type\": \"string\"\n            }\n          },\n          \"required\": [\"joke\"]\n        }\n      }\n    },\n    \"temperature\": 0.7,\n    \"max_tokens\": 50,\n    \"stream\": false\n  }'\n```\n\nAll parameters recognized by `/v1/chat/completions` will be honored, and the JSON schema should be provided in the `json_schema` field of `response_format`.\n\nThe JSON object will be provided in `string` form in the typical response field, `choices[0].message.content`, and will need to be parsed into a JSON object.\n\n**Example using `python`**\n\n```python\nfrom openai import OpenAI\nimport json\n\n# Initialize OpenAI client that points to the local LM Studio server\nclient = OpenAI(\n    base_url=\"http://localhost:1234/v1\",\n    api_key=\"lm-studio\"\n)\n\n# Define the conversation with the AI\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n    {\"role\": \"user\", \"content\": \"Create 1-3 fictional characters\"}\n]\n\n# Define the expected response structure\ncharacter_schema = {\n    \"type\": \"json_schema\",\n    \"json_schema\": {\n        \"name\": \"characters\",\n        \"schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"characters\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"name\": {\"type\": \"string\"},\n                            \"occupation\": {\"type\": \"string\"},\n                            \"personality\": {\"type\": \"string\"},\n                            \"background\": {\"type\": \"string\"}\n                        },\n                        \"required\": [\"name\", \"occupation\", \"personality\", \"background\"]\n                    },\n                    \"minItems\": 1,\n                }\n            },\n            \"required\": [\"characters\"]\n        },\n    }\n}\n\n# Get response from AI\nresponse = client.chat.completions.create(\n    model=\"your-model\",\n    messages=messages,\n    response_format=character_schema,\n)\n\n# Parse and display the results\nresults = json.loads(response.choices[0].message.content)\nprint(json.dumps(results, indent=2))\n```\n\n**Important**: Not all models are capable of structured output, particularly LLMs below 7B parameters.\n\nCheck the model card README if you are unsure if the model supports structured output.\n\n### Structured output engine\n\n- For `GGUF` models: utilize `llama.cpp`'s grammar-based sampling APIs.\n- For `MLX` models: using [Outlines](https://github.com/dottxt-ai/outlines).\n\nThe MLX implementation is available on Github: [lmstudio-ai/mlx-engine](https://github.com/lmstudio-ai/mlx-engine).\n\n\u003chr\u003e\n\n### Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n"])</script><script>self.__next_f.push([1,"2c:Ta108,"])</script><script>self.__next_f.push([1,"\nTool use enables LLMs to request calls to external functions and APIs through the `/v1/chat/completions` and `v1/responses` endpoints ([Learn more](/docs/developer/openai-compat)), via LM Studio's REST API (or via any OpenAI client). This expands their functionality far beyond text output.\n\n\u003chr\u003e\n\n## Quick Start\n\n### 1. Start LM Studio as a server\n\nTo use LM Studio programmatically from your own code, run LM Studio as a local server.\n\nYou can turn on the server from the \"Developer\" tab in LM Studio, or via the `lms` CLI:\n\n```bash\nlms server start\n```\n\n###### Install `lms` by running `npx lmstudio install-cli`\n\nThis will allow you to interact with LM Studio via the REST API. For an intro to LM Studio's REST API, see [REST API Overview](/docs/developer/rest).\n\n### 2. Load a Model\n\nYou can load a model from the \"Chat\" or \"Developer\" tabs in LM Studio, or via the `lms` CLI:\n\n```bash\nlms load\n```\n\n### 3. Copy, Paste, and Run an Example!\n\n- `Curl`\n  - [Single Turn Tool Call Request](#example-using-curl)\n- `Python`\n  - [Single Turn Tool Call + Tool Use](#single-turn-example)\n  - [Multi-Turn Example](#multi-turn-example)\n  - [Advanced Agent Example](#advanced-agent-example)\n\n## Tool Use\n\n### What really is \"Tool Use\"?\n\nTool use describes:\n\n- LLMs output text requesting functions to be called (LLMs cannot directly execute code)\n- Your code executes those functions\n- Your code feeds the results back to the LLM.\n\n### High-level flow\n\n```xml\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ SETUP: LLM + Tool list   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚    Get user input        â”‚â—„â”€â”€â”€â”€â”\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n           â–¼                     â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\nâ”‚ LLM prompted w/messages  â”‚     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n           â–¼                     â”‚\n     Needs tools?                â”‚\n      â”‚         â”‚                â”‚\n    Yes         No               â”‚\n      â”‚         â”‚                â”‚\n      â–¼         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚\nâ”‚Tool Responseâ”‚              â”‚   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚\n       â–¼                     â”‚   â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚\nâ”‚Execute toolsâ”‚              â”‚   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚\n       â–¼                     â–¼   â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Add results  â”‚          â”‚  Normal   â”‚\nâ”‚to messages  â”‚          â”‚ response  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n       â”‚                       â–²\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### In-depth flow\n\nLM Studio supports tool use through the `/v1/chat/completions` endpoint when given function definitions in the `tools` parameter of the request body. Tools are specified as an array of function definitions that describe their parameters and usage, like:\n\nIt follows the same format as OpenAI's [Function Calling](https://platform.openai.com/docs/guides/function-calling) API and is expected to work via the OpenAI client SDKs.\n\nWe will use [lmstudio-community/Qwen2.5-7B-Instruct-GGUF](https://model.lmstudio.ai/download/lmstudio-community/Qwen2.5-7B-Instruct-GGUF) as the model in this example flow.\n\n1. You provide a list of tools to an LLM. These are the tools that the model can _request_ calls to.\n   For example:\n\n```json\n// the list of tools is model-agnostic\n[\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"get_delivery_date\",\n      \"description\": \"Get the delivery date for a customer's order\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"order_id\": {\n            \"type\": \"string\"\n          }\n        },\n        \"required\": [\"order_id\"]\n      }\n    }\n  }\n]\n```\n\nThis list will be injected into the `system` prompt of the model depending on the model's chat template. For `Qwen2.5-Instruct`, this looks like:\n\n```json\n\u003c|im_start|\u003esystem\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within \u003ctools\u003e\u003c/tools\u003e XML tags:\n\u003ctools\u003e\n{\"type\": \"function\", \"function\": {\"name\": \"get_delivery_date\", \"description\": \"Get the delivery date for a customer's order\", \"parameters\": {\"type\": \"object\", \"properties\": {\"order_id\": {\"type\": \"string\"}}, \"required\": [\"order_id\"]}}}\n\u003c/tools\u003e\n\nFor each function call, return a json object with function name and arguments within \u003ctool_call\u003e\u003c/tool_call\u003e XML tags:\n\u003ctool_call\u003e\n{\"name\": \u003cfunction-name\u003e, \"arguments\": \u003cargs-json-object\u003e}\n\u003c/tool_call\u003e\u003c|im_end|\u003e\n```\n\n**Important**: The model can only _request_ calls to these tools because LLMs _cannot_ directly call functions, APIs, or any other tools. They can only output text, which can then be parsed to programmatically call the functions.\n\n2. When prompted, the LLM can then decide to either:\n\n   - (a) Call one or more tools\n\n   ```xml\n   User: Get me the delivery date for order 123\n   Model: \u003ctool_call\u003e\n   {\"name\": \"get_delivery_date\", \"arguments\": {\"order_id\": \"123\"}}\n   \u003c/tool_call\u003e\n   ```\n\n   - (b) Respond normally\n\n   ```xml\n   User: Hi\n   Model: Hello! How can I assist you today?\n   ```\n\n3. LM Studio parses the text output from the model into an OpenAI-compliant `chat.completion` response object.\n\n   - If the model was given access to `tools`, LM Studio will attempt to parse the tool calls into the `response.choices[0].message.tool_calls` field of the `chat.completion` response object.\n   - If LM Studio cannot parse any **correctly formatted** tool calls, it will simply return the response to the standard `response.choices[0].message.content` field.\n   - **Note**: Smaller models and models that were not trained for tool use may output improperly formatted tool calls, resulting in LM Studio being unable to parse them into the `tool_calls` field. This is useful for troubleshooting when you do not receive `tool_calls` as expected. Example of an improperly formatting `Qwen2.5-Instruct` tool call:\n\n   ```xml\n   \u003ctool_call\u003e\n   [\"name\": \"get_delivery_date\", function: \"date\"]\n   \u003c/tool_call\u003e\n   ```\n\n   \u003e Note that the brackets are incorrect, and the call does not follow the `name, argument` format.\n\n4. Your code parses the `chat.completion` response to check for tool calls from the model, then calls the appropriate tools with the parameters specified by the model. Your code then adds both:\n\n   1. The model's tool call message\n   2. The result of the tool call\n\n   To the `messages` array to send back to the model\n\n   ```python\n   # pseudocode, see examples for copy-paste snippets\n   if response.has_tool_calls:\n       for each tool_call:\n           # Extract function name \u0026 args\n           function_to_call = tool_call.name     # e.g. \"get_delivery_date\"\n           args = tool_call.arguments            # e.g. {\"order_id\": \"123\"}\n\n           # Execute the function\n           result = execute_function(function_to_call, args)\n\n           # Add result to conversation\n           add_to_messages([\n               ASSISTANT_TOOL_CALL_MESSAGE,      # The request to use the tool\n               TOOL_RESULT_MESSAGE               # The tool's response\n           ])\n   else:\n       # Normal response without tools\n       add_to_messages(response.content)\n   ```\n\n5. The LLM is then prompted again with the updated messages array, but without access to tools. This is because:\n   - The LLM already has the tool results in the conversation history\n   - We want the LLM to provide a final response to the user, not call more tools\n   ```python\n   # Example messages\n   messages = [\n       {\"role\": \"user\", \"content\": \"When will order 123 be delivered?\"},\n       {\"role\": \"assistant\", \"function_call\": {\n           \"name\": \"get_delivery_date\",\n           \"arguments\": {\"order_id\": \"123\"}\n       }},\n       {\"role\": \"tool\", \"content\": \"2024-03-15\"},\n   ]\n   response = client.chat.completions.create(\n       model=\"lmstudio-community/qwen2.5-7b-instruct\",\n       messages=messages\n   )\n   ```\n   The `response.choices[0].message.content` field after this call may be something like:\n   ```xml\n   Your order #123 will be delivered on March 15th, 2024\n   ```\n6. The loop continues back at step 2 of the flow\n\nNote: This is the `pedantic` flow for tool use. However, you can certainly experiment with this flow to best fit your use case.\n\n## Supported Models\n\nThrough LM Studio, **all** models support at least some degree of tool use.\n\nHowever, there are currently two levels of support that may impact the quality of the experience: Native and Default.\n\nModels with Native tool use support will have a hammer badge in the app, and generally perform better in tool use scenarios.\n\n### Native tool use support\n\n\"Native\" tool use support means that both:\n\n1. The model has a chat template that supports tool use (usually means the model has been trained for tool use)\n   - This is what will be used to format the `tools` array into the system prompt and tell them model how to format tool calls\n   - Example: [Qwen2.5-Instruct chat template](https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit/blob/c26a38f6a37d0a51b4e9a1eb3026530fa35d9fed/tokenizer_config.json#L197)\n2. LM Studio supports that model's tool use format\n   - Required for LM Studio to properly input the chat history into the chat template, and parse the tool calls the model outputs into the `chat.completion` object\n\nModels that currently have native tool use support in LM Studio (subject to change):\n\n- Qwen\n  - `GGUF` [lmstudio-community/Qwen2.5-7B-Instruct-GGUF](https://model.lmstudio.ai/download/lmstudio-community/Qwen2.5-7B-Instruct-GGUF) (4.68 GB)\n  - `MLX` [mlx-community/Qwen2.5-7B-Instruct-4bit](https://model.lmstudio.ai/download/mlx-community/Qwen2.5-7B-Instruct-4bit) (4.30 GB)\n- Llama-3.1, Llama-3.2\n  - `GGUF` [lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF](https://model.lmstudio.ai/download/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF) (4.92 GB)\n  - `MLX` [mlx-community/Meta-Llama-3.1-8B-Instruct-8bit](https://model.lmstudio.ai/download/mlx-community/Meta-Llama-3.1-8B-Instruct-8bit) (8.54 GB)\n- Mistral\n  - `GGUF` [bartowski/Ministral-8B-Instruct-2410-GGUF](https://model.lmstudio.ai/download/bartowski/Ministral-8B-Instruct-2410-GGUF) (4.67 GB)\n  - `MLX` [mlx-community/Ministral-8B-Instruct-2410-4bit](https://model.lmstudio.ai/download/mlx-community/Ministral-8B-Instruct-2410-4bit) (4.67 GB GB)\n\n### Default tool use support\n\n\"Default\" tool use support means that **either**:\n\n1. The model does not have chat template that supports tool use (usually means the model has not been trained for tool use)\n2. LM Studio does not currently support that model's tool use format\n\nUnder the hood, default tool use works by:\n\n- Giving models a custom system prompt and a default tool call format to use\n- Converting `tool` role messages to the `user` role so that chat templates without the `tool` role are compatible\n- Converting `assistant` role `tool_calls` into the default tool call format\n\nResults will vary by model.\n\nYou can see the default format by running `lms log stream` in your terminal, then sending a chat completion request with `tools` to a model that doesn't have Native tool use support. The default format is subject to change.\n\n\u003cdetails\u003e\n\u003csummary\u003eExpand to see example of default tool use format\u003c/summary\u003e\n\n```bash\n-\u003e % lms log stream\nStreaming logs from LM Studio\n\ntimestamp: 11/13/2024, 9:35:15 AM\ntype: llm.prediction.input\nmodelIdentifier: gemma-2-2b-it\nmodelPath: lmstudio-community/gemma-2-2b-it-GGUF/gemma-2-2b-it-Q4_K_M.gguf\ninput: \"\u003cstart_of_turn\u003esystem\nYou are a tool-calling AI. You can request calls to available tools with this EXACT format:\n[TOOL_REQUEST]{\"name\": \"tool_name\", \"arguments\": {\"param1\": \"value1\"}}[END_TOOL_REQUEST]\n\nAVAILABLE TOOLS:\n{\n  \"type\": \"toolArray\",\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_delivery_date\",\n        \"description\": \"Get the delivery date for a customer's order\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"order_id\": {\n              \"type\": \"string\"\n            }\n          },\n          \"required\": [\n            \"order_id\"\n          ]\n        }\n      }\n    }\n  ]\n}\n\nRULES:\n- Only use tools from AVAILABLE TOOLS\n- Include all required arguments\n- Use one [TOOL_REQUEST] block per tool\n- Never use [TOOL_RESULT]\n- If you decide to call one or more tools, there should be no other text in your message\n\nExamples:\n\"Check Paris weather\"\n[TOOL_REQUEST]{\"name\": \"get_weather\", \"arguments\": {\"location\": \"Paris\"}}[END_TOOL_REQUEST]\n\n\"Send email to John about meeting and open browser\"\n[TOOL_REQUEST]{\"name\": \"send_email\", \"arguments\": {\"to\": \"John\", \"subject\": \"meeting\"}}[END_TOOL_REQUEST]\n[TOOL_REQUEST]{\"name\": \"open_browser\", \"arguments\": {}}[END_TOOL_REQUEST]\n\nRespond conversationally if no matching tools exist.\u003cend_of_turn\u003e\n\u003cstart_of_turn\u003euser\nGet me delivery date for order 123\u003cend_of_turn\u003e\n\u003cstart_of_turn\u003emodel\n\"\n```\n\nIf the model follows this format exactly to call tools, i.e:\n\n```\n[TOOL_REQUEST]{\"name\": \"get_delivery_date\", \"arguments\": {\"order_id\": \"123\"}}[END_TOOL_REQUEST]\n```\n\nThen LM Studio will be able to parse those tool calls into the `chat.completions` object, just like for natively supported models.\n\n\u003c/details\u003e\n\nAll models that don't have native tool use support will have default tool use support.\n\n## Example using `curl`\n\nThis example demonstrates a model requesting a tool call using the `curl` utility.\n\nTo run this example on Mac or Linux, use any terminal. On Windows, use [Git Bash](https://git-scm.com/download/win).\n\n```bash\ncurl http://localhost:1234/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"lmstudio-community/qwen2.5-7b-instruct\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What dell products do you have under $50 in electronics?\"}],\n    \"tools\": [\n      {\n        \"type\": \"function\",\n        \"function\": {\n          \"name\": \"search_products\",\n          \"description\": \"Search the product catalog by various criteria. Use this whenever a customer asks about product availability, pricing, or specifications.\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"query\": {\n                \"type\": \"string\",\n                \"description\": \"Search terms or product name\"\n              },\n              \"category\": {\n                \"type\": \"string\",\n                \"description\": \"Product category to filter by\",\n                \"enum\": [\"electronics\", \"clothing\", \"home\", \"outdoor\"]\n              },\n              \"max_price\": {\n                \"type\": \"number\",\n                \"description\": \"Maximum price in dollars\"\n              }\n            },\n            \"required\": [\"query\"],\n            \"additionalProperties\": false\n          }\n        }\n      }\n    ]\n  }'\n```\n\nAll parameters recognized by `/v1/chat/completions` will be honored, and the array of available tools should be provided in the `tools` field.\n\nIf the model decides that the user message would be best fulfilled with a tool call, an array of tool call request objects will be provided in the response field, `choices[0].message.tool_calls`.\n\nThe `finish_reason` field of the top-level response object will also be populated with `\"tool_calls\"`.\n\nAn example response to the above `curl` request will look like:\n\n```bash\n{\n  \"id\": \"chatcmpl-gb1t1uqzefudice8ntxd9i\",\n  \"object\": \"chat.completion\",\n  \"created\": 1730913210,\n  \"model\": \"lmstudio-community/qwen2.5-7b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"logprobs\": null,\n      \"finish_reason\": \"tool_calls\",\n      \"message\": {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n          {\n            \"id\": \"365174485\",\n            \"type\": \"function\",\n            \"function\": {\n              \"name\": \"search_products\",\n              \"arguments\": \"{\\\"query\\\":\\\"dell\\\",\\\"category\\\":\\\"electronics\\\",\\\"max_price\\\":50}\"\n            }\n          }\n        ]\n      }\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 263,\n    \"completion_tokens\": 34,\n    \"total_tokens\": 297\n  },\n  \"system_fingerprint\": \"lmstudio-community/qwen2.5-7b-instruct\"\n}\n```\n\nIn plain english, the above response can be thought of as the model saying:\n\n\u003e \"Please call the `search_products` function, with arguments:\n\u003e\n\u003e - 'dell' for the `query` parameter,\n\u003e - 'electronics' for the `category` parameter\n\u003e - '50' for the `max_price` parameter\n\u003e\n\u003e and give me back the results\"\n\nThe `tool_calls` field will need to be parsed to call actual functions/APIs. The below examples demonstrate how.\n\n## Examples using `python`\n\nTool use shines when paired with program languages like python, where you can implement the functions specified in the `tools` field to programmatically call them when the model requests.\n\n### Single-turn example\n\nBelow is a simple single-turn (model is only called once) example of enabling a model to call a function called `say_hello` that prints a hello greeting to the console:\n\n`single-turn-example.py`\n\n```python\nfrom openai import OpenAI\n\n# Connect to LM Studio\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\n# Define a simple function\ndef say_hello(name: str) -\u003e str:\n    print(f\"Hello, {name}!\")\n\n# Tell the AI about our function\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"say_hello\",\n            \"description\": \"Says hello to someone\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"name\": {\n                        \"type\": \"string\",\n                        \"description\": \"The person's name\"\n                    }\n                },\n                \"required\": [\"name\"]\n            }\n        }\n    }\n]\n\n# Ask the AI to use our function\nresponse = client.chat.completions.create(\n    model=\"lmstudio-community/qwen2.5-7b-instruct\",\n    messages=[{\"role\": \"user\", \"content\": \"Can you say hello to Bob the Builder?\"}],\n    tools=tools\n)\n\n# Get the name the AI wants to use a tool to say hello to\n# (Assumes the AI has requested a tool call and that tool call is say_hello)\ntool_call = response.choices[0].message.tool_calls[0]\nname = eval(tool_call.function.arguments)[\"name\"]\n\n# Actually call the say_hello function\nsay_hello(name) # Prints: Hello, Bob the Builder!\n\n```\n\nRunning this script from the console should yield results like:\n\n```xml\n-\u003e % python single-turn-example.py\nHello, Bob the Builder!\n```\n\nPlay around with the name in\n\n```python\nmessages=[{\"role\": \"user\", \"content\": \"Can you say hello to Bob the Builder?\"}]\n```\n\nto see the model call the `say_hello` function with different names.\n\n### Multi-turn example\n\nNow for a slightly more complex example.\n\nIn this example, we'll:\n\n1. Enable the model to call a `get_delivery_date` function\n2. Hand the result of calling that function back to the model, so that it can fulfill the user's request in plain text\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003ccode\u003emulti-turn-example.py\u003c/code\u003e (click to expand) \u003c/summary\u003e\n\n```python\nfrom datetime import datetime, timedelta\nimport json\nimport random\nfrom openai import OpenAI\n\n# Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\nmodel = \"lmstudio-community/qwen2.5-7b-instruct\"\n\n\ndef get_delivery_date(order_id: str) -\u003e datetime:\n    # Generate a random delivery date between today and 14 days from now\n    # in a real-world scenario, this function would query a database or API\n    today = datetime.now()\n    random_days = random.randint(1, 14)\n    delivery_date = today + timedelta(days=random_days)\n    print(\n        f\"\\nget_delivery_date function returns delivery date:\\n\\n{delivery_date}\",\n        flush=True,\n    )\n    return delivery_date\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_delivery_date\",\n            \"description\": \"Get the delivery date for a customer's order. Call this whenever you need to know the delivery date, for example when a customer asks 'Where is my package'\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"order_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The customer's order ID.\",\n                    },\n                },\n                \"required\": [\"order_id\"],\n                \"additionalProperties\": False,\n            },\n        },\n    }\n]\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful customer support assistant. Use the supplied tools to assist the user.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Give me the delivery date and time for order number 1017\",\n    },\n]\n\n# LM Studio\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    tools=tools,\n)\n\nprint(\"\\nModel response requesting tool call:\\n\", flush=True)\nprint(response, flush=True)\n\n# Extract the arguments for get_delivery_date\n# Note this code assumes we have already determined that the model generated a function call.\ntool_call = response.choices[0].message.tool_calls[0]\narguments = json.loads(tool_call.function.arguments)\n\norder_id = arguments.get(\"order_id\")\n\n# Call the get_delivery_date function with the extracted order_id\ndelivery_date = get_delivery_date(order_id)\n\nassistant_tool_call_request_message = {\n    \"role\": \"assistant\",\n    \"tool_calls\": [\n        {\n            \"id\": response.choices[0].message.tool_calls[0].id,\n            \"type\": response.choices[0].message.tool_calls[0].type,\n            \"function\": response.choices[0].message.tool_calls[0].function,\n        }\n    ],\n}\n\n# Create a message containing the result of the function call\nfunction_call_result_message = {\n    \"role\": \"tool\",\n    \"content\": json.dumps(\n        {\n            \"order_id\": order_id,\n            \"delivery_date\": delivery_date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        }\n    ),\n    \"tool_call_id\": response.choices[0].message.tool_calls[0].id,\n}\n\n# Prepare the chat completion call payload\ncompletion_messages_payload = [\n    messages[0],\n    messages[1],\n    assistant_tool_call_request_message,\n    function_call_result_message,\n]\n\n# Call the OpenAI API's chat completions endpoint to send the tool call result back to the model\n# LM Studio\nresponse = client.chat.completions.create(\n    model=model,\n    messages=completion_messages_payload,\n)\n\nprint(\"\\nFinal model response with knowledge of the tool call result:\\n\", flush=True)\nprint(response.choices[0].message.content, flush=True)\n\n```\n\n\u003c/details\u003e\n\nRunning this script from the console should yield results like:\n\n```xml\n-\u003e % python multi-turn-example.py\n\nModel response requesting tool call:\n\nChatCompletion(id='chatcmpl-wwpstqqu94go4hvclqnpwn', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='377278620', function=Function(arguments='{\"order_id\":\"1017\"}', name='get_delivery_date'), type='function')]))], created=1730916196, model='lmstudio-community/qwen2.5-7b-instruct', object='chat.completion', service_tier=None, system_fingerprint='lmstudio-community/qwen2.5-7b-instruct', usage=CompletionUsage(completion_tokens=24, prompt_tokens=223, total_tokens=247, completion_tokens_details=None, prompt_tokens_details=None))\n\nget_delivery_date function returns delivery date:\n\n2024-11-19 13:03:17.773298\n\nFinal model response with knowledge of the tool call result:\n\nYour order number 1017 is scheduled for delivery on November 19, 2024, at 13:03 PM.\n```\n\n### Advanced agent example\n\nBuilding upon the principles above, we can combine LM Studio models with locally defined functions to create an \"agent\" - a system that pairs a language model with custom functions to understand requests and perform actions beyond basic text generation.\n\nThe agent in the below example can:\n\n1. Open safe urls in your default browser\n2. Check the current time\n3. Analyze directories in your file system\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003ccode\u003eagent-chat-example.py\u003c/code\u003e (click to expand) \u003c/summary\u003e\n\n```python\nimport json\nfrom urllib.parse import urlparse\nimport webbrowser\nfrom datetime import datetime\nimport os\nfrom openai import OpenAI\n\n# Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\nmodel = \"lmstudio-community/qwen2.5-7b-instruct\"\n\n\ndef is_valid_url(url: str) -\u003e bool:\n\n    try:\n        result = urlparse(url)\n        return bool(result.netloc)  # Returns True if there's a valid network location\n    except Exception:\n        return False\n\n\ndef open_safe_url(url: str) -\u003e dict:\n    # List of allowed domains (expand as needed)\n    SAFE_DOMAINS = {\n        \"lmstudio.ai\",\n        \"github.com\",\n        \"google.com\",\n        \"wikipedia.org\",\n        \"weather.com\",\n        \"stackoverflow.com\",\n        \"python.org\",\n        \"docs.python.org\",\n    }\n\n    try:\n        # Add http:// if no scheme is present\n        if not url.startswith(('http://', 'https://')):\n            url = 'http://' + url\n\n        # Validate URL format\n        if not is_valid_url(url):\n            return {\"status\": \"error\", \"message\": f\"Invalid URL format: {url}\"}\n\n        # Parse the URL and check domain\n        parsed_url = urlparse(url)\n        domain = parsed_url.netloc.lower()\n        base_domain = \".\".join(domain.split(\".\")[-2:])\n\n        if base_domain in SAFE_DOMAINS:\n            webbrowser.open(url)\n            return {\"status\": \"success\", \"message\": f\"Opened {url} in browser\"}\n        else:\n            return {\n                \"status\": \"error\",\n                \"message\": f\"Domain {domain} not in allowed list\",\n            }\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": str(e)}\n\n\ndef get_current_time() -\u003e dict:\n    \"\"\"Get the current system time with timezone information\"\"\"\n    try:\n        current_time = datetime.now()\n        timezone = datetime.now().astimezone().tzinfo\n        formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n        return {\n            \"status\": \"success\",\n            \"time\": formatted_time,\n            \"timezone\": str(timezone),\n            \"timestamp\": current_time.timestamp(),\n        }\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": str(e)}\n\n\ndef analyze_directory(path: str = \".\") -\u003e dict:\n    \"\"\"Count and categorize files in a directory\"\"\"\n    try:\n        stats = {\n            \"total_files\": 0,\n            \"total_dirs\": 0,\n            \"file_types\": {},\n            \"total_size_bytes\": 0,\n        }\n\n        for entry in os.scandir(path):\n            if entry.is_file():\n                stats[\"total_files\"] += 1\n                ext = os.path.splitext(entry.name)[1].lower() or \"no_extension\"\n                stats[\"file_types\"][ext] = stats[\"file_types\"].get(ext, 0) + 1\n                stats[\"total_size_bytes\"] += entry.stat().st_size\n            elif entry.is_dir():\n                stats[\"total_dirs\"] += 1\n                # Add size of directory contents\n                for root, _, files in os.walk(entry.path):\n                    for file in files:\n                        try:\n                            stats[\"total_size_bytes\"] += os.path.getsize(os.path.join(root, file))\n                        except (OSError, FileNotFoundError):\n                            continue\n\n        return {\"status\": \"success\", \"stats\": stats, \"path\": os.path.abspath(path)}\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": str(e)}\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"open_safe_url\",\n            \"description\": \"Open a URL in the browser if it's deemed safe\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"url\": {\n                        \"type\": \"string\",\n                        \"description\": \"The URL to open\",\n                    },\n                },\n                \"required\": [\"url\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_time\",\n            \"description\": \"Get the current system time with timezone information\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_directory\",\n            \"description\": \"Analyze the contents of a directory, counting files and folders\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"path\": {\n                        \"type\": \"string\",\n                        \"description\": \"The directory path to analyze. Defaults to current directory if not specified.\",\n                    },\n                },\n                \"required\": [],\n            },\n        },\n    },\n]\n\n\ndef process_tool_calls(response, messages):\n    \"\"\"Process multiple tool calls and return the final response and updated messages\"\"\"\n    # Get all tool calls from the response\n    tool_calls = response.choices[0].message.tool_calls\n\n    # Create the assistant message with tool calls\n    assistant_tool_call_message = {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n            {\n                \"id\": tool_call.id,\n                \"type\": tool_call.type,\n                \"function\": tool_call.function,\n            }\n            for tool_call in tool_calls\n        ],\n    }\n\n    # Add the assistant's tool call message to the history\n    messages.append(assistant_tool_call_message)\n\n    # Process each tool call and collect results\n    tool_results = []\n    for tool_call in tool_calls:\n        # For functions with no arguments, use empty dict\n        arguments = (\n            json.loads(tool_call.function.arguments)\n            if tool_call.function.arguments.strip()\n            else {}\n        )\n\n        # Determine which function to call based on the tool call name\n        if tool_call.function.name == \"open_safe_url\":\n            result = open_safe_url(arguments[\"url\"])\n        elif tool_call.function.name == \"get_current_time\":\n            result = get_current_time()\n        elif tool_call.function.name == \"analyze_directory\":\n            path = arguments.get(\"path\", \".\")\n            result = analyze_directory(path)\n        else:\n            # llm tried to call a function that doesn't exist, skip\n            continue\n\n        # Add the result message\n        tool_result_message = {\n            \"role\": \"tool\",\n            \"content\": json.dumps(result),\n            \"tool_call_id\": tool_call.id,\n        }\n        tool_results.append(tool_result_message)\n        messages.append(tool_result_message)\n\n    # Get the final response\n    final_response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n    )\n\n    return final_response\n\n\ndef chat():\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that can open safe web links, tell the current time, and analyze directory contents. Use these capabilities whenever they might be helpful.\",\n        }\n    ]\n\n    print(\n        \"Assistant: Hello! I can help you open safe web links, tell you the current time, and analyze directory contents. What would you like me to do?\"\n    )\n    print(\"(Type 'quit' to exit)\")\n\n    while True:\n        # Get user input\n        user_input = input(\"\\nYou: \").strip()\n\n        # Check for quit command\n        if user_input.lower() == \"quit\":\n            print(\"Assistant: Goodbye!\")\n            break\n\n        # Add user message to conversation\n        messages.append({\"role\": \"user\", \"content\": user_input})\n\n        try:\n            # Get initial response\n            response = client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n            )\n\n            # Check if the response includes tool calls\n            if response.choices[0].message.tool_calls:\n                # Process all tool calls and get final response\n                final_response = process_tool_calls(response, messages)\n                print(\"\\nAssistant:\", final_response.choices[0].message.content)\n\n                # Add assistant's final response to messages\n                messages.append(\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": final_response.choices[0].message.content,\n                    }\n                )\n            else:\n                # If no tool call, just print the response\n                print(\"\\nAssistant:\", response.choices[0].message.content)\n\n                # Add assistant's response to messages\n                messages.append(\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": response.choices[0].message.content,\n                    }\n                )\n\n        except Exception as e:\n            print(f\"\\nAn error occurred: {str(e)}\")\n            exit(1)\n\n\nif __name__ == \"__main__\":\n    chat()\n\n```\n\n\u003c/details\u003e\n\nRunning this script from the console will allow you to chat with the agent:\n\n```xml\n-\u003e % python agent-example.py\nAssistant: Hello! I can help you open safe web links, tell you the current time, and analyze directory contents. What would you like me to do?\n(Type 'quit' to exit)\n\nYou: What time is it?\n\nAssistant: The current time is 14:11:40 (EST) as of November 6, 2024.\n\nYou: What time is it now?\n\nAssistant: The current time is 14:13:59 (EST) as of November 6, 2024.\n\nYou: Open lmstudio.ai\n\nAssistant: The link to lmstudio.ai has been opened in your default web browser.\n\nYou: What's in my current directory?\n\nAssistant: Your current directory at `/Users/matt/project` contains a total of 14 files and 8 directories. Here's the breakdown:\n\n- Files without an extension: 3\n- `.mjs` files: 2\n- `.ts` (TypeScript) files: 3\n- Markdown (`md`) file: 1\n- JSON files: 4\n- TOML file: 1\n\nThe total size of these items is 1,566,990,604 bytes.\n\nYou: Thank you!\n\nAssistant: You're welcome! If you have any other questions or need further assistance, feel free to ask.\n\nYou:\n```\n\n### Streaming\n\nWhen streaming through `/v1/chat/completions` (`stream=true`), tool calls are sent in chunks. Function names and arguments are sent in pieces via `chunk.choices[0].delta.tool_calls.function.name` and `chunk.choices[0].delta.tool_calls.function.arguments`.\n\nFor example, to call `get_current_weather(location=\"San Francisco\")`, the streamed `ChoiceDeltaToolCall` in each `chunk.choices[0].delta.tool_calls[0]` object will look like:\n\n```py\nChoiceDeltaToolCall(index=0, id='814890118', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_weather'), type='function')\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type=None)\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='location', name=None), type=None)\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":\"', name=None), type=None)\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='San Francisco', name=None), type=None)\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type=None)\n```\n\nThese chunks must be accumulated throughout the stream to form the complete function signature for execution.\n\nThe below example shows how to create a simple tool-enhanced chatbot through the `/v1/chat/completions` streaming endpoint (`stream=true`).\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003ccode\u003etool-streaming-chatbot.py\u003c/code\u003e (click to expand) \u003c/summary\u003e\n\n```python\nfrom openai import OpenAI\nimport time\n\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\nMODEL = \"lmstudio-community/qwen2.5-7b-instruct\"\n\nTIME_TOOL = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_time\",\n        \"description\": \"Get the current time, only if asked\",\n        \"parameters\": {\"type\": \"object\", \"properties\": {}},\n    },\n}\n\ndef get_current_time():\n    return {\"time\": time.strftime(\"%H:%M:%S\")}\n\ndef process_stream(stream, add_assistant_label=True):\n    \"\"\"Handle streaming responses from the API\"\"\"\n    collected_text = \"\"\n    tool_calls = []\n    first_chunk = True\n\n    for chunk in stream:\n        delta = chunk.choices[0].delta\n\n        # Handle regular text output\n        if delta.content:\n            if first_chunk:\n                print()\n                if add_assistant_label:\n                    print(\"Assistant:\", end=\" \", flush=True)\n                first_chunk = False\n            print(delta.content, end=\"\", flush=True)\n            collected_text += delta.content\n\n        # Handle tool calls\n        elif delta.tool_calls:\n            for tc in delta.tool_calls:\n                if len(tool_calls) \u003c= tc.index:\n                    tool_calls.append({\n                        \"id\": \"\", \"type\": \"function\",\n                        \"function\": {\"name\": \"\", \"arguments\": \"\"}\n                    })\n                tool_calls[tc.index] = {\n                    \"id\": (tool_calls[tc.index][\"id\"] + (tc.id or \"\")),\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": (tool_calls[tc.index][\"function\"][\"name\"] + (tc.function.name or \"\")),\n                        \"arguments\": (tool_calls[tc.index][\"function\"][\"arguments\"] + (tc.function.arguments or \"\"))\n                    }\n                }\n    return collected_text, tool_calls\n\ndef chat_loop():\n    messages = []\n    print(\"Assistant: Hi! I am an AI agent empowered with the ability to tell the current time (Type 'quit' to exit)\")\n\n    while True:\n        user_input = input(\"\\nYou: \").strip()\n        if user_input.lower() == \"quit\":\n            break\n\n        messages.append({\"role\": \"user\", \"content\": user_input})\n\n        # Get initial response\n        response_text, tool_calls = process_stream(\n            client.chat.completions.create(\n                model=MODEL,\n                messages=messages,\n                tools=[TIME_TOOL],\n                stream=True,\n                temperature=0.2\n            )\n        )\n\n        if not tool_calls:\n            print()\n\n        text_in_first_response = len(response_text) \u003e 0\n        if text_in_first_response:\n            messages.append({\"role\": \"assistant\", \"content\": response_text})\n\n        # Handle tool calls if any\n        if tool_calls:\n            tool_name = tool_calls[0][\"function\"][\"name\"]\n            print()\n            if not text_in_first_response:\n                print(\"Assistant:\", end=\" \", flush=True)\n            print(f\"**Calling Tool: {tool_name}**\")\n            messages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})\n\n            # Execute tool calls\n            for tool_call in tool_calls:\n                if tool_call[\"function\"][\"name\"] == \"get_current_time\":\n                    result = get_current_time()\n                    messages.append({\n                        \"role\": \"tool\",\n                        \"content\": str(result),\n                        \"tool_call_id\": tool_call[\"id\"]\n                    })\n\n            # Get final response after tool execution\n            final_response, _ = process_stream(\n                client.chat.completions.create(\n                    model=MODEL,\n                    messages=messages,\n                    stream=True\n                ),\n                add_assistant_label=False\n            )\n\n            if final_response:\n                print()\n                messages.append({\"role\": \"assistant\", \"content\": final_response})\n\nif __name__ == \"__main__\":\n    chat_loop()\n```\n\n\u003c/details\u003e\n\nYou can chat with the bot by running this script from the console:\n\n```xml\n-\u003e % python tool-streaming-chatbot.py\nAssistant: Hi! I am an AI agent empowered with the ability to tell the current time (Type 'quit' to exit)\n\nYou: Tell me a joke, then tell me the current time\n\nAssistant: Sure! Here's a light joke for you: Why don't scientists trust atoms? Because they make up everything.\n\nNow, let me get the current time for you.\n\n**Calling Tool: get_current_time**\n\nThe current time is 18:49:31. Enjoy your day!\n\nYou:\n```\n\n## Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n"])</script><script>self.__next_f.push([1,"2d:T5f4,\n- Method: `POST`\n- See OpenAI docs: https://platform.openai.com/docs/api-reference/responses\n\n##### cURL (nonâ€‘streaming)\n\n```bash\ncurl http://localhost:1234/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai/gpt-oss-20b\",\n    \"input\": \"Provide a prime number less than 50\",\n    \"reasoning\": { \"effort\": \"low\" }\n  }'\n```\n\n##### Stateful followâ€‘up\n\nUse the `id` from a previous response as `previous_response_id`.\n\n```bash\ncurl http://localhost:1234/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai/gpt-oss-20b\",\n    \"input\": \"Multiply it by 2\",\n    \"previous_response_id\": \"resp_123\"\n  }'\n```\n\n##### Streaming\n\n```bash\ncurl http://localhost:1234/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai/gpt-oss-20b\",\n    \"input\": \"Hello\",\n    \"stream\": true\n  }'\n```\n\nYou will receive SSE events such as `response.created`, `response.output_text.delta`, and `response.completed`.\n\n##### Tools and Remote MCP (optâ€‘in)\n\nEnable Remote MCP in the app (Developer â†’ Settings). Example payload using an MCP server tool:\n\n```bash\ncurl http://localhost:1234/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai/gpt-oss-20b\",\n    \"tools\": [{\n      \"type\": \"mcp\",\n      \"server_label\": \"tiktoken\",\n      \"server_url\": \"https://gitmcp.io/openai/tiktoken\",\n      \"allowed_tools\": [\"fetch_tiktoken_documentation\"]\n    }],\n    \"input\": \"What is the first sentence of the tiktoken documentation?\"\n  }'\n```\n2e:T13ca,"])</script><script>self.__next_f.push([1,"\n`lmstudio-python` provides you a set APIs to interact with LLMs, embeddings models, and agentic flows.\n\n## Installing the SDK\n\n`lmstudio-python` is available as a PyPI package. You can install it using pip.\n\n```lms_code_snippet\n  variants:\n    pip:\n      language: bash\n      code: |\n        pip install lmstudio\n```\n\nFor the source code and open source contribution, visit [lmstudio-python](https://github.com/lmstudio-ai/lmstudio-python) on GitHub.\n\n## Features\n\n- Use LLMs to [respond in chats](./python/llm-prediction/chat-completion) or predict [text completions](./python/llm-prediction/completion)\n- Define functions as tools, and turn LLMs into [autonomous agents](./python/agent) that run completely locally\n- [Load](./python/manage-models/loading), [configure](./python/llm-prediction/parameters), and [unload](./python/manage-models/loading) models from memory\n- Generate embeddings for text, and more!\n\n## Quick Example: Chat with a Llama Model\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen/qwen3-4b-2507\")\n        result = model.respond(\"What is the meaning of life?\")\n\n        print(result)\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\"qwen/qwen3-4b-2507\")\n            result = model.respond(\"What is the meaning of life?\")\n\n            print(result)\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\"qwen/qwen3-4b-2507\")\n            result = await model.respond(\"What is the meaning of life?\")\n\n            print(result)\n```\n\n### Getting Local Models\n\nThe above code requires the [qwen3-4b-2507](https://lmstudio.ai/models/qwen/qwen3-4b-2507) model.\nIf you don't have the model, run the following command in the terminal to download it.\n\n```bash\nlms get qwen/qwen3-4b-2507\n```\n\nRead more about `lms get` in LM Studio's CLI [here](./cli/get).\n\n# Interactive Convenience, Deterministic Resource Management, or Structured Concurrency?\n\nAs shown in the example above, there are three distinct approaches for working\nwith the LM Studio Python SDK.\n\nThe first is the interactive convenience API (listed as \"Python (convenience API)\"\nin examples), which focuses on the use of a default LM Studio client instance for\nconvenient interactions at a synchronous Python prompt, or when using Jupyter notebooks.\n\nThe second is a synchronous scoped resource API (listed as \"Python (scoped resource API)\"\nin examples), which uses context managers to ensure that allocated resources\n(such as network connections) are released deterministically, rather than\npotentially remaining open until the entire process is terminated.\n\nThe last is an asynchronous structured concurrency API (listed as \"Python (asynchronous API)\" in\nexamples), which is designed for use in asynchronous programs that follow the design principles of\n[\"structured concurrency\"](https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/)\nin order to ensure the background tasks handling the SDK's connections to the API server host\nare managed correctly. Asynchronous applications which do not adhere to those design principles\nwill need to rely on threaded access to the synchronous scoped resource API rather than attempting\nto use the SDK's native asynchronous API. Python SDK version 1.5.0 is the first version to fully\nsupport the asynchronous API.\n\nSome examples are common between the interactive convenience API and the synchronous scoped\nresource API. These examples are listed as \"Python (synchronous API)\".\n\n## Timeouts in the synchronous API\n\n_Required Python SDK version_: **1.5.0**\n\nStarting in Python SDK version 1.5.0, the synchronous API defaults to timing out after 60 seconds\nwith no activity when waiting for a response or streaming event notification from the API server.\n\nThe number of seconds to wait for responses and event notifications can be adjusted using the\n`lmstudio.set_sync_api_timeout()` function. Setting the timeout to `None` disables the timeout\nentirely (restoring the behaviour of previous SDK versions).\n\nThe current synchronous API timeout can be queried using the `lmstudio.get_sync_api_timeout()`\nfunction.\n\n## Timeouts in the asynchronous API\n\n_Required Python SDK version_: **1.5.0**\n\nAs asynchronous coroutines support cancellation, there is no specific timeout support implemented\nin the asynchronous API. Instead, general purpose async timeout mechanisms, such as\n[`asyncio.wait_for()`](https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for) or\n[`anyio.move_on_after()`](https://anyio.readthedocs.io/en/stable/cancellation.html#timeouts),\nshould be used.\n"])</script><script>self.__next_f.push([1,"2f:T16fc,"])</script><script>self.__next_f.push([1,"\n`lmstudio` is a library published on PyPI that allows you to use `lmstudio-python` in your own projects.\nIt is open source and developed on GitHub.\nYou can find the source code [here](https://github.com/lmstudio-ai/lmstudio-python).\n\n## Installing `lmstudio-python`\n\nAs it is published to PyPI, `lmstudio-python` may be installed using `pip`\nor your preferred project dependency manager (`pdm` and `uv` are shown, but other\nPython project management tools offer similar dependency addition commands).\n\n```lms_code_snippet\n  variants:\n    pip:\n      language: bash\n      code: |\n        pip install lmstudio\n    pdm:\n      language: bash\n      code: |\n        pdm add lmstudio\n    uv:\n      language: bash\n      code: |\n        uv add lmstudio\n```\n\n## Customizing the server API host and TCP port\n\nAll of the examples in the documentation assume that the server API is running locally\non one of the default application ports (Note: in Python SDK versions prior to 1.5.0, the\nSDK also required that the optional HTTP REST server be enabled).\n\nThe network location of the server API can be overridden by\npassing a `\"host:port\"` string when creating the client instance.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n        SERVER_API_HOST = \"localhost:1234\"\n\n        # This must be the *first* convenience API interaction (otherwise the SDK\n        # implicitly creates a client that accesses the default server API host)\n        lms.configure_default_client(SERVER_API_HOST)\n\n        # Note: the dedicated configuration API was added in lmstudio-python 1.3.0\n        # For compatibility with earlier SDK versions, it is still possible to use\n        # lms.get_default_client(SERVER_API_HOST) to configure the default client\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n        SERVER_API_HOST = \"localhost:1234\"\n\n        # When using the scoped resource API, each client instance\n        # can be configured to use a specific server API host\n        with lms.Client(SERVER_API_HOST) as client:\n            model = client.llm.model()\n\n            for fragment in model.respond_stream(\"What is the meaning of life?\"):\n                print(fragment.content, end=\"\", flush=True)\n            print() # Advance to a new line at the end of the response\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n        SERVER_API_HOST = \"localhost:1234\"\n\n        # When using the asynchronous API, each client instance\n        # can be configured to use a specific server API host\n        async with lms.AsyncClient(SERVER_API_HOST) as client:\n            model = await client.llm.model()\n\n            for fragment in await model.respond_stream(\"What is the meaning of life?\"):\n                print(fragment.content, end=\"\", flush=True)\n            print() # Advance to a new line at the end of the response\n```\n\n### Checking a specified API server host is running\n\n*Required Python SDK version*: **1.5.0**\n\nWhile the most common connection pattern is to let the SDK raise an exception if it can't\nconnect to the specified API server host, the SDK also supports running the API check directly\nwithout creating an SDK client instance first:\n\n```lms_code_snippet\n  variants:\n    \"Python (synchronous API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n        SERVER_API_HOST = \"localhost:1234\"\n\n        if lms.Client.is_valid_api_host(SERVER_API_HOST):\n            print(f\"An LM Studio API server instance is available at {SERVER_API_HOST}\")\n        else:\n            print(\"No LM Studio API server instance found at {SERVER_API_HOST}\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n        SERVER_API_HOST = \"localhost:1234\"\n\n        if await lms.AsyncClient.is_valid_api_host(SERVER_API_HOST):\n            print(f\"An LM Studio API server instance is available at {SERVER_API_HOST}\")\n        else:\n            print(\"No LM Studio API server instance found at {SERVER_API_HOST}\")\n```\n\n\n### Determining the default local API server port\n\n*Required Python SDK version*: **1.5.0**\n\nWhen no API server host is specified, the SDK queries a number of ports on the local loopback\ninterface for a running API server instance. This scan is repeated for each new client instance\ncreated. Rather than letting the SDK perform this scan implicitly, the SDK also supports running\nthe scan explicitly, and passing in the reported API server details when creating clients:\n\n```lms_code_snippet\n  variants:\n    \"Python (synchronous API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        api_host = lms.Client.find_default_local_api_host()\n        if api_host is not None:\n            print(f\"An LM Studio API server instance is available at {api_host}\")\n          else:\n            print(\"No LM Studio API server instance found on any of the default local ports\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        api_host = await lms.AsyncClient.find_default_local_api_host()\n        if api_host is not None:\n            print(f\"An LM Studio API server instance is available at {api_host}\")\n          else:\n            print(\"No LM Studio API server instance found on any of the default local ports\")\n```\n"])</script><script>self.__next_f.push([1,"30:Tec1,"])</script><script>self.__next_f.push([1,"\nTo simplify interactive use, `lmstudio-python` offers a convenience API which manages\nits resources via `atexit` hooks, allowing a default synchronous client session\nto be used across multiple interactive commands.\n\nThis convenience API is shown in the examples throughout the documentation as the\n`Python (convenience API)` tab (alongside the `Python (scoped resource API)` examples,\nwhich use `with` statements to ensure deterministic cleanup of network communication\nresources).\n\nThe convenience API allows the standard Python REPL, or more flexible alternatives like\nJuypter Notebooks, to be used to interact with AI models loaded into LM Studio. For\nexample:\n\n```lms_code_snippet\n  title: \"Python REPL\"\n  variants:\n    \"Interactive chat session\":\n      language: python\n      code: |\n        \u003e\u003e\u003e import lmstudio as lms\n        \u003e\u003e\u003e loaded_models = lms.list_loaded_models()\n        \u003e\u003e\u003e for idx, model in enumerate(loaded_models):\n        ...     print(f\"{idx:\u003e3} {model}\")\n        ...\n          0 LLM(identifier='qwen2.5-7b-instruct')\n        \u003e\u003e\u003e model = loaded_models[0]\n        \u003e\u003e\u003e chat = lms.Chat(\"You answer questions concisely\")\n        \u003e\u003e\u003e chat = lms.Chat(\"You answer questions concisely\")\n        \u003e\u003e\u003e chat.add_user_message(\"Tell me three fruits\")\n        UserMessage(content=[TextData(text='Tell me three fruits')])\n        \u003e\u003e\u003e print(model.respond(chat, on_message=chat.append))\n        Banana, apple, orange.\n        \u003e\u003e\u003e chat.add_user_message(\"Tell me three more fruits\")\n        UserMessage(content=[TextData(text='Tell me three more fruits')])\n        \u003e\u003e\u003e print(model.respond(chat, on_message=chat.append))\n        Mango, strawberry, avocado.\n        \u003e\u003e\u003e chat.add_user_message(\"How many fruits have you told me?\")\n        UserMessage(content=[TextData(text='How many fruits have you told me?')])\n        \u003e\u003e\u003e print(model.respond(chat, on_message=chat.append))\n        You asked for three initial fruits and three more, so I've listed a total of six fruits.\n\n```\n\nWhile not primarily intended for use this way, the SDK's asynchronous structured concurrency API\nis compatible with the asynchronous Python REPL that is launched by `python -m asyncio`.\nFor example:\n\n```lms_code_snippet\n  title: \"Python REPL\"\n  variants:\n    \"Asynchronous chat session\":\n      language: python\n      code: |\n        # Note: assumes use of the \"python -m asyncio\" asynchronous REPL (or equivalent)\n        # Requires Python SDK version 1.5.0 or later\n        \u003e\u003e\u003e from contextlib import AsyncExitStack\n        \u003e\u003e\u003e import lmstudio as lms\n        \u003e\u003e\u003e resources = AsyncExitStack()\n        \u003e\u003e\u003e client = await resources.enter_async_context(lms.AsyncClient())\n        \u003e\u003e\u003e loaded_models = await client.llm.list_loaded()\n        \u003e\u003e\u003e for idx, model in enumerate(loaded_models):\n        ...     print(f\"{idx:\u003e3} {model}\")\n        ...\n          0 AsyncLLM(identifier='qwen2.5-7b-instruct-1m')\n        \u003e\u003e\u003e model = loaded_models[0]\n        \u003e\u003e\u003e chat = lms.Chat(\"You answer questions concisely\")\n        \u003e\u003e\u003e chat.add_user_message(\"Tell me three fruits\")\n        UserMessage(content=[TextData(text='Tell me three fruits')])\n        \u003e\u003e\u003e print(await model.respond(chat, on_message=chat.append))\n        Apple, banana, and orange.\n        \u003e\u003e\u003e chat.add_user_message(\"Tell me three more fruits\")\n        UserMessage(content=[TextData(text='Tell me three more fruits')])\n        \u003e\u003e\u003e print(await model.respond(chat, on_message=chat.append))\n        Mango, strawberry, and pineapple.\n        \u003e\u003e\u003e chat.add_user_message(\"How many fruits have you told me?\")\n        UserMessage(content=[TextData(text='How many fruits have you told me?')])\n        \u003e\u003e\u003e print(await model.respond(chat, on_message=chat.append))\n        You asked for three fruits initially, then three more, so I've listed six fruits in total.\n\n```\n"])</script><script>self.__next_f.push([1,"31:T3272,"])</script><script>self.__next_f.push([1,"\nUse `llm.respond(...)` to generate completions for a chat conversation.\n\n## Quick Example: Generate a Chat Response\n\nThe following snippet shows how to obtain the AI's response to a quick chat prompt.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n        print(model.respond(\"What is the meaning of life?\"))\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n            print(model.respond(\"What is the meaning of life?\"))\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model()\n            print(await model.respond(\"What is the meaning of life?\"))\n\n```\n\n## Streaming a Chat Response\n\nThe following snippet shows how to stream the AI's response to a chat prompt,\ndisplaying text fragments as they are received (rather than waiting for the\nentire response to be generated before displaying anything).\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n        model = lms.llm()\n\n        for fragment in model.respond_stream(\"What is the meaning of life?\"):\n            print(fragment.content, end=\"\", flush=True)\n        print() # Advance to a new line at the end of the response\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n\n            for fragment in model.respond_stream(\"What is the meaning of life?\"):\n                print(fragment.content, end=\"\", flush=True)\n            print() # Advance to a new line at the end of the response\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model()\n\n            async for fragment in model.respond_stream(\"What is the meaning of life?\"):\n                print(fragment.content, end=\"\", flush=True)\n            print() # Advance to a new line at the end of the response\n\n```\n\n## Cancelling a Chat Response\n\nSee the [Cancelling a Prediction](./cancelling-predictions) section for how to cancel a prediction in progress.\n\n## Obtain a Model\n\nFirst, you need to get a model handle.\nThis can be done using the top-level `llm` convenience API,\nor the `model` method in the `llm` namespace when using the scoped resource API.\nFor example, here is how to use Qwen2.5 7B Instruct.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen2.5-7b-instruct\")\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\"qwen2.5-7b-instruct\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\"qwen2.5-7b-instruct\")\n\n```\n\nThere are other ways to get a model handle. See [Managing Models in Memory](./../manage-models/loading) for more info.\n\n## Manage Chat Context\n\nThe input to the model is referred to as the \"context\".\nConceptually, the model receives a multi-turn conversation as input,\nand it is asked to predict the assistant's response in that conversation.\n\n```lms_code_snippet\n  variants:\n    \"Constructing a Chat object\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        # Create a chat with an initial system prompt.\n        chat = lms.Chat(\"You are a resident AI philosopher.\")\n\n        # Build the chat context by adding messages of relevant types.\n        chat.add_user_message(\"What is the meaning of life?\")\n        # ... continued in next example\n\n  \"From chat history data\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        # Create a chat object from a chat history dict\n        chat = lms.Chat.from_history({\n            \"messages\": [\n                { \"role\": \"system\", \"content\": \"You are a resident AI philosopher.\" },\n                { \"role\": \"user\", \"content\": \"What is the meaning of life?\" },\n            ]\n        })\n        # ... continued in next example\n\n```\n\nSee [Working with Chats](./working-with-chats) for more information on managing chat context.\n\n\u003c!-- , and [`Chat`](./../api-reference/chat) for API reference for the `Chat` class. --\u003e\n\n## Generate a response\n\nYou can ask the LLM to predict the next response in the chat context using the `respond()` method.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming (synchronous API)\":\n      language: python\n      code: |\n        # The `chat` object is created in the previous step.\n        result = model.respond(chat)\n\n        print(result)\n\n    \"Streaming (synchronous API)\":\n      language: python\n      code: |\n        # The `chat` object is created in the previous step.\n        prediction_stream = model.respond_stream(chat)\n\n        for fragment in prediction_stream:\n            print(fragment.content, end=\"\", flush=True)\n        print() # Advance to a new line at the end of the response\n\n    \"Non-streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        # The `chat` object is created in the previous step.\n        result = await model.respond(chat)\n\n        print(result)\n\n    \"Streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        # The `chat` object is created in the previous step.\n        prediction_stream = await model.respond_stream(chat)\n\n        async for fragment in prediction_stream:\n            print(fragment.content, end=\"\", flush=True)\n        print() # Advance to a new line at the end of the response\n\n```\n\n## Customize Inferencing Parameters\n\nYou can pass in inferencing parameters via the `config` keyword parameter on `.respond()`.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming (synchronous API)\":\n      language: python\n      code: |\n        result = model.respond(chat, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \"Streaming (synchronous API)\":\n      language: python\n      code: |\n        prediction_stream = model.respond_stream(chat, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \"Non-streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        result = await model.respond(chat, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \"Streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        prediction_stream = await model.respond_stream(chat, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n```\n\nSee [Configuring the Model](./parameters) for more information on what can be configured.\n\n## Print prediction stats\n\nYou can also print prediction metadata, such as the model used for generation, number of generated\ntokens, time to first token, and stop reason.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: python\n      code: |\n        # `result` is the response from the model.\n        print(\"Model used:\", result.model_info.display_name)\n        print(\"Predicted tokens:\", result.stats.predicted_tokens_count)\n        print(\"Time to first token (seconds):\", result.stats.time_to_first_token_sec)\n        print(\"Stop reason:\", result.stats.stop_reason)\n\n    \"Streaming\":\n      language: python\n      code: |\n        # After iterating through the prediction fragments,\n        # the overall prediction result may be obtained from the stream\n        result = prediction_stream.result()\n\n        print(\"Model used:\", result.model_info.display_name)\n        print(\"Predicted tokens:\", result.stats.predicted_tokens_count)\n        print(\"Time to first token (seconds):\", result.stats.time_to_first_token_sec)\n        print(\"Stop reason:\", result.stats.stop_reason)\n\n```\n\nBoth the non-streaming and streaming result access is consistent across the synchronous and\nasynchronous APIs, as `prediction_stream.result()` is a non-blocking API that raises an exception\nif no result is available (either because the prediction is still running, or because the\nprediction request failed). Prediction streams also offer a blocking (synchronous API) or\nawaitable (asynchronous API) `prediction_stream.wait_for_result()` method that internally handles\niterating the stream to completion before returning the result.\n\n## Example: Multi-turn Chat\n\n```lms_code_snippet\n  title: \"chatbot.py\"\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n        chat = lms.Chat(\"You are a task focused AI assistant\")\n\n        while True:\n            try:\n                user_input = input(\"You (leave blank to exit): \")\n            except EOFError:\n                print()\n                break\n            if not user_input:\n                break\n            chat.add_user_message(user_input)\n            prediction_stream = model.respond_stream(\n                chat,\n                on_message=chat.append,\n            )\n            print(\"Bot: \", end=\"\", flush=True)\n            for fragment in prediction_stream:\n                print(fragment.content, end=\"\", flush=True)\n            print()\n\n```\n\n### Progress Callbacks\n\nLong prompts will often take a long time to first token, i.e. it takes the model a long time to process your prompt.\nIf you want to get updates on the progress of this process, you can provide a float callback to `respond`\nthat receives a float from 0.0-1.0 representing prompt processing progress.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        llm = lms.llm()\n\n        response = llm.respond(\n            \"What is LM Studio?\",\n            on_prompt_processing_progress = (lambda progress: print(f\"{progress*100}% complete\")),\n        )\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            llm = client.llm.model()\n\n            response = llm.respond(\n                \"What is LM Studio?\",\n                on_prompt_processing_progress = (lambda progress: print(f\"{progress*100}% complete\")),\n            )\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            llm = await client.llm.model()\n\n            response = await llm.respond(\n                \"What is LM Studio?\",\n                on_prompt_processing_progress = (lambda progress: print(f\"{progress*100}% complete\")),\n            )\n\n\n```\n\nIn addition to `on_prompt_processing_progress`, the other available progress callbacks are:\n\n- `on_first_token`: called after prompt processing is complete and the first token is being emitted.\n  Does not receive any arguments (use the streaming iteration API or `on_prediction_fragment`\n  to process tokens as they are emitted).\n- `on_prediction_fragment`: called for each prediction fragment received by the client.\n  Receives the same prediction fragments as iterating over the stream iteration API.\n- `on_message`: called with an assistant response message when the prediction is complete.\n  Intended for appending received messages to a chat history instance.\n"])</script><script>self.__next_f.push([1,"32:T1245,"])</script><script>self.__next_f.push([1,"\n*Required Python SDK version*: **1.1.0**\n\nSome models, known as VLMs (Vision-Language Models), can accept images as input. You can pass images to the model using the `.respond()` method.\n\n### Prerequisite: Get a VLM (Vision-Language Model)\n\nIf you don't yet have a VLM, you can download a model like `qwen2-vl-2b-instruct` using the following command:\n\n```bash\nlms get qwen2-vl-2b-instruct\n```\n\n## 1. Instantiate the Model\n\nConnect to LM Studio and obtain a handle to the VLM (Vision-Language Model) you want to use.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen2-vl-2b-instruct\")\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\"qwen2-vl-2b-instruct\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\"qwen2-vl-2b-instruct\")\n\n```\n\n## 2. Prepare the Image\n\nUse the `prepare_image()` function or `files` namespace method to\nget a handle to the image that can subsequently be passed to the model.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n        image_handle = lms.prepare_image(image_path)\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n            image_handle = client.files.prepare_image(image_path)\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n            image_handle = await client.files.prepare_image(image_path)\n\n```\n\nIf you only have the raw data of the image, you can supply the raw data directly as a bytes\nobject without having to write it to disk first. Due to this feature, binary filesystem\npaths are *not* supported (as they will be handled as malformed image data rather than as\nfilesystem paths).\n\nBinary IO objects are also accepted as local file inputs.\n\nThe LM Studio server supports JPEG, PNG, and WebP image formats.\n\n## 3. Pass the Image to the Model in `.respond()`\n\nGenerate a prediction by passing the image to the model in the `.respond()` method.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n        image_handle = lms.prepare_image(image_path)\n        model = lms.llm(\"qwen2-vl-2b-instruct\")\n        chat = lms.Chat()\n        chat.add_user_message(\"Describe this image please\", images=[image_handle])\n        prediction = model.respond(chat)\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n            image_handle = client.files.prepare_image(image_path)\n            model = client.llm.model(\"qwen2-vl-2b-instruct\")\n            chat = lms.Chat()\n            chat.add_user_message(\"Describe this image please\", images=[image_handle])\n            prediction = model.respond(chat)\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n            image_handle = client.files.prepare_image(image_path)\n            model = await client.llm.model(\"qwen2-vl-2b-instruct\")\n            chat = lms.Chat()\n            chat.add_user_message(\"Describe this image please\", images=[image_handle])\n            prediction = await model.respond(chat)\n\n```\n"])</script><script>self.__next_f.push([1,"33:Td73,"])</script><script>self.__next_f.push([1,"\nOne benefit of using the streaming API is the ability to cancel the\nprediction request based on criteria that can't be represented using\nthe `stopStrings` or `maxPredictedTokens` configuration settings.\n\nThe following snippet illustrates cancelling the request in response\nto an application specification cancellation condition (such as polling\nan event set by another thread).\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n        model = lms.llm()\n\n        prediction_stream = model.respond_stream(\"What is the meaning of life?\")\n        cancelled = False\n        for fragment in prediction_stream:\n            if ...: # Cancellation condition will be app specific\n                cancelled = True\n                prediction_stream.cancel()\n                # Note: it is recommended to let the iteration complete,\n                # as doing so allows the partial result to be recorded.\n                # Breaking the loop *is* permitted, but means the partial result\n                # and final prediction stats won't be available to the client\n        # The stream allows the prediction result to be retrieved after iteration\n        if not cancelled:\n            print(prediction_stream.result())\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n\n            prediction_stream = model.respond_stream(\"What is the meaning of life?\")\n            cancelled = False\n            for fragment in prediction_stream:\n                if ...: # Cancellation condition will be app specific\n                    cancelled = True\n                    prediction_stream.cancel()\n                    # Note: it is recommended to let the iteration complete,\n                    # as doing so allows the partial result to be recorded.\n                    # Breaking the loop *is* permitted, but means the partial result\n                    # and final prediction stats won't be available to the client\n            # The stream allows the prediction result to be retrieved after iteration\n            if not cancelled:\n                print(prediction_stream.result())\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model()\n\n            prediction_stream = await model.respond_stream(\"What is the meaning of life?\")\n            cancelled = False\n            async for fragment in prediction_stream:\n                if ...: # Cancellation condition will be app specific\n                    cancelled = True\n                    await prediction_stream.cancel()\n                    # Note: it is recommended to let the iteration complete,\n                    # as doing so allows the partial result to be recorded.\n                    # Breaking the loop *is* permitted, but means the partial result\n                    # and final prediction stats won't be available to the client\n            # The stream allows the prediction result to be retrieved after iteration\n            if not cancelled:\n                print(prediction_stream.result())\n\n```\n"])</script><script>self.__next_f.push([1,"34:T16f5,"])</script><script>self.__next_f.push([1,"\nYou can enforce a particular response format from an LLM by providing a JSON schema to the `.respond()` method.\nThis guarantees that the model's output conforms to the schema you provide.\n\nThe JSON schema can either be provided directly,\nor by providing an object that implements the `lmstudio.ModelSchema` protocol,\nsuch as `pydantic.BaseModel` or `lmstudio.BaseModel`.\n\nThe `lmstudio.ModelSchema` protocol is defined as follows:\n\n```python\n@runtime_checkable\nclass ModelSchema(Protocol):\n    \"\"\"Protocol for classes that provide a JSON schema for their model.\"\"\"\n\n    @classmethod\n    def model_json_schema(cls) -\u003e DictSchema:\n        \"\"\"Return a JSON schema dict describing this model.\"\"\"\n        ...\n\n```\n\nWhen a schema is provided, the prediction result's `parsed` field will contain a string-keyed dictionary that conforms\nto the given schema (for unstructured results, this field is a string field containing the same value as `content`).\n\n\n## Enforce Using a Class Based Schema Definition\n\nIf you wish the model to generate JSON that satisfies a given schema,\nit is recommended to provide a class based schema definition using a library\nsuch as [`pydantic`](https://docs.pydantic.dev/) or [`msgspec`](https://jcristharif.com/msgspec/).\n\nPydantic models natively implement the `lmstudio.ModelSchema` protocol,\nwhile `lmstudio.BaseModel` is a `msgspec.Struct` subclass that implements `.model_json_schema()` appropriately.\n\n#### Define a Class Based Schema\n\n```lms_code_snippet\n  variants:\n    \"pydantic.BaseModel\":\n      language: python\n      code: |\n        from pydantic import BaseModel\n\n        # A class based schema for a book\n        class BookSchema(BaseModel):\n            title: str\n            author: str\n            year: int\n\n    \"lmstudio.BaseModel\":\n      language: python\n      code: |\n        from lmstudio import BaseModel\n\n        # A class based schema for a book\n        class BookSchema(BaseModel):\n            title: str\n            author: str\n            year: int\n\n```\n\n#### Generate a Structured Response\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: python\n      code: |\n        result = model.respond(\"Tell me about The Hobbit\", response_format=BookSchema)\n        book = result.parsed\n\n        print(book)\n        #           ^\n        # Note that `book` is correctly typed as { title: string, author: string, year: number }\n\n    Streaming:\n      language: python\n      code: |\n        prediction_stream = model.respond_stream(\"Tell me about The Hobbit\", response_format=BookSchema)\n\n        # Optionally stream the response\n        # for fragment in prediction:\n        #   print(fragment.content, end=\"\", flush=True)\n        # print()\n        # Note that even for structured responses, the *fragment* contents are still only text\n\n        # Get the final structured result\n        result = prediction_stream.result()\n        book = result.parsed\n\n        print(book)\n        #           ^\n        # Note that `book` is correctly typed as { title: string, author: string, year: number }\n```\n\n## Enforce Using a JSON Schema\n\nYou can also enforce a structured response using a JSON schema.\n\n#### Define a JSON Schema\n\n```python\n# A JSON schema for a book\nschema = {\n  \"type\": \"object\",\n  \"properties\": {\n    \"title\": { \"type\": \"string\" },\n    \"author\": { \"type\": \"string\" },\n    \"year\": { \"type\": \"integer\" },\n  },\n  \"required\": [\"title\", \"author\", \"year\"],\n}\n```\n\n#### Generate a Structured Response\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: python\n      code: |\n        result = model.respond(\"Tell me about The Hobbit\", response_format=schema)\n        book = result.parsed\n\n        print(book)\n        #     ^\n        # Note that `book` is correctly typed as { title: string, author: string, year: number }\n\n    Streaming:\n      language: python\n      code: |\n        prediction_stream = model.respond_stream(\"Tell me about The Hobbit\", response_format=schema)\n\n        # Stream the response\n        for fragment in prediction:\n            print(fragment.content, end=\"\", flush=True)\n        print()\n        # Note that even for structured responses, the *fragment* contents are still only text\n\n        # Get the final structured result\n        result = prediction_stream.result()\n        book = result.parsed\n\n        print(book)\n        #     ^\n        # Note that `book` is correctly typed as { title: string, author: string, year: number }\n```\n\n\u003c!--\n\nTODO: Info about structured generation caveats\n\n ## Overview\n\nOnce you have [downloaded and loaded](/docs/basics/index) a large language model,\nyou can use it to respond to input through the API. This article covers getting JSON structured output, but you can also\n[request text completions](/docs/api/sdk/completion),\n[request chat responses](/docs/api/sdk/chat-completion), and\n[use a vision-language model to chat about images](/docs/api/sdk/image-input).\n\n### Usage\n\nCertain models are trained to output valid JSON data that conforms to\na user-provided schema, which can be used programmatically in applications\nthat need structured data. This structured data format is supported by both\n[`complete`](/docs/api/sdk/completion) and [`respond`](/docs/api/sdk/chat-completion)\nmethods, and relies on Pydantic in Python and Zod in TypeScript.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        const Book = z.object({\n          title: z.string(),\n          author: z.string(),\n          year: z.number().int()\n        })\n\n        const client = new LMStudioClient()\n        const llm = client.llm.model()\n\n        const response = llm.respond(\n          \"Tell me about The Hobbit.\",\n          { structured: Book },\n        )\n\n        console.log(response.content.title)\n``` --\u003e\n"])</script><script>self.__next_f.push([1,"35:T70e,\n_Required Python SDK version_: **1.2.0**\n\nSpeculative decoding is a technique that can substantially increase the generation speed of large language models (LLMs) without reducing response quality. See [Speculative Decoding](./../../app/advanced/speculative-decoding) for more info.\n\nTo use speculative decoding in `lmstudio-python`, simply provide a `draftModel` parameter when performing the prediction. You do not need to load the draft model separately.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        main_model_key = \"qwen2.5-7b-instruct\"\n        draft_model_key = \"qwen2.5-0.5b-instruct\"\n\n        model = lms.llm(main_model_key)\n        result = model.respond(\n            \"What are the prime numbers between 0 and 100?\",\n            config={\n                \"draftModel\": draft_model_key,\n            }\n        )\n\n        print(result)\n        stats = result.stats\n        print(f\"Accepted {stats.accepted_draft_tokens_count}/{stats.predicted_tokens_count} tokens\")\n\n\n    Streaming:\n      language: python\n      code: |\n        import lmstudio as lms\n\n        main_model_key = \"qwen2.5-7b-instruct\"\n        draft_model_key = \"qwen2.5-0.5b-instruct\"\n\n        model = lms.llm(main_model_key)\n        prediction_stream = model.respond_stream(\n            \"What are the prime numbers between 0 and 100?\",\n            config={\n                \"draftModel\": draft_model_key,\n            }\n        )\n        for fragment in prediction_stream:\n            print(fragment.content, end=\"\", flush=True)\n        print() # Advance to a new line at the end of the response\n\n        stats = prediction_stream.result().stats\n        print(f\"Accepted {stats.accepted_draft_tokens_count}/{stats.predicted_tokens_count} tokens\")\n```\n36:T24f5,"])</script><script>self.__next_f.push([1,"\nUse `llm.complete(...)` to generate text completions from a loaded language model.\nText completions mean sending a non-formatted string to the model with the expectation that the model will complete the text.\n\nThis is different from multi-turn chat conversations. For more information on chat completions, see [Chat Completions](./chat-completion).\n\n## 1. Instantiate a Model\n\nFirst, you need to load a model to generate completions from.\nThis can be done using the top-level `llm` convenience API,\nor the `model` method in the `llm` namespace when using the scoped resource API.\nFor example, here is how to use Qwen2.5 7B Instruct.\n\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen2.5-7b-instruct\")\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\"qwen2.5-7b-instruct\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\"qwen2.5-7b-instruct\")\n\n```\n\n## 2. Generate a Completion\n\nOnce you have a loaded model, you can generate completions by passing a string to the `complete` method on the `llm` handle.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming (synchronous API)\":\n      language: python\n      code: |\n        # The `chat` object is created in the previous step.\n        result = model.complete(\"My name is\", config={\"maxTokens\": 100})\n\n        print(result)\n\n    \"Streaming (synchronous API)\":\n      language: python\n      code: |\n        # The `chat` object is created in the previous step.\n        prediction_stream = model.complete_stream(\"My name is\", config={\"maxTokens\": 100})\n\n        for fragment in prediction_stream:\n            print(fragment.content, end=\"\", flush=True)\n        print() # Advance to a new line at the end of the response\n\n    \"Non-streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        # The `chat` object is created in the previous step.\n        result = await model.complete(\"My name is\", config={\"maxTokens\": 100})\n\n        print(result)\n\n    \"Streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        # The `chat` object is created in the previous step.\n        prediction_stream = await model.complete_stream(\"My name is\", config={\"maxTokens\": 100})\n\n        async for fragment in prediction_stream:\n            print(fragment.content, end=\"\", flush=True)\n        print() # Advance to a new line at the end of the response\n\n```\n\n## 3. Print Prediction Stats\n\nYou can also print prediction metadata, such as the model used for generation, number of generated tokens, time to first token, and stop reason.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: python\n      code: |\n        # `result` is the response from the model.\n        print(\"Model used:\", result.model_info.display_name)\n        print(\"Predicted tokens:\", result.stats.predicted_tokens_count)\n        print(\"Time to first token (seconds):\", result.stats.time_to_first_token_sec)\n        print(\"Stop reason:\", result.stats.stop_reason)\n\n    \"Streaming\":\n      language: python\n      code: |\n        # After iterating through the prediction fragments,\n        # the overall prediction result may be obtained from the stream\n        result = prediction_stream.result()\n\n        print(\"Model used:\", result.model_info.display_name)\n        print(\"Predicted tokens:\", result.stats.predicted_tokens_count)\n        print(\"Time to first token (seconds):\", result.stats.time_to_first_token_sec)\n        print(\"Stop reason:\", result.stats.stop_reason)\n\n```\n\nBoth the non-streaming and streaming result access is consistent across the synchronous and\nasynchronous APIs, as `prediction_stream.result()` is a non-blocking API that raises an exception\nif no result is available (either because the prediction is still running, or because the\nprediction request failed). Prediction streams also offer a blocking (synchronous API) or\nawaitable (asynchronous API) `prediction_stream.wait_for_result()` method that internally handles\niterating the stream to completion before returning the result.\n\n## Example: Get an LLM to Simulate a Terminal\n\nHere's an example of how you might use the `complete` method to simulate a terminal.\n\n```lms_code_snippet\n  title: \"terminal-sim.py\"\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n        console_history = []\n\n        while True:\n            try:\n                user_command = input(\"$ \")\n            except EOFError:\n                print()\n                break\n            if user_command.strip() == \"exit\":\n                break\n            console_history.append(f\"$ {user_command}\")\n            history_prompt = \"\\n\".join(console_history)\n            prediction_stream = model.complete_stream(\n                history_prompt,\n                config={ \"stopStrings\": [\"$\"] },\n            )\n            for fragment in prediction_stream:\n                print(fragment.content, end=\"\", flush=True)\n            print()\n            console_history.append(prediction_stream.result().content)\n\n```\n\n## Customize Inferencing Parameters\n\nYou can pass in inferencing parameters via the `config` keyword parameter on `.complete()`.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming (synchronous API)\":\n      language: python\n      code: |\n        result = model.complete(initial_text, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \"Streaming (synchronous API)\":\n      language: python\n      code: |\n        prediction_stream = model.complete_stream(initial_text, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \"Non-streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        result = await model.complete(initial_text, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \"Streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        prediction_stream = await model.complete_stream(initial_text, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n```\n\nSee [Configuring the Model](./parameters) for more information on what can be configured.\n\n### Progress Callbacks\n\nLong prompts will often take a long time to first token, i.e. it takes the model a long time to process your prompt.\nIf you want to get updates on the progress of this process, you can provide a float callback to `complete`\nthat receives a float from 0.0-1.0 representing prompt processing progress.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        llm = lms.llm()\n\n        completion = llm.complete(\n            \"My name is\",\n            on_prompt_processing_progress = (lambda progress: print(f\"{progress*100}% complete\")),\n        )\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            llm = client.llm.model()\n\n            completion = llm.complete(\n                \"My name is\",\n                on_prompt_processing_progress = (lambda progress: print(f\"{progress*100}% processed\")),\n            )\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            llm = await client.llm.model()\n\n            completion = await llm.complete(\n                \"My name is\",\n                on_prompt_processing_progress = (lambda progress: print(f\"{progress*100}% processed\")),\n            )\n\n```\n\nIn addition to `on_prompt_processing_progress`, the other available progress callbacks are:\n\n* `on_first_token`: called after prompt processing is complete and the first token is being emitted.\n  Does not receive any arguments (use the streaming iteration API or `on_prediction_fragment`\n  to process tokens as they are emitted).\n* `on_prediction_fragment`: called for each prediction fragment received by the client.\n  Receives the same prediction fragments as iterating over the stream iteration API.\n* `on_message`: called with an assistant response message when the prediction is complete.\n  Intended for appending received messages to a chat history instance.\n"])</script><script>self.__next_f.push([1,"37:T1285,"])</script><script>self.__next_f.push([1,"\nYou can customize both inference-time and load-time parameters for your model. Inference parameters can be set on a per-request basis, while load parameters are set when loading the model.\n\n# Inference Parameters\n\nSet inference-time parameters such as `temperature`, `maxTokens`, `topP` and more.\n\n```lms_code_snippet\n  variants:\n    \".respond()\":\n      language: python\n      code: |\n        result = model.respond(chat, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \".complete()\":\n      language: python\n      code: |\n        result = model.complete(chat, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n            \"stopStrings\": [\"\\n\\n\"],\n          })\n\n```\n\nSee [`LLMPredictionConfigInput`](./../../typescript/api-reference/llm-prediction-config-input) in the\nTypescript SDK documentation for all configurable fields.\n\nNote that while `structured` can be set to a JSON schema definition as an inference-time configuration parameter\n(Zod schemas are not supported in the Python SDK), the preferred approach is to instead set the\n[dedicated `response_format` parameter](\u003c(./structured-responses)\u003e), which allows you to more rigorously\nenforce the structure of the output using a JSON or class based schema definition.\n\n# Load Parameters\n\nSet load-time parameters such as the context length, GPU offload ratio, and more.\n\n### Set Load Parameters with `.model()`\n\nThe `.model()` retrieves a handle to a model that has already been loaded, or loads a new one on demand (JIT loading).\n\n**Note**: if the model is already loaded, the given configuration will be **ignored**.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen2.5-7b-instruct\", config={\n            \"contextLength\": 8192,\n            \"gpu\": {\n              \"ratio\": 0.5,\n            }\n        })\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\n                \"qwen2.5-7b-instruct\",\n                config={\n                    \"contextLength\": 8192,\n                    \"gpu\": {\n                      \"ratio\": 0.5,\n                    }\n                }\n            )\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\n                \"qwen2.5-7b-instruct\",\n                config={\n                    \"contextLength\": 8192,\n                    \"gpu\": {\n                      \"ratio\": 0.5,\n                    }\n                }\n            )\n\n```\n\nSee [`LLMLoadModelConfig`](./../../typescript/api-reference/llm-load-model-config) in the\nTypescript SDK documentation for all configurable fields.\n\n### Set Load Parameters with `.load_new_instance()`\n\nThe `.load_new_instance()` method creates a new model instance and loads it with the specified configuration.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        client = lms.get_default_client()\n        model = client.llm.load_new_instance(\"qwen2.5-7b-instruct\", config={\n            \"contextLength\": 8192,\n            \"gpu\": {\n              \"ratio\": 0.5,\n            }\n        })\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.load_new_instance(\n                \"qwen2.5-7b-instruct\",\n                config={\n                    \"contextLength\": 8192,\n                    \"gpu\": {\n                      \"ratio\": 0.5,\n                    }\n                }\n            )\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.load_new_instance(\n                \"qwen2.5-7b-instruct\",\n                config={\n                    \"contextLength\": 8192,\n                    \"gpu\": {\n                      \"ratio\": 0.5,\n                    }\n                }\n            )\n\n```\n\nSee [`LLMLoadModelConfig`](./../../typescript/api-reference/llm-load-model-config) in the\nTypescript SDK documentation for all configurable fields.\n"])</script><script>self.__next_f.push([1,"38:T88b,"])</script><script>self.__next_f.push([1,"\nSDK methods such as `llm.respond()`, `llm.applyPromptTemplate()`, or `llm.act()`\ntake in a chat parameter as an input.\nThere are a few ways to represent a chat when using the SDK.\n\n## Option 1: Input a Single String\n\nIf your chat only has one single user message, you can use a single string to represent the chat.\nHere is an example with the `.respond` method.\n\n```lms_code_snippet\nvariants:\n  \"Single string\":\n    language: python\n    code: |\n      prediction = llm.respond(\"What is the meaning of life?\")\n```\n\n## Option 2: Using the `Chat` Helper Class\n\nFor more complex tasks, it is recommended to use the `Chat` helper class.\nIt provides various commonly used methods to manage the chat.\nHere is an example with the `Chat` class, where the initial system prompt\nis supplied when initializing the chat instance, and then the initial user\nmessage is added via the corresponding method call.\n\n```lms_code_snippet\nvariants:\n  \"Simple chat\":\n    language: python\n    code: |\n      chat = Chat(\"You are a resident AI philosopher.\")\n      chat.add_user_message(\"What is the meaning of life?\")\n\n      prediction = llm.respond(chat)\n```\n\nYou can also quickly construct a `Chat` object using the `Chat.from_history` method.\n\n```lms_code_snippet\nvariants:\n  \"Chat history data\":\n    language: python\n    code: |\n      chat = Chat.from_history({\"messages\": [\n        { \"role\": \"system\", \"content\": \"You are a resident AI philosopher.\" },\n        { \"role\": \"user\", \"content\": \"What is the meaning of life?\" },\n      ]})\n\n  \"Single string\":\n    language: python\n    code: |\n      # This constructs a chat with a single user message\n      chat = Chat.from_history(\"What is the meaning of life?\")\n\n```\n\n## Option 3: Providing Chat History Data Directly\n\nAs the APIs that accept chat histories use `Chat.from_history` internally,\nthey also accept the chat history data format as a regular dictionary:\n\n```lms_code_snippet\nvariants:\n  \"Chat history data\":\n    language: python\n    code: |\n      prediction = llm.respond({\"messages\": [\n        { \"role\": \"system\", \"content\": \"You are a resident AI philosopher.\" },\n        { \"role\": \"user\", \"content\": \"What is the meaning of life?\" },\n      ]})\n```\n"])</script><script>self.__next_f.push([1,"39:T22d6,"])</script><script>self.__next_f.push([1,"\n## Automatic tool calling\n\nWe introduce the concept of execution \"rounds\" to describe the combined process of running a tool, providing its output to the LLM, and then waiting for the LLM to decide what to do next.\n\n**Execution Round**\n\n```\n â€¢ run a tool -\u003e\n â†‘   â€¢ provide the result to the LLM -\u003e\n â”‚       â€¢ wait for the LLM to generate a response\n â”‚\n â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””âž” (return)\n```\n\nA model might choose to run tools multiple times before returning a final result. For example, if the LLM is writing code, it might choose to compile or run the program, fix errors, and then run it again, rinse and repeat until it gets the desired result.\n\nWith this in mind, we say that the `.act()` API is an automatic \"multi-round\" tool calling API.\n\n### Quick Example\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        def multiply(a: float, b: float) -\u003e float:\n            \"\"\"Given two numbers a and b. Returns the product of them.\"\"\"\n            return a * b\n\n        model = lms.llm(\"qwen2.5-7b-instruct\")\n        model.act(\n          \"What is the result of 12345 multiplied by 54321?\",\n          [multiply],\n          on_message=print,\n        )\n```\n\n### What does it mean for an LLM to \"use a tool\"?\n\nLLMs are largely text-in, text-out programs. So, you may ask \"how can an LLM use a tool?\". The answer is that some LLMs are trained to ask the human to call the tool for them, and expect the tool output to to be provided back in some format.\n\nImagine you're giving computer support to someone over the phone. You might say things like \"run this command for me ... OK what did it output? ... OK now click there and tell me what it says ...\". In this case you're the LLM! And you're \"calling tools\" vicariously through the person on the other side of the line.\n\n### Running multiple tool calls in parallel\n\nBy default, version 1.4.0 and later of the Python SDK will only run a single tool call request at a time,\neven if the model requests multiple tool calls in a single response message. This ensures the requests will\nbe processed correctly even if the tool implementations do not support multiple concurrent calls.\n\nWhen the tool implementations are known to be thread-safe, and are both slow and frequent enough to be worth\nrunning in parallel, the `max_parallel_tool_calls` option specifies the maximum number of tool call requests\nthat will be processed in parallel from a single model response. This value defaults to 1 (waiting for each\ntool call to complete before starting the next one). Setting this value to `None` will automatically scale\nthe maximum number of parallel tool calls to a multiple of the number of CPU cores available to the process.\n\n### Important: Model Selection\n\nThe model selected for tool use will greatly impact performance.\n\nSome general guidance when selecting a model:\n\n- Not all models are capable of intelligent tool use\n- Bigger is better (i.e., a 7B parameter model will generally perform better than a 3B parameter model)\n- We've observed [Qwen2.5-7B-Instruct](https://model.lmstudio.ai/download/lmstudio-community/Qwen2.5-7B-Instruct-GGUF) to perform well in a wide variety of cases\n- This guidance may change\n\n### Example: Multiple Tools\n\nThe following code demonstrates how to provide multiple tools in a single `.act()` call.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import math\n        import lmstudio as lms\n\n        def add(a: int, b: int) -\u003e int:\n            \"\"\"Given two numbers a and b, returns the sum of them.\"\"\"\n            return a + b\n\n        def is_prime(n: int) -\u003e bool:\n            \"\"\"Given a number n, returns True if n is a prime number.\"\"\"\n            if n \u003c 2:\n                return False\n            sqrt = int(math.sqrt(n))\n            for i in range(2, sqrt):\n                if n % i == 0:\n                    return False\n            return True\n\n        model = lms.llm(\"qwen2.5-7b-instruct\")\n        model.act(\n          \"Is the result of 12345 + 45668 a prime? Think step by step.\",\n          [add, is_prime],\n          on_message=print,\n        )\n```\n\n### Example: Chat Loop with Create File Tool\n\nThe following code creates a conversation loop with an LLM agent that can create files.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import readline # Enables input line editing\n        from pathlib import Path\n\n        import lmstudio as lms\n\n        def create_file(name: str, content: str):\n            \"\"\"Create a file with the given name and content.\"\"\"\n            dest_path = Path(name)\n            if dest_path.exists():\n                return \"Error: File already exists.\"\n            try:\n                dest_path.write_text(content, encoding=\"utf-8\")\n            except Exception as exc:\n                return \"Error: {exc!r}\"\n            return \"File created.\"\n\n        def print_fragment(fragment, round_index=0):\n            # .act() supplies the round index as the second parameter\n            # Setting a default value means the callback is also\n            # compatible with .complete() and .respond().\n            print(fragment.content, end=\"\", flush=True)\n\n        model = lms.llm()\n        chat = lms.Chat(\"You are a task focused AI assistant\")\n\n        while True:\n            try:\n                user_input = input(\"You (leave blank to exit): \")\n            except EOFError:\n                print()\n                break\n            if not user_input:\n                break\n            chat.add_user_message(user_input)\n            print(\"Bot: \", end=\"\", flush=True)\n            model.act(\n                chat,\n                [create_file],\n                on_message=chat.append,\n                on_prediction_fragment=print_fragment,\n            )\n            print()\n\n```\n\n### Progress Callbacks\n\nComplex interactions with a tool using agent may take some time to process.\n\nThe regular progress callbacks for any prediction request are available,\nbut the expected capabilities differ from those for single round predictions.\n\n* `on_prompt_processing_progress`: called during prompt processing for each\n  prediction round. Receives the progress ratio (as a float) and the round\n  index as positional arguments.\n* `on_first_token`: called after prompt processing is complete for each prediction round.\n  Receives the round index as its sole argument.\n* `on_prediction_fragment`: called for each prediction fragment received by the client.\n  Receives the prediction fragment and the round index as positional arguments.\n* `on_message`: called with an assistant response message when each prediction round is\n  complete, and with tool result messages as each tool call request is completed.\n  Intended for appending received messages to a chat history instance, and hence\n  does *not* receive the round index as an argument.\n\nThe following additional callbacks are available to monitor the prediction rounds:\n\n* `on_round_start`: called before submitting the prediction request for each round.\n  Receives the round index as its sole argument.\n* `on_prediction_completed`: called after the prediction for the round has been completed,\n  but before any requested tool calls have been initiated. Receives the round's prediction\n  result as its sole argument. A round prediction result is a regular prediction result\n  with an additional `round_index` attribute.\n* `on_round_end`: called after any tool call requests for the round have been resolved.\n\nFinally, applications may request notifications when agents emit invalid tool requests:\n\n* `handle_invalid_tool_request`: called when a tool request was unable to be processed.\n  Receives the exception that is about to be reported, as well as the original tool\n  request that resulted in the problem. When no tool request is given, this is\n  purely a notification of an unrecoverable error before the agent interaction raises\n  the given exception (allowing the application to raise its own exception instead).\n  When a tool request is given, it indicates that rather than being raised locally,\n  the text description of the exception is going to be passed back to the agent\n  as the result of that failed tool request. In these cases, the callback may either\n  return `None` to indicate that the error description should be sent to the agent,\n  raise the given exception (or a different exception) locally, or return a text\n  string that should be sent to the agent instead of the error description.\n\nFor additional details on defining tools, and an example of overriding the invalid\ntool request handling to raise all exceptions locally instead of passing them to\nback the agent, refer to [Tool Definition](./tools.md).\n"])</script><script>self.__next_f.push([1,"3a:T18c7,"])</script><script>self.__next_f.push([1,"\nYou can define tools as regular Python functions and pass them to the model in the `act()` call.\nAlternatively, tools can be defined with `lmstudio.ToolFunctionDef` in order to control the\nname and description passed to the language model.\n\n## Anatomy of a Tool\n\nFollow one of the following examples to define functions as tools (the first approach\nis typically going to be the most convenient):\n\n```lms_code_snippet\n  variants:\n    \"Python function\":\n      language: python\n      code: |\n        # Type hinted functions with clear names and docstrings\n        # may be used directly as tool definitions\n        def add(a: int, b: int) -\u003e int:\n            \"\"\"Given two numbers a and b, returns the sum of them.\"\"\"\n            # The SDK ensures arguments are coerced to their specified types\n            return a + b\n\n        # Pass `add` directly to `act()` as a tool definition\n\n    \"ToolFunctionDef.from_callable\":\n      language: python\n      code: |\n        from lmstudio import ToolFunctionDef\n\n        def cryptic_name(a: int, b: int) -\u003e int:\n            return a + b\n\n        # Type hinted functions with cryptic names and missing or poor docstrings\n        # can be turned into clear tool definitions with `from_callable`\n        tool_def = ToolFunctionDef.from_callable(\n          cryptic_name,\n          name=\"add\",\n          description=\"Given two numbers a and b, returns the sum of them.\"\n        )\n        # Pass `tool_def` to `act()` as a tool definition\n\n    \"ToolFunctionDef\":\n      language: python\n      code: |\n        from lmstudio import ToolFunctionDef\n\n        def cryptic_name(a, b):\n            return a + b\n\n        # Functions without type hints can be used without wrapping them\n        # at runtime by defining a tool function directly.\n        tool_def = ToolFunctionDef(\n          name=\"add\",\n          description=\"Given two numbers a and b, returns the sum of them.\",\n          parameters={\n            \"a\": int,\n            \"b\": int,\n          },\n          implementation=cryptic_name,\n        )\n        # Pass `tool_def` to `act()` as a tool definition\n\n```\n\n**Important**: The tool name, description, and the parameter definitions are all passed to the model!\n\nThis means that your wording will affect the quality of the generation. Make sure to always provide a clear description of the tool so the model knows how to use it.\n\n## Tools with External Effects (like Computer Use or API Calls)\n\nTools can also have external effects, such as creating files or calling programs and even APIs. By implementing tools with external effects, you\ncan essentially turn your LLMs into autonomous agents that can perform tasks on your local machine.\n\n## Example: `create_file_tool`\n\n### Tool Definition\n\n```lms_code_snippet\n  title: \"create_file_tool.py\"\n  variants:\n    Python:\n      language: python\n      code: |\n        from pathlib import Path\n\n        def create_file(name: str, content: str):\n            \"\"\"Create a file with the given name and content.\"\"\"\n            dest_path = Path(name)\n            if dest_path.exists():\n                return \"Error: File already exists.\"\n            try:\n                dest_path.write_text(content, encoding=\"utf-8\")\n            except Exception as exc:\n                return \"Error: {exc!r}\"\n            return \"File created.\"\n\n```\n\n### Example code using the `create_file` tool:\n\n```lms_code_snippet\n  title: \"example.py\"\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n        from create_file_tool import create_file\n\n        model = lms.llm(\"qwen2.5-7b-instruct\")\n        model.act(\n          \"Please create a file named output.txt with your understanding of the meaning of life.\",\n          [create_file],\n        )\n```\n\n## Handling tool calling errors\n\nBy default, version 1.3.0 and later of the Python SDK will automatically convert\nexceptions raised by tool calls to text and report them back to the language model.\nIn many cases, when notified of an error in this way, a language model is able\nto either adjust its request to avoid the failure, or else accept the failure as\na valid response to its request (consider a prompt like `Attempt to divide 1 by 0\nusing the provided tool. Explain the result.`, where the expected\nresponse is an explanation of the `ZeroDivisionError` exception the Python\ninterpreter raises when instructed to divide by zero).\n\nThis error handling behaviour can be overridden using the `handle_invalid_tool_request`\ncallback. For example, the following code reverts the error handling back to raising\nexceptions locally in the client:\n\n```lms_code_snippet\n  title: \"example.py\"\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        def divide(numerator: float, denominator: float) -\u003e float:\n            \"\"\"Divide the given numerator by the given denominator. Return the result.\"\"\"\n            return numerator / denominator\n\n        model = lms.llm(\"qwen2.5-7b-instruct\")\n        chat = Chat()\n        chat.add_user_message(\n            \"Attempt to divide 1 by 0 using the tool. Explain the result.\"\n        )\n\n        def _raise_exc_in_client(\n            exc: LMStudioPredictionError, request: ToolCallRequest | None\n        ) -\u003e None:\n            raise exc\n\n        act_result = llm.act(\n            chat,\n            [divide],\n            handle_invalid_tool_request=_raise_exc_in_client,\n        )\n```\n\nWhen a tool request is passed in, the callback results are processed as follows:\n\n* `None`: the original exception text is passed to the LLM unmodified\n* a string: the returned string is passed to the LLM instead of the original\n  exception text\n* raising an exception (whether the passed in exception or a new exception):\n  the raised exception is propagated locally in the client, terminating the\n  prediction process\n\nIf no tool request is passed in, the callback invocation is a notification only,\nand the exception cannot be converted to text for passing pack to the LLM\n(although it can still be replaced with a different exception). These cases\nindicate failures in the expected communication with the server API that mean\nthe prediction process cannot reasonably continue, so if the callback doesn't\nraise an exception, the calling code will raise the original exception directly.\n"])</script><script>self.__next_f.push([1,"3b:T8d2,"])</script><script>self.__next_f.push([1,"\nModels use a tokenizer to internally convert text into \"tokens\" they can deal with more easily. LM Studio exposes this tokenizer for utility.\n\n## Tokenize\n\nYou can tokenize a string with a loaded LLM or embedding model using the SDK.\nIn the below examples, the LLM reference can be replaced with an\nembedding model reference without requiring any other changes.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n\n        tokens = model.tokenize(\"Hello, world!\")\n\n        print(tokens) # Array of token IDs.\n```\n\n## Count tokens\n\nIf you only care about the number of tokens, simply check the length of the resulting array.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        token_count = len(model.tokenize(\"Hello, world!\"))\n        print(\"Token count:\", token_count)\n```\n\n### Example: count context\n\nYou can determine if a given conversation fits into a model's context by doing the following:\n\n1. Convert the conversation to a string using the prompt template.\n2. Count the number of tokens in the string.\n3. Compare the token count to the model's context length.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        def does_chat_fit_in_context(model: lms.LLM, chat: lms.Chat) -\u003e bool:\n            # Convert the conversation to a string using the prompt template.\n            formatted = model.apply_prompt_template(chat)\n            # Count the number of tokens in the string.\n            token_count = len(model.tokenize(formatted))\n            # Get the current loaded context length of the model\n            context_length = model.get_context_length()\n            return token_count \u003c context_length\n\n        model = lms.llm()\n\n        chat = lms.Chat.from_history({\n            \"messages\": [\n                { \"role\": \"user\", \"content\": \"What is the meaning of life.\" },\n                { \"role\": \"assistant\", \"content\": \"The meaning of life is...\" },\n                # ... More messages\n            ]\n        })\n\n        print(\"Fits in context:\", does_chat_fit_in_context(model, chat))\n\n```\n"])</script><script>self.__next_f.push([1,"3c:T7c6,\nYou can iterate through locally available models using the downloaded model listing methods.\n\nThe listing results offer `.model()` and `.load_new_instance()` methods, which allow the\ndownloaded model reference to be converted in the full SDK handle for a loaded model.\n\n## Available Models on the LM Studio Server\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        downloaded = lms.list_downloaded_models()\n        llm_only = lms.list_downloaded_models(\"llm\")\n        embedding_only = lms.list_downloaded_models(\"embedding\")\n\n        for model in downloaded:\n            print(model)\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            downloaded = client.list_downloaded_models()\n            llm_only = client.llm.list_downloaded()\n            embedding_only = client.embedding.list_downloaded()\n\n        for model in downloaded:\n            print(model)\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            downloaded = await client.list_downloaded_models()\n            llm_only = await client.llm.list_downloaded()\n            embedding_only = await client.embedding.list_downloaded()\n\n        for model in downloaded:\n            print(model)\n\n```\nThis will give you results equivalent to using [`lms ls`](../../cli/ls) in the CLI.\n\n\n### Example output:\n\n```python\nDownloadedLlm(model_key='qwen2.5-7b-instruct-1m', display_name='Qwen2.5 7B Instruct 1M', architecture='qwen2', vision=False)\nDownloadedEmbeddingModel(model_key='text-embedding-nomic-embed-text-v1.5', display_name='Nomic Embed Text v1.5', architecture='nomic-bert')\n```\n3d:T5e0,\nYou can iterate through models loaded int"])</script><script>self.__next_f.push([1,"o memory using the functions and methods shown below.\n\nThe results are full SDK model handles, allowing access to all model functionality. \n\n\n## List Models Currently Loaded in Memory\n\nThis will give you results equivalent to using [`lms ps`](../../cli/ps) in the CLI.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        all_loaded_models = lms.list_loaded_models()\n        llm_only = lms.list_loaded_models(\"llm\")\n        embedding_only = lms.list_loaded_models(\"embedding\")\n\n        print(all_loaded_models)\n\n    Python (scoped resource API):\n      language: python\n      code: |\n        import lms\n\n        with lms.Client() as client:\n            all_loaded_models = client.list_loaded_models()\n            llm_only = client.llm.list_loaded()\n            embedding_only = client.embedding.list_loaded()\n\n            print(all_loaded_models)\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            all_loaded_models = await client.list_loaded_models()\n            llm_only = await client.llm.list_loaded()\n            embedding_only = await client.embedding.list_loaded()\n\n            print(all_loaded_models)\n\n```\n3e:T1bd7,"])</script><script>self.__next_f.push([1,"\nAI models are huge. It can take a while to load them into memory. LM Studio's SDK allows you to precisely control this process.\n\n**Model namespaces:**\n\n- LLMs are accessed through the `client.llm` namespace\n- Embedding models are accessed through the `client.embedding` namespace\n- `lmstudio.llm` is equivalent to `client.llm.model` on the default client\n- `lmstudio.embedding_model` is equivalent to `client.embedding.model` on the default client\n\n**Most commonly:**\n\n- Use `.model()` to get any currently loaded model\n- Use `.model(\"model-key\")` to use a specific model\n\n**Advanced (manual model management):**\n\n- Use `.load_new_instance(\"model-key\")` to load a new instance of a model\n- Use `.unload(\"model-key\")` or `model_handle.unload()` to unload a model from memory\n\n## Get the Current Model with `.model()`\n\nIf you already have a model loaded in LM Studio (either via the GUI or `lms load`),\nyou can use it by calling `.model()` without any arguments.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model()\n\n```\n\n## Get a Specific Model with `.model(\"model-key\")`\n\nIf you want to use a specific model, you can provide the model key as an argument to `.model()`.\n\n#### Get if Loaded, or Load if not\n\nCalling `.model(\"model-key\")` will load the model if it's not already loaded, or return the existing instance if it is.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen/qwen3-4b-2507\")\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\"qwen/qwen3-4b-2507\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\"qwen/qwen3-4b-2507\")\n\n```\n\n\u003c!--\nLearn more about the `.model()` method and the parameters it accepts in the [API Reference](../api-reference/model).\n--\u003e\n\n## Load a New Instance of a Model with `.load_new_instance()`\n\nUse `load_new_instance()` to load a new instance of a model, even if one already exists.\nThis allows you to have multiple instances of the same or different models loaded at the same time.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        client = lms.get_default_client()\n        model = client.llm.load_new_instance(\"qwen/qwen3-4b-2507\")\n        another_model = client.llm.load_new_instance(\"qwen/qwen3-4b-2507\", \"my-second-model\")\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.load_new_instance(\"qwen/qwen3-4b-2507\")\n            another_model = client.llm.load_new_instance(\"qwen/qwen3-4b-2507\", \"my-second-model\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.load_new_instance(\"qwen/qwen3-4b-2507\")\n            another_model = await client.llm.load_new_instance(\"qwen/qwen3-4b-2507\", \"my-second-model\")\n\n```\n\n\u003c!--\nLearn more about the `.load_new_instance()` method and the parameters it accepts in the [API Reference](../api-reference/load_new_instance).\n--\u003e\n\n### Note about Instance Identifiers\n\nIf you provide an instance identifier that already exists, the server will throw an error.\nSo if you don't really care, it's safer to not provide an identifier, in which case\nthe server will generate one for you. You can always check in the server tab in LM Studio, too!\n\n## Unload a Model from Memory with `.unload()`\n\nOnce you no longer need a model, you can unload it by simply calling `unload()` on its handle.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n        model.unload()\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n            model.unload()\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model()\n            await model.unload()\n\n```\n\n## Set Custom Load Config Parameters\n\nYou can also specify the same load-time configuration options when loading a model, such as Context Length and GPU offload.\n\nSee [load-time configuration](../llm-prediction/parameters) for more.\n\n## Set an Auto Unload Timer (TTL)\n\nYou can specify a _time to live_ for a model you load, which is the idle time (in seconds)\nafter the last request until the model unloads. See [Idle TTL](/docs/app/api/ttl-and-auto-evict) for more on this.\n\n```lms_protip\nIf you specify a TTL to `model()`, it will only apply if `model()` loads\na new instance, and will _not_ retroactively change the TTL of an existing instance.\n```\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen/qwen3-4b-2507\", ttl=3600)\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\"qwen/qwen3-4b-2507\", ttl=3600)\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\"qwen/qwen3-4b-2507\", ttl=3600)\n\n```\n\n\u003c!--\n(TODO?: Cover the JIT implications of setting a TTL, and the default TTL variations)\n--\u003e\n"])</script><script>self.__next_f.push([1,"3f:T8e5,"])</script><script>self.__next_f.push([1,"\nLLMs and embedding models, due to their fundamental architecture, have a property called `context length`, and more specifically a **maximum** context length. Loosely speaking, this is how many tokens the models can \"keep in memory\" when generating text or embeddings. Exceeding this limit will result in the model behaving erratically.\n\n## Use the `get_context_length()` function on the model object\n\nIt's useful to be able to check the context length of a model, especially as an extra check before providing potentially long input to the model.\n\n```lms_code_snippet\n  title: \"example.py\"\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        context_length = model.get_context_length()\n```\n\nThe `model` in the above code snippet is an instance of a loaded model you get from the `llm.model` method. See [Manage Models in Memory](../manage-models/loading) for more information.\n\n### Example: Check if the input will fit in the model's context window\n\nYou can determine if a given conversation fits into a model's context by doing the following:\n\n1. Convert the conversation to a string using the prompt template.\n2. Count the number of tokens in the string.\n3. Compare the token count to the model's context length.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        def does_chat_fit_in_context(model: lms.LLM, chat: lms.Chat) -\u003e bool:\n            # Convert the conversation to a string using the prompt template.\n            formatted = model.apply_prompt_template(chat)\n            # Count the number of tokens in the string.\n            token_count = len(model.tokenize(formatted))\n            # Get the current loaded context length of the model\n            context_length = model.get_context_length()\n            return token_count \u003c context_length\n\n        model = lms.llm()\n\n        chat = lms.Chat.from_history({\n            \"messages\": [\n                { \"role\": \"user\", \"content\": \"What is the meaning of life.\" },\n                { \"role\": \"assistant\", \"content\": \"The meaning of life is...\" },\n                # ... More messages\n            ]\n        })\n\n        print(\"Fits in context:\", does_chat_fit_in_context(model, chat))\n\n```\n"])</script><script>self.__next_f.push([1,"40:T54c,\n*Required Python SDK version*: **1.2.0**\n\nLM Studio allows you to configure certain parameters when loading a model\n[through the server UI](/docs/advanced/per-model) or [through the API](/docs/api/sdk/load-model).\n\nYou can retrieve the config with which a given model was loaded using the SDK.\n\nIn the below examples, the LLM reference can be replaced with an\nembedding model reference without requiring any other changes.\n\n```lms_protip\nContext length is a special case that [has its own method](/docs/api/sdk/get-context-length).\n```\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n\n        print(model.get_load_config())\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n\n            print(model.get_load_config())\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.Client() as client:\n            model = await client.llm.model()\n\n            print(await model.get_load_config())\n\n```\n41:T637,\nYou can access general information and metadata about a model itself from a loaded\ninstance of that model.\n\nIn the below examples, the LLM reference can be replaced with an\nembedding model reference without requiring any other changes.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n\n        print(model.get_info())\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n\n            print(model.get_info())\n\n    \"Python (asynchronous "])</script><script>self.__next_f.push([1,"API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model()\n\n            print(await model.get_info())\n\n```\n\n## Example output\n\n```python\nLlmInstanceInfo.from_dict({\n  \"architecture\": \"qwen2\",\n  \"contextLength\": 4096,\n  \"displayName\": \"Qwen2.5 7B Instruct 1M\",\n  \"format\": \"gguf\",\n  \"identifier\": \"qwen2.5-7b-instruct\",\n  \"instanceReference\": \"lpFZPBQjhSZPrFevGyY6Leq8\",\n  \"maxContextLength\": 1010000,\n  \"modelKey\": \"qwen2.5-7b-instruct-1m\",\n  \"paramsString\": \"7B\",\n  \"path\": \"lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF/Qwen2.5-7B-Instruct-1M-Q4_K_M.gguf\",\n  \"sizeBytes\": 4683073888,\n  \"trainedForToolUse\": true,\n  \"type\": \"llm\",\n  \"vision\": false\n})\n```\n42:T78f,\nThe SDK provides you a set of programmatic tools to interact with LLMs, embeddings models, and agentic flows.\n\n## Installing the SDK\n\n`lmstudio-js` is available as an npm package. You can install it using npm, yarn, or pnpm.\n\n```lms_code_snippet\n  variants:\n    npm:\n      language: bash\n      code: |\n        npm install @lmstudio/sdk --save\n    yarn:\n      language: bash\n      code: |\n        yarn add @lmstudio/sdk\n    pnpm:\n      language: bash\n      code: |\n        pnpm add @lmstudio/sdk\n```\n\nFor the source code and open source contribution, visit [lmstudio-js](https://github.com/lmstudio-ai/lmstudio-js) on GitHub.\n\n## Features\n\n- Use LLMs to [respond in chats](./typescript/llm-prediction/chat-completion) or predict [text completions](./typescript/llm-prediction/completion)\n- Define functions as tools, and turn LLMs into [autonomous agents](./typescript/agent/act) that run completely locally\n- [Load](./typescript/manage-models/loading), [configure](./typescript/llm-prediction/parameters), and [unload](./typescript/manage-models/loading) models from memory\n- Supports for both browser and any Node-compatibl"])</script><script>self.__next_f.push([1,"e environments\n- Generate embeddings for text, and more!\n\n## Quick Example: Chat with a Llama Model\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model(\"qwen/qwen3-4b-2507\");\n        const result = await model.respond(\"What is the meaning of life?\");\n\n        console.info(result.content);\n```\n\n### Getting Local Models\n\nThe above code requires the [qwen3-4b-2507](https://lmstudio.ai/models/qwen/qwen3-4b-2507). If you don't have the model, run the following command in the terminal to download it.\n\n```bash\nlms get qwen/qwen3-4b-2507\n```\n\nRead more about `lms get` in LM Studio's CLI [here](./cli/get).\n43:T1d9d,"])</script><script>self.__next_f.push([1,"\nUse `llm.respond(...)` to generate completions for a chat conversation.\n\n## Quick Example: Generate a Chat Response\n\nThe following snippet shows how to stream the AI's response to quick chat prompt.\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model();\n\n        for await (const fragment of model.respond(\"What is the meaning of life?\")) {\n          process.stdout.write(fragment.content);\n        }\n```\n\n## Obtain a Model\n\nFirst, you need to get a model handle. This can be done using the `model` method in the `llm` namespace. For example, here is how to use Qwen2.5 7B Instruct.\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n```\n\nThere are other ways to get a model handle. See [Managing Models in Memory](./../manage-models/loading) for more info.\n\n## Manage Chat Context\n\nThe input to the model is referred to as the \"context\". Conceptually, the model receives a multi-turn conversation as input, and it is asked to predict the assistant's response in that conversation.\n\n```lms_code_snippet\n  variants:\n    \"Using an array of messages\":\n      language: typescript\n      code: |\n        import { Chat } from \"@lmstudio/sdk\";\n\n        // Create a chat object from an array of messages.\n        const chat = Chat.from([\n          { role: \"system\", content: \"You are a resident AI philosopher.\" },\n          { role: \"user\", content: \"What is the meaning of life?\" },\n        ]);\n    \"Constructing a Chat object\":\n      language: typescript\n      code: |\n        import { Chat } from \"@lmstudio/sdk\";\n\n        // Create an empty chat object.\n        const chat = Chat.empty();\n\n        // Build the chat context by appending messages.\n        chat.append(\"system\", \"You are a resident AI philosopher.\");\n        chat.append(\"user\", \"What is the meaning of life?\");\n```\n\nSee [Working with Chats](./working-with-chats) for more information on managing chat context.\n\n\u003c!-- , and [`Chat`](./../api-reference/chat) for API reference for the `Chat` class. --\u003e\n\n## Generate a response\n\nYou can ask the LLM to predict the next response in the chat context using the `respond()` method.\n\n```lms_code_snippet\n  variants:\n    Streaming:\n      language: typescript\n      code: |\n        // The `chat` object is created in the previous step.\n        const prediction = model.respond(chat);\n\n        for await (const { content } of prediction) {\n          process.stdout.write(content);\n        }\n\n        console.info(); // Write a new line to prevent text from being overwritten by your shell.\n\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        // The `chat` object is created in the previous step.\n        const result = await model.respond(chat);\n\n        console.info(result.content);\n```\n\n## Customize Inferencing Parameters\n\nYou can pass in inferencing parameters as the second parameter to `.respond()`.\n\n```lms_code_snippet\n  variants:\n    Streaming:\n      language: typescript\n      code: |\n        const prediction = model.respond(chat, {\n          temperature: 0.6,\n          maxTokens: 50,\n        });\n\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        const result = await model.respond(chat, {\n          temperature: 0.6,\n          maxTokens: 50,\n        });\n```\n\nSee [Configuring the Model](./parameters) for more information on what can be configured.\n\n## Print prediction stats\n\nYou can also print prediction metadata, such as the model used for generation, number of generated\ntokens, time to first token, and stop reason.\n\n```lms_code_snippet\n  variants:\n    Streaming:\n      language: typescript\n      code: |\n        // If you have already iterated through the prediction fragments,\n        // doing this will not result in extra waiting.\n        const result = await prediction.result();\n\n        console.info(\"Model used:\", result.modelInfo.displayName);\n        console.info(\"Predicted tokens:\", result.stats.predictedTokensCount);\n        console.info(\"Time to first token (seconds):\", result.stats.timeToFirstTokenSec);\n        console.info(\"Stop reason:\", result.stats.stopReason);\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        // `result` is the response from the model.\n        console.info(\"Model used:\", result.modelInfo.displayName);\n        console.info(\"Predicted tokens:\", result.stats.predictedTokensCount);\n        console.info(\"Time to first token (seconds):\", result.stats.timeToFirstTokenSec);\n        console.info(\"Stop reason:\", result.stats.stopReason);\n```\n\n## Example: Multi-turn Chat\n\n\u003c!-- TODO: Probably needs polish here: --\u003e\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { Chat, LMStudioClient } from \"@lmstudio/sdk\";\n        import { createInterface } from \"readline/promises\";\n\n        const rl = createInterface({ input: process.stdin, output: process.stdout });\n        const client = new LMStudioClient();\n        const model = await client.llm.model();\n        const chat = Chat.empty();\n\n        while (true) {\n          const input = await rl.question(\"You: \");\n          // Append the user input to the chat\n          chat.append(\"user\", input);\n\n          const prediction = model.respond(chat, {\n            // When the model finish the entire message, push it to the chat\n            onMessage: (message) =\u003e chat.append(message),\n          });\n          process.stdout.write(\"Bot: \");\n          for await (const { content } of prediction) {\n            process.stdout.write(content);\n          }\n          process.stdout.write(\"\\n\");\n        }\n```\n\n\u003c!-- ### Progress callbacks\n\nTODO: Cover onFirstToken callback (Python SDK has this now)\n\nLong prompts will often take a long time to first token, i.e. it takes the model a long time to process your prompt.\nIf you want to get updates on the progress of this process, you can provide a float callback to `respond`\nthat receives a float from 0.0-1.0 representing prompt processing progress.\n\n```lms_code_snippet\n  variants:\n    Python:\n      language: python\n      code: |\n        import lmstudio as lm\n\n        llm = lm.llm()\n\n        response = llm.respond(\n            \"What is LM Studio?\",\n            on_progress: lambda progress: print(f\"{progress*100}% complete\")\n        )\n\n    Python (with scoped resources):\n      language: python\n      code: |\n        import lmstudio\n\n        with lmstudio.Client() as client:\n            llm = client.llm.model()\n\n            response = llm.respond(\n                \"What is LM Studio?\",\n                on_progress: lambda progress: print(f\"{progress*100}% processed\")\n            )\n\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const llm = await client.llm.model();\n\n        const prediction = llm.respond(\n          \"What is LM Studio?\",\n          {onPromptProcessingProgress: (progress) =\u003e process.stdout.write(`${progress*100}% processed`)});\n```\n\n### Prediction configuration\n\nYou can also specify the same prediction configuration options as you could in the\nin-app chat window sidebar. Please consult your specific SDK to see exact syntax. --\u003e\n"])</script><script>self.__next_f.push([1,"44:Ta86,"])</script><script>self.__next_f.push([1,"\nSDK methods such as `model.respond()`, `model.applyPromptTemplate()`, or `model.act()`\ntakes in a chat parameter as an input. There are a few ways to represent a chat in the SDK.\n\n## Option 1: Array of Messages\n\nYou can use an array of messages to represent a chat. Here is an example with the `.respond()` method.\n\n```lms_code_snippet\nvariants:\n  \"Text-only\":\n    language: typescript\n    code: |\n      const prediction = model.respond([\n        { role: \"system\", content: \"You are a resident AI philosopher.\" },\n        { role: \"user\", content: \"What is the meaning of life?\" },\n      ]);\n  With Images:\n    language: typescript\n    code: |\n      const image = await client.files.prepareImage(\"/path/to/image.jpg\");\n\n      const prediction = model.respond([\n        { role: \"system\", content: \"You are a state-of-art object recognition system.\" },\n        { role: \"user\", content: \"What is this object?\", images: [image] },\n      ]);\n```\n\n## Option 2: Input a Single String\n\nIf your chat only has one single user message, you can use a single string to represent the chat. Here is an example with the `.respond` method.\n\n```lms_code_snippet\nvariants:\n  TypeScript:\n    language: typescript\n    code: |\n      const prediction = model.respond(\"What is the meaning of life?\");\n```\n\n## Option 3: Using the `Chat` Helper Class\n\nFor more complex tasks, it is recommended to use the `Chat` helper classes. It provides various commonly used methods to manage the chat. Here is an example with the `Chat` class.\n\n```lms_code_snippet\nvariants:\n  \"Text-only\":\n    language: typescript\n    code: |\n      const chat = Chat.empty();\n      chat.append(\"system\", \"You are a resident AI philosopher.\");\n      chat.append(\"user\", \"What is the meaning of life?\");\n\n      const prediction = model.respond(chat);\n  With Images:\n    language: typescript\n    code: |\n      const image = await client.files.prepareImage(\"/path/to/image.jpg\");\n\n      const chat = Chat.empty();\n      chat.append(\"system\", \"You are a state-of-art object recognition system.\");\n      chat.append(\"user\", \"What is this object?\", { images: [image] });\n\n      const prediction = model.respond(chat);\n```\n\nYou can also quickly construct a `Chat` object using the `Chat.from` method.\n\n```lms_code_snippet\nvariants:\n  \"Array of messages\":\n    language: typescript\n    code: |\n      const chat = Chat.from([\n        { role: \"system\", content: \"You are a resident AI philosopher.\" },\n        { role: \"user\", content: \"What is the meaning of life?\" },\n      ]);\n  \"Single string\":\n    language: typescript\n    code: |\n      // This constructs a chat with a single user message\n      const chat = Chat.from(\"What is the meaning of life?\");\n```\n"])</script><script>self.__next_f.push([1,"45:T816,"])</script><script>self.__next_f.push([1,"\nSometimes you may want to halt a prediction before it finishes. For example, the user might change their mind or your UI may navigate away. `lmstudio-js` provides two simple ways to cancel a running prediction.\n\n## 1. Call `.cancel()` on the prediction\n\nEvery prediction method returns an `OngoingPrediction` instance. Calling `.cancel()` stops generation and causes the final `stopReason` to be `\"userStopped\"`. In the example below we schedule the cancel call on a timer:\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n\n        const prediction = model.respond(\"What is the meaning of life?\", {\n          maxTokens: 50,\n        });\n        setTimeout(() =\u003e prediction.cancel(), 1000); // cancel after 1 second\n\n        const result = await prediction.result();\n        console.info(result.stats.stopReason); // \"userStopped\"\n```\n\n## 2. Use an `AbortController`\n\nIf your application already uses an `AbortController` to propagate cancellation, you can pass its `signal` to the prediction method. Aborting the controller stops the prediction with the same `stopReason`:\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n\n        const controller = new AbortController();\n        const prediction = model.respond(\"What is the meaning of life?\", {\n          maxTokens: 50,\n          signal: controller.signal,\n        });\n        setTimeout(() =\u003e controller.abort(), 1000); // cancel after 1 second\n\n        const result = await prediction.result();\n        console.info(result.stats.stopReason); // \"userStopped\"\n```\n\nBoth approaches halt generation immediately, and the returned stats indicate that the prediction ended because you stopped it.\n"])</script><script>self.__next_f.push([1,"46:T776,\nSome models, known as VLMs (Vision-Language Models), can accept images as input. You can pass images to the model using the `.respond()` method.\n\n### Prerequisite: Get a VLM (Vision-Language Model)\n\nIf you don't yet have a VLM, you can download a model like `qwen2-vl-2b-instruct` using the following command:\n\n```bash\nlms get qwen2-vl-2b-instruct\n```\n\n## 1. Instantiate the Model\n\nConnect to LM Studio and obtain a handle to the VLM (Vision-Language Model) you want to use.\n\n```lms_code_snippet\n  variants:\n    Example:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model(\"qwen2-vl-2b-instruct\");\n```\n\n## 2. Prepare the Image\n\nUse the `client.files.prepareImage()` method to get a handle to the image that can be subsequently passed to the model.\n\n```lms_code_snippet\n  variants:\n    Example:\n      language: typescript\n      code: |\n        const imagePath = \"/path/to/image.jpg\"; // Replace with the path to your image\n        const image = await client.files.prepareImage(imagePath);\n\n```\n\nIf you only have the image in the form of a base64 string, you can use the `client.files.prepareImageBase64()` method instead.\n\n```lms_code_snippet\n  variants:\n    Example:\n      language: typescript\n      code: |\n        const imageBase64 = \"Your base64 string here\";\n        const image = await client.files.prepareImageBase64(imageBase64);\n```\n\nThe LM Studio server supports JPEG, PNG, and WebP image formats.\n\n## 3. Pass the Image to the Model in `.respond()`\n\nGenerate a prediction by passing the image to the model in the `.respond()` method.\n\n```lms_code_snippet\n  variants:\n    Example:\n      language: typescript\n      code: |\n        const prediction = model.respond([\n          { role: \"user\", content: \"Describe this image please\", images: [image] },\n        ]);\n```\n47:T15ac,"])</script><script>self.__next_f.push([1,"\nYou can enforce a particular response format from an LLM by providing a schema (JSON or `zod`) to the `.respond()` method. This guarantees that the model's output conforms to the schema you provide.\n\n## Enforce Using a `zod` Schema\n\nIf you wish the model to generate JSON that satisfies a given schema, it is recommended to provide\nthe schema using [`zod`](https://zod.dev/). When a `zod` schema is provided, the prediction result will contain an extra field `parsed`, which contains parsed, validated, and typed result.\n\n#### Define a `zod` Schema\n\n```ts\nimport { z } from \"zod\";\n\n// A zod schema for a book\nconst bookSchema = z.object({\n  title: z.string(),\n  author: z.string(),\n  year: z.number().int(),\n});\n```\n\n#### Generate a Structured Response\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        const result = await model.respond(\"Tell me about The Hobbit.\",\n          { structured: bookSchema },\n          maxTokens: 100, // Recommended to avoid getting stuck\n        );\n\n        const book = result.parsed;\n        console.info(book);\n        //           ^\n        // Note that `book` is now correctly typed as { title: string, author: string, year: number }\n\n    Streaming:\n      language: typescript\n      code: |\n        const prediction = model.respond(\"Tell me about The Hobbit.\",\n          { structured: bookSchema },\n          maxTokens: 100, // Recommended to avoid getting stuck\n        );\n\n        for await (const { content } of prediction) {\n          process.stdout.write(content);\n        }\n        process.stdout.write(\"\\n\");\n\n        // Get the final structured result\n        const result = await prediction.result();\n        const book = result.parsed;\n\n        console.info(book);\n        //           ^\n        // Note that `book` is now correctly typed as { title: string, author: string, year: number }\n```\n\n## Enforce Using a JSON Schema\n\nYou can also enforce a structured response using a JSON schema.\n\n#### Define a JSON Schema\n\n```ts\n// A JSON schema for a book\nconst schema = {\n  type: \"object\",\n  properties: {\n    title: { type: \"string\" },\n    author: { type: \"string\" },\n    year: { type: \"integer\" },\n  },\n  required: [\"title\", \"author\", \"year\"],\n};\n```\n\n#### Generate a Structured Response\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        const result = await model.respond(\"Tell me about The Hobbit.\", {\n          structured: {\n            type: \"json\",\n            jsonSchema: schema,\n          },\n          maxTokens: 100, // Recommended to avoid getting stuck\n        });\n\n        const book = JSON.parse(result.content);\n        console.info(book);\n    Streaming:\n      language: typescript\n      code: |\n        const prediction = model.respond(\"Tell me about The Hobbit.\", {\n          structured: {\n            type: \"json\",\n            jsonSchema: schema,\n          },\n          maxTokens: 100, // Recommended to avoid getting stuck\n        });\n\n        for await (const { content } of prediction) {\n          process.stdout.write(content);\n        }\n        process.stdout.write(\"\\n\");\n\n        const result = await prediction.result();\n        const book = JSON.parse(result.content);\n\n        console.info(\"Parsed\", book);\n```\n\n```lms_warning\nStructured generation works by constraining the model to only generate tokens that conform to the provided schema. This ensures valid output in normal cases, but comes with two important limitations:\n\n1. Models (especially smaller ones) may occasionally get stuck in an unclosed structure (like an open bracket), when they \"forget\" they are in such structure and cannot stop due to schema requirements. Thus, it is recommended to always include a `maxTokens` parameter to prevent infinite generation.\n\n2. Schema compliance is only guaranteed for complete, successful generations. If generation is interrupted (by cancellation, reaching the `maxTokens` limit, or other reasons), the output will likely violate the schema. With `zod` schema input, this will raise an error; with JSON schema, you'll receive an invalid string that doesn't satisfy schema.\n```\n\n\u003c!-- ## Overview\n\nOnce you have [downloaded and loaded](/docs/basics/index) a large language model,\nyou can use it to respond to input through the API. This article covers getting JSON structured output, but you can also\n[request text completions](/docs/api/sdk/completion),\n[request chat responses](/docs/api/sdk/chat-completion), and\n[use a vision-language model to chat about images](/docs/api/sdk/image-input).\n\n### Usage\n\nCertain models are trained to output valid JSON data that conforms to\na user-provided schema, which can be used programmatically in applications\nthat need structured data. This structured data format is supported by both\n[`complete`](/docs/api/sdk/completion) and [`respond`](/docs/api/sdk/chat-completion)\nmethods, and relies on Pydantic in Python and Zod in TypeScript.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        const Book = z.object({\n          title: z.string(),\n          author: z.string(),\n          year: z.number().int()\n        })\n\n        const client = new LMStudioClient();\n        const llm = await client.llm.model();\n\n        const response = await llm.respond(\n          \"Tell me about The Hobbit.\",\n          { structured: Book },\n        )\n\n        console.log(response.content.title)\n``` --\u003e\n"])</script><script>self.__next_f.push([1,"48:T755,\nSpeculative decoding is a technique that can substantially increase the generation speed of large language models (LLMs) without reducing response quality. See [Speculative Decoding](./../../app/advanced/speculative-decoding) for more info.\n\nTo use speculative decoding in `lmstudio-js`, simply provide a `draftModel` parameter when performing the prediction. You do not need to load the draft model separately.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n\n        const mainModelKey = \"qwen2.5-7b-instruct\";\n        const draftModelKey = \"qwen2.5-0.5b-instruct\";\n\n        const model = await client.llm.model(mainModelKey);\n        const result = await model.respond(\"What are the prime numbers between 0 and 100?\", {\n          draftModel: draftModelKey,\n        });\n\n        const { content, stats } = result;\n        console.info(content);\n        console.info(`Accepted ${stats.acceptedDraftTokensCount}/${stats.predictedTokensCount} tokens`);\n\n\n    Streaming:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n\n        const mainModelKey = \"qwen2.5-7b-instruct\";\n        const draftModelKey = \"qwen2.5-0.5b-instruct\";\n\n        const model = await client.llm.model(mainModelKey);\n        const prediction = model.respond(\"What are the prime numbers between 0 and 100?\", {\n          draftModel: draftModelKey,\n        });\n\n        for await (const { content } of prediction) {\n          process.stdout.write(content);\n        }\n        process.stdout.write(\"\\n\");\n\n        const { stats } = await prediction.result();\n        console.info(`Accepted ${stats.acceptedDraftTokensCount}/${stats.predictedTokensCount} tokens`);\n```\n49:T148b,"])</script><script>self.__next_f.push([1,"\nUse `llm.complete(...)` to generate text completions from a loaded language model. Text completions mean sending an non-formatted string to the model with the expectation that the model will complete the text.\n\nThis is different from multi-turn chat conversations. For more information on chat completions, see [Chat Completions](./chat-completion).\n\n## 1. Instantiate a Model\n\nFirst, you need to load a model to generate completions from. This can be done using the `model` method on the `llm` handle.\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n```\n\n## 2. Generate a Completion\n\nOnce you have a loaded model, you can generate completions by passing a string to the `complete` method on the `llm` handle.\n\n```lms_code_snippet\n  variants:\n    Streaming:\n      language: typescript\n      code: |\n        const completion = model.complete(\"My name is\", {\n          maxTokens: 100,\n        });\n\n        for await (const { content } of completion) {\n          process.stdout.write(content);\n        }\n\n        console.info(); // Write a new line for cosmetic purposes\n\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        const completion = await model.complete(\"My name is\", {\n          maxTokens: 100,\n        });\n\n        console.info(completion.content);\n```\n\n## 3. Print Prediction Stats\n\nYou can also print prediction metadata, such as the model used for generation, number of generated tokens, time to first token, and stop reason.\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        console.info(\"Model used:\", completion.modelInfo.displayName);\n        console.info(\"Predicted tokens:\", completion.stats.predictedTokensCount);\n        console.info(\"Time to first token (seconds):\", completion.stats.timeToFirstTokenSec);\n        console.info(\"Stop reason:\", completion.stats.stopReason);\n```\n\n## Example: Get an LLM to Simulate a Terminal\n\nHere's an example of how you might use the `complete` method to simulate a terminal.\n\n```lms_code_snippet\n  title: \"terminal-sim.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        import { createInterface } from \"node:readline/promises\";\n\n        const rl = createInterface({ input: process.stdin, output: process.stdout });\n        const client = new LMStudioClient();\n        const model = await client.llm.model();\n        let history = \"\";\n\n        while (true) {\n          const command = await rl.question(\"$ \");\n          history += \"$ \" + command + \"\\n\";\n\n          const prediction = model.complete(history, { stopStrings: [\"$\"] });\n          for await (const { content } of prediction) {\n            process.stdout.write(content);\n          }\n          process.stdout.write(\"\\n\");\n\n          const { content } = await prediction.result();\n          history += content;\n        }\n```\n\n\u003c!-- ## Advanced Usage\n\n### Prediction metadata\n\nPrediction responses are really returned as `PredictionResult` objects that contain additional dot-accessible metadata about the inference request.\nThis entails info about the model used, the configuration with which it was loaded, and the configuration for this particular prediction. It also provides\ninference statistics like stop reason, time to first token, tokens per second, and number of generated tokens.\n\nPlease consult your specific SDK to see exact syntax.\n\n### Progress callbacks\n\nTODO: TS has onFirstToken callback which Python does not\n\nLong prompts will often take a long time to first token, i.e. it takes the model a long time to process your prompt.\nIf you want to get updates on the progress of this process, you can provide a float callback to `complete`\nthat receives a float from 0.0-1.0 representing prompt processing progress.\n\n```lms_code_snippet\n  variants:\n    Python:\n      language: python\n      code: |\n        import lmstudio as lm\n\n        llm = lm.llm()\n\n        completion = llm.complete(\n            \"My name is\",\n            on_progress: lambda progress: print(f\"{progress*100}% complete\")\n        )\n\n    Python (with scoped resources):\n      language: python\n      code: |\n        import lmstudio\n\n        with lmstudio.Client() as client:\n            llm = client.llm.model()\n\n            completion = llm.complete(\n                \"My name is\",\n                on_progress: lambda progress: print(f\"{progress*100}% processed\")\n            )\n\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const llm = await client.llm.model();\n\n        const prediction = llm.complete(\n          \"My name is\",\n          {onPromptProcessingProgress: (progress) =\u003e process.stdout.write(`${progress*100}% processed`)});\n```\n\n### Prediction configuration\n\nYou can also specify the same prediction configuration options as you could in the\nin-app chat window sidebar. Please consult your specific SDK to see exact syntax. --\u003e\n"])</script><script>self.__next_f.push([1,"4a:T90d,"])</script><script>self.__next_f.push([1,"\nYou can customize both inference-time and load-time parameters for your model. Inference parameters can be set on a per-request basis, while load parameters are set when loading the model.\n\n# Inference Parameters\n\nSet inference-time parameters such as `temperature`, `maxTokens`, `topP` and more.\n\n```lms_code_snippet\n  variants:\n    \".respond()\":\n      language: typescript\n      code: |\n        const prediction = model.respond(chat, {\n          temperature: 0.6,\n          maxTokens: 50,\n        });\n    \".complete()\":\n        language: typescript\n        code: |\n          const prediction = model.complete(prompt, {\n            temperature: 0.6,\n            maxTokens: 50,\n            stop: [\"\\n\\n\"],\n          });\n```\n\nSee [`LLMPredictionConfigInput`](./../api-reference/llm-prediction-config-input) for all configurable fields.\n\nAnother useful inference-time configuration parameter is [`structured`](\u003c(./structured-responses)\u003e), which allows you to rigorously enforce the structure of the output using a JSON or zod schema.\n\n# Load Parameters\n\nSet load-time parameters such as the context length, GPU offload ratio, and more.\n\n### Set Load Parameters with `.model()`\n\nThe `.model()` retrieves a handle to a model that has already been loaded, or loads a new one on demand (JIT loading).\n\n**Note**: if the model is already loaded, the configuration will be **ignored**.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\", {\n          config: {\n            contextLength: 8192,\n            gpu: {\n              ratio: 0.5,\n            },\n          },\n        });\n```\n\nSee [`LLMLoadModelConfig`](./../api-reference/llm-load-model-config) for all configurable fields.\n\n### Set Load Parameters with `.load()`\n\nThe `.load()` method creates a new model instance and loads it with the specified configuration.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        const model = await client.llm.load(\"qwen2.5-7b-instruct\", {\n          config: {\n            contextLength: 8192,\n            gpu: {\n              ratio: 0.5,\n            },\n          },\n        });\n```\n\nSee [`LLMLoadModelConfig`](./../api-reference/llm-load-model-config) for all configurable fields.\n"])</script><script>self.__next_f.push([1,"4b:T16f8,"])</script><script>self.__next_f.push([1,"\n## Automatic tool calling\n\nWe introduce the concept of execution \"rounds\" to describe the combined process of running a tool, providing its output to the LLM, and then waiting for the LLM to decide what to do next.\n\n**Execution Round**\n\n```\n â€¢ run a tool -\u003e\n â†‘   â€¢ provide the result to the LLM -\u003e\n â”‚       â€¢ wait for the LLM to generate a response\n â”‚\n â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””âž” (return)\n```\n\nA model might choose to run tools multiple times before returning a final result. For example, if the LLM is writing code, it might choose to compile or run the program, fix errors, and then run it again, rinse and repeat until it gets the desired result.\n\nWith this in mind, we say that the `.act()` API is an automatic \"multi-round\" tool calling API.\n\n\n### Quick Example\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient, tool } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        const client = new LMStudioClient();\n\n        const multiplyTool = tool({\n          name: \"multiply\",\n          description: \"Given two numbers a and b. Returns the product of them.\",\n          parameters: { a: z.number(), b: z.number() },\n          implementation: ({ a, b }) =\u003e a * b,\n        });\n\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n        await model.act(\"What is the result of 12345 multiplied by 54321?\", [multiplyTool], {\n          onMessage: (message) =\u003e console.info(message.toString()),\n        });\n```\n\n\u003e **_NOTE:_**  at this time, this code expects zod v3\n\n### What does it mean for an LLM to \"use a tool\"?\n\nLLMs are largely text-in, text-out programs. So, you may ask \"how can an LLM use a tool?\". The answer is that some LLMs are trained to ask the human to call the tool for them, and expect the tool output to to be provided back in some format.\n\nImagine you're giving computer support to someone over the phone. You might say things like \"run this command for me ... OK what did it output? ... OK now click there and tell me what it says ...\". In this case you're the LLM! And you're \"calling tools\" vicariously through the person on the other side of the line.\n\n\n### Important: Model Selection\n\nThe model selected for tool use will greatly impact performance.\n\nSome general guidance when selecting a model:\n\n- Not all models are capable of intelligent tool use\n- Bigger is better (i.e., a 7B parameter model will generally perform better than a 3B parameter model)\n- We've observed [Qwen2.5-7B-Instruct](https://model.lmstudio.ai/download/lmstudio-community/Qwen2.5-7B-Instruct-GGUF) to perform well in a wide variety of cases\n- This guidance may change\n\n### Example: Multiple Tools\n\nThe following code demonstrates how to provide multiple tools in a single `.act()` call.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient, tool } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        const client = new LMStudioClient();\n\n        const additionTool = tool({\n          name: \"add\",\n          description: \"Given two numbers a and b. Returns the sum of them.\",\n          parameters: { a: z.number(), b: z.number() },\n          implementation: ({ a, b }) =\u003e a + b,\n        });\n\n        const isPrimeTool = tool({\n          name: \"isPrime\",\n          description: \"Given a number n. Returns true if n is a prime number.\",\n          parameters: { n: z.number() },\n          implementation: ({ n }) =\u003e {\n            if (n \u003c 2) return false;\n            const sqrt = Math.sqrt(n);\n            for (let i = 2; i \u003c= sqrt; i++) {\n              if (n % i === 0) return false;\n            }\n            return true;\n          },\n        });\n\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n        await model.act(\n          \"Is the result of 12345 + 45668 a prime? Think step by step.\",\n          [additionTool, isPrimeTool],\n          { onMessage: (message) =\u003e console.info(message.toString()) },\n        );\n```\n\n### Example: Chat Loop with Create File Tool\n\nThe following code creates a conversation loop with an LLM agent that can create files.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { Chat, LMStudioClient, tool } from \"@lmstudio/sdk\";\n        import { existsSync } from \"fs\";\n        import { writeFile } from \"fs/promises\";\n        import { createInterface } from \"readline/promises\";\n        import { z } from \"zod\";\n\n        const rl = createInterface({ input: process.stdin, output: process.stdout });\n        const client = new LMStudioClient();\n        const model = await client.llm.model();\n        const chat = Chat.empty();\n\n        const createFileTool = tool({\n          name: \"createFile\",\n          description: \"Create a file with the given name and content.\",\n          parameters: { name: z.string(), content: z.string() },\n          implementation: async ({ name, content }) =\u003e {\n            if (existsSync(name)) {\n              return \"Error: File already exists.\";\n            }\n            await writeFile(name, content, \"utf-8\");\n            return \"File created.\";\n          },\n        });\n\n        while (true) {\n          const input = await rl.question(\"You: \");\n          // Append the user input to the chat\n          chat.append(\"user\", input);\n\n          process.stdout.write(\"Bot: \");\n          await model.act(chat, [createFileTool], {\n            // When the model finish the entire message, push it to the chat\n            onMessage: (message) =\u003e chat.append(message),\n            onPredictionFragment: ({ content }) =\u003e {\n              process.stdout.write(content);\n            },\n          });\n          process.stdout.write(\"\\n\");\n        }\n```\n"])</script><script>self.__next_f.push([1,"4c:Tad2,"])</script><script>self.__next_f.push([1,"\nYou can define tools with the `tool()` function and pass them to the model in the `act()` call.\n\n## Anatomy of a Tool\n\nFollow this standard format to define functions as tools:\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        const exampleTool = tool({\n          // The name of the tool\n          name: \"add\",\n\n          // A description of the tool\n          description: \"Given two numbers a and b. Returns the sum of them.\",\n\n          // zod schema of the parameters\n          parameters: { a: z.number(), b: z.number() },\n\n          // The implementation of the tool. Just a regular function.\n          implementation: ({ a, b }) =\u003e a + b,\n        });\n```\n\n**Important**: The tool name, description, and the parameter definitions are all passed to the model!\n\nThis means that your wording will affect the quality of the generation. Make sure to always provide a clear description of the tool so the model knows how to use it.\n\n## Tools with External Effects (like Computer Use or API Calls)\n\nTools can also have external effects, such as creating files or calling programs and even APIs. By implementing tools with external effects, you\ncan essentially turn your LLMs into autonomous agents that can perform tasks on your local machine.\n\n## Example: `createFileTool`\n\n### Tool Definition\n\n```lms_code_snippet\n  title: \"createFileTool.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool } from \"@lmstudio/sdk\";\n        import { existsSync } from \"fs\";\n        import { writeFile } from \"fs/promises\";\n        import { z } from \"zod\";\n\n        const createFileTool = tool({\n          name: \"createFile\",\n          description: \"Create a file with the given name and content.\",\n          parameters: { name: z.string(), content: z.string() },\n          implementation: async ({ name, content }) =\u003e {\n            if (existsSync(name)) {\n              return \"Error: File already exists.\";\n            }\n            await writeFile(name, content, \"utf-8\");\n            return \"File created.\";\n          },\n        });\n```\n\n### Example code using the `createFile` tool:\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        import { createFileTool } from \"./createFileTool\";\n\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n        await model.act(\n          \"Please create a file named output.txt with your understanding of the meaning of life.\",\n          [createFileTool],\n        );\n```\n"])</script><script>self.__next_f.push([1,"4d:T71d,\nPlugins extend LM Studio's functionality by providing \"hook functions\" that execute at specific points during operation.\n\nPlugins are currently written in JavaScript/TypeScript and run on Node.js v20.18.0. Python support is in development.\n\n## Getting Started\n\nLM Studio includes Node.js, so no separate installation is required.\n\n### Create a new plugin\n\nTo create a new plugin, navigate to LM Studio... [TO BE CONTINUED]\n\n### Run a plugin in development mode\n\nOnce you've created a plugin, run this command in the plugin directory to start development mode:\n\n```bash\nlms dev\n```\n\nYour plugin will appear in LM Studio's plugin list. Development mode automatically rebuilds and reloads your plugin when you make code changes.\n\nYou only need `lms dev` during development. When the plugin is installed, LM Studio automatically runs them as needed. Learn more about distributing and installing plugins in the [Sharing Plugins](./plugins/sharing) section.\n\n## Next Steps\n\n- [Tools Providers](./plugins/tools-provider)\n\n  Give models extra capabilities by creating tools they can use during generation, like accessing external APIs or performing calculations.\n\n- [Prompt Preprocessors](./plugins/prompt-preprocessor)\n\n  Modify user input before it reaches the model - handle file uploads, inject context, or transform queries.\n\n- [Generators](./plugins/generator)\n\n  Create custom text generation sources that replace the local model, perfect for online model adapters.\n\n- [Custom Configurations](./plugins/custom-configuration)\n\n  Add configuration UIs so users can customize your plugin's behavior.\n\n- [Third-Party Dependencies](./plugins/dependencies)\n\n  Use npm packages to leverage existing libraries in your plugins.\n\n- [Sharing Plugins](./plugins/publish-plugins)\n\n  Package and share your plugins with the community.\n4e:Tb20,"])</script><script>self.__next_f.push([1,"\nTo setup a tools provider, first create the a file `toolsProvider.ts` in your plugin's `src` directory:\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool, Tool, ToolsProviderController } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n        import { existsSync } from \"fs\";\n        import { writeFile } from \"fs/promises\";\n        import { join } from \"path\";\n\n        export async function toolsProvider(ctl: ToolsProviderController) {\n          const tools: Tool[] = [];\n\n          const createFileTool = tool({\n            // Name of the tool, this will be passed to the model. Aim for concise, descriptive names\n            name: `create_file`,\n            // Your description here, more details will help the model to understand when to use the tool\n            description: \"Create a file with the given name and content.\",\n            parameters: { file_name: z.string(), content: z.string() },\n            implementation: async ({ file_name, content }) =\u003e {\n              const filePath = join(ctl.getWorkingDirectory(), file_name);\n              if (existsSync(filePath)) {\n                return \"Error: File already exists.\";\n              }\n              await writeFile(filePath, content, \"utf-8\");\n              return \"File created.\";\n            },\n          });\n          tools.push(createFileTool);\n\n          return tools;\n        }\n```\n\nThe above tools provider defines a single tool called `create_file` that allows the model to create a file with a specified name and content inside the working directory. You can learn more about defining tools in [Tool Definition](../agent/tools).\n\nThen register the tools provider in your plugin's `index.ts`:\n\n```lms_code_snippet\n  title: \"src/index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        // ... other imports ...\n        import { toolsProvider } from \"./toolsProvider\";\n\n        export async function main(context: PluginContext) {\n          // ... other plugin setup code ...\n\n          // Register the tools provider.\n          context.withToolsProvider(toolsProvider); // \u003c-- Register the tools provider\n\n          // ... other plugin setup code ...\n        }\n```\n\nNow, you can try to ask the LLM to create a file, and it should be able to do so using the tool you just created.\n\n## Tips\n\n- **Use Descriptive Names and Descriptions**: When defining tools, use descriptive names and detailed descriptions. This helps the model understand when and how to use each tool effectively.\n- **Return Errors as Strings**: Sometimes, the model may make a mistake when calling a tool. In such cases, you can return an error message as a string. In most cases, the model will try to correct itself and call the tool again with the correct parameters.\n"])</script><script>self.__next_f.push([1,"4f:T7ea,\nA tools provider can define multiple tools for the model to use. Simply create additional tool instances and add them to the tools array.\n\nIn the example below, we add a second tool to read the content of a file:\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool, Tool, ToolsProviderController } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n        import { existsSync } from \"fs\";\n        import { readFile, writeFile } from \"fs/promises\";\n        import { join } from \"path\";\n\n        export async function toolsProvider(ctl: ToolsProviderController) {\n          const tools: Tool[] = [];\n\n          const createFileTool = tool({\n            name: `create_file`,\n            description: \"Create a file with the given name and content.\",\n            parameters: { file_name: z.string(), content: z.string() },\n            implementation: async ({ file_name, content }) =\u003e {\n              const filePath = join(ctl.getWorkingDirectory(), file_name);\n              if (existsSync(filePath)) {\n                return \"Error: File already exists.\";\n              }\n              await writeFile(filePath, content, \"utf-8\");\n              return \"File created.\";\n            },\n          });\n          tools.push(createFileTool); // First tool\n\n          const readFileTool = tool({\n            name: `read_file`,\n            description: \"Read the content of a file with the given name.\",\n            parameters: { file_name: z.string() },\n            implementation: async ({ file_name }) =\u003e {\n              const filePath = join(ctl.getWorkingDirectory(), file_name);\n              if (!existsSync(filePath)) {\n                return \"Error: File does not exist.\";\n              }\n              const content = await readFile(filePath, \"utf-8\");\n              return content;\n            },\n          });\n          tools.push(readFileTool); // Second tool\n\n          return tools; // Return the tools array\n        }\n```\n50:Tb4a,"])</script><script>self.__next_f.push([1,"\nYou can add custom configuration options to your tools provider, so the user of plugin can customize the behavior without modifying the code.\n\nIn the example below, we will ask the user to specify a folder name, and we will create files inside that folder within the working directory.\n\nFirst, add the config field to `config.ts`:\n\n```lms_code_snippet\n  title: \"src/config.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        export const configSchematics = createConfigSchematics()\n          .field(\n            \"folderName\", // Key of the configuration field\n            \"string\", // Type of the configuration field\n            {\n              displayName: \"Folder Name\",\n              subtitle: \"The name of the folder where files will be created.\",\n            },\n            \"default_folder\", // Default value\n          )\n          .build();\n```\n\n```lms_info\nIn this example, we added the field to `configSchematics`, which is the \"per-chat\" configuration. If you want to add a global configuration field that is shared across different chats, you should add it under the section `globalConfigSchematics` in the same file.\n\nLearn more about configurations in [Custom Configurations](../plugins/configurations).\n```\n\nThen, modify the tools provider to use the configuration value:\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool, Tool, ToolsProviderController } from \"@lmstudio/sdk\";\n        import { existsSync } from \"fs\";\n        import { mkdir, writeFile } from \"fs/promises\";\n        import { join } from \"path\";\n        import { z } from \"zod\";\n        import { configSchematics } from \"./config\";\n\n        export async function toolsProvider(ctl: ToolsProviderController) {\n          const tools: Tool[] = [];\n\n          const createFileTool = tool({\n            name: `create_file`,\n            description: \"Create a file with the given name and content.\",\n            parameters: { file_name: z.string(), content: z.string() },\n            implementation: async ({ file_name, content }) =\u003e {\n              // Read the config field\n              const folderName = ctl.getPluginConfig(configSchematics).get(\"folderName\");\n              const folderPath = join(ctl.getWorkingDirectory(), folderName);\n\n              // Ensure the folder exists\n              await mkdir(folderPath, { recursive: true });\n\n              // Create the file\n              const filePath = join(folderPath, file_name);\n              if (existsSync(filePath)) {\n                return \"Error: File already exists.\";\n              }\n              await writeFile(filePath, content, \"utf-8\");\n              return \"File created.\";\n            },\n          });\n          tools.push(createFileTool); // First tool\n\n          return tools; // Return the tools array\n        }\n```\n"])</script><script>self.__next_f.push([1,"51:Td5b,"])</script><script>self.__next_f.push([1,"\nSometimes, a tool may take a long time to execute. In such cases, it will be helpful to provide status updates, so the user knows what is happening. In order times, you may want to warn the user about potential issues.\n\nYou can use `status` and `warn` methods on the second parameter of the tool's implementation function to send status updates and warnings.\n\nThe following example shows how to implement a tool that waits for a specified number of seconds, providing status updates and warnings if the wait time exceeds 10 seconds:\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool, Tool, ToolsProviderController } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        export async function toolsProvider(ctl: ToolsProviderController) {\n          const tools: Tool[] = [];\n\n          const waitTool = tool({\n            name: `wait`,\n            description: \"Wait for a specified number of seconds.\",\n            parameters: { seconds: z.number().min(1) },\n            implementation: async ({ seconds }, { status, warn }) =\u003e {\n              if (seconds \u003e 10) {\n                warn(\"The model asks to wait for more than 10 seconds.\");\n              }\n              for (let i = 0; i \u003c seconds; i++) {\n                status(`Waiting... ${i + 1}/${seconds} seconds`);\n                await new Promise((resolve) =\u003e setTimeout(resolve, 1000));\n              }\n            },\n          });\n          tools.push(waitTool);\n\n          return tools; // Return the tools array\n        }\n```\n\nNote status updates and warnings are only visible to the user. If you want the model to also see those messages, you should return them as part of the tool's return value.\n\n## Handling Aborts\n\nA prediction may be aborted by the user while your tool is still running. In such cases, you should handle the abort gracefully by handling the `AbortSignal` object passed as the second parameter to the tool's implementation function.\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool, Tool, ToolsProviderController } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        export async function toolsProvider(ctl: ToolsProviderController) {\n          const tools: Tool[] = [];\n\n          const fetchTool = tool({\n            name: `fetch`,\n            description: \"Fetch a URL using GET method.\",\n            parameters: { url: z.string() },\n            implementation: async ({ url }, { signal }) =\u003e {\n              const response = await fetch(url, {\n                method: \"GET\",\n                signal, // \u003c-- Here, we pass the signal to fetch to allow cancellation\n              });\n              if (!response.ok) {\n                return `Error: Failed to fetch ${url}: ${response.statusText}`;\n              }\n              const data = await response.text();\n              return {\n                status: response.status,\n                headers: Object.fromEntries(response.headers.entries()),\n                data: data.substring(0, 1000), // Limit to 1000 characters\n              };\n            },\n          });\n          tools.push(fetchTool);\n\n          return tools;\n        }\n```\n\nYou can learn more about `AbortSignal` in the [MDN documentation](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal).\n"])</script><script>self.__next_f.push([1,"52:T662,\nA prediction may be aborted by the user while your tool is still running. In such cases, you should handle the abort gracefully by handling the `AbortSignal` object passed as the second parameter to the tool's implementation function.\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool, Tool, ToolsProviderController } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        export async function toolsProvider(ctl: ToolsProviderController) {\n          const tools: Tool[] = [];\n\n          const fetchTool = tool({\n            name: `fetch`,\n            description: \"Fetch a URL using GET method.\",\n            parameters: { url: z.string() },\n            implementation: async ({ url }, { signal }) =\u003e {\n              const response = await fetch(url, {\n                method: \"GET\",\n                signal, // \u003c-- Here, we pass the signal to fetch to allow cancellation\n              });\n              if (!response.ok) {\n                return `Error: Failed to fetch ${url}: ${response.statusText}`;\n              }\n              const data = await response.text();\n              return {\n                status: response.status,\n                headers: Object.fromEntries(response.headers.entries()),\n                data: data.substring(0, 1000), // Limit to 1000 characters\n              };\n            },\n          });\n          tools.push(fetchTool);\n\n          return tools;\n        }\n```\n\nYou can learn more about `AbortSignal` in the [MDN documentation](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal).\n53:T456,\nPrompt Preprocessor is a function that is called upon the user hitting the \"Send\" button. It receives the user input and can modify it before it reaches the model. If multiple prompt preprocessors are registered, they will be chained together, with each one receiving the output of the previous one.\n\nThe modified result will be saved in the chat history, meaning that even if your plugin is disab"])</script><script>self.__next_f.push([1,"led afterwards, the modified input will still be used.\n\nPrompt preprocessors will only be triggered for the current user input. It will not be triggered for previous messages in the chat history even if they were not preprocessed.\n\nPrompt preprocessors takes in a `ctl` object for controlling the preprocessing and a `userMessage` it needs to preprocess. It returns either a string or a message object which will replace the user message.\n\n### Examples\n\nThe following are some plugins that make use of prompt preprocessors:\n\n- [lmstudio/rag-v1](https://lmstudio.ai/lmstudio/rag-v1)\n\n  Retrieval Augmented Generation (RAG) for LM Studio. This is the plugin that gives document handling capabilities to LM Studio.\n54:T5ce,\n### Example: Inject Current Time\n\nThe following is an example preprocessor that injects the current time before each user message.\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { type PromptPreprocessorController, type ChatMessage } from \"@lmstudio/sdk\";\n        export async function preprocess(ctl: PromptPreprocessorController, userMessage: ChatMessage) {\n          const textContent = userMessage.getText();\n          const transformed = `Current time: ${new Date().toString()}\\n\\n${textContent}`;\n          return transformed;\n        }\n```\n\n### Example: Replace Trigger Words\n\nAnother example you can do it with simple text only processing is by replacing certain trigger words. For example, you can replace a `@init` trigger with a special initialization message.\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { type PromptPreprocessorController, type ChatMessage, text } from \"@lmstudio/sdk\";\n\n        const mySpecialInstructions = text`\n          Here are some special instructions...\n        `;\n\n        export async function preprocess(ctl: PromptPreprocessorController, userMessage: ChatMessage) {\n          const textConte"])</script><script>self.__next_f.push([1,"nt = userMessage.getText();\n          const transformed = textContent.replaceAll(\"@init\", mySpecialInstructions);\n          return transformed;\n        }\n```\n55:T93b,"])</script><script>self.__next_f.push([1,"\nYou can access custom configurations via `ctl.getPluginConfig` and `ctl.getGlobalPluginConfig`. See [Custom Configurations](./configurations) for more details.\n\nThe following is an example of how you can make the `specialInstructions` and `triggerWord` configurable:\n\nFirst, add the config field to `config.ts`:\n\n```lms_code_snippet\n  title: \"src/config.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { createConfigSchematics } from \"@lmstudio/sdk\";\n        export const configSchematics = createConfigSchematics()\n          .field(\n            \"specialInstructions\",\n            \"string\",\n            {\n              displayName: \"Special Instructions\",\n              subtitle: \"Special instructions to be injected when the trigger word is found.\",\n            },\n            \"Here is some default special instructions.\",\n          )\n          .field(\n            \"triggerWord\",\n            \"string\",\n            {\n              displayName: \"Trigger Word\",\n              subtitle: \"The word that will trigger the special instructions.\",\n            },\n            \"@init\",\n          )\n          .build();\n```\n\n```lms_info\nIn this example, we added the field to `configSchematics`, which is the \"per-chat\" configuration. If you want to add a global configuration field that is shared across different chats, you should add it under the section `globalConfigSchematics` in the same file.\n\nLearn more about configurations in [Custom Configurations](../plugins/configurations).\n```\n\nThen, modify the prompt preprocessor to use the configuration:\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { type PromptPreprocessorController, type ChatMessage } from \"@lmstudio/sdk\";\n        import { configSchematics } from \"./config\";\n\n        export async function preprocess(ctl: PromptPreprocessorController, userMessage: ChatMessage) {\n          const textContent = userMessage.getText();\n          const pluginConfig = ctl.getPluginConfig(configSchematics);\n\n          const triggerWord = pluginConfig.get(\"triggerWord\");\n          const specialInstructions = pluginConfig.get(\"specialInstructions\");\n\n          const transformed = textContent.replaceAll(triggerWord, specialInstructions);\n          return transformed;\n        }\n```\n"])</script><script>self.__next_f.push([1,"56:T459,\nDepending on the task, the prompt preprocessor may take some time to complete, for example, it may need to fetch some data from the internet or perform some heavy computation. In such cases, you can report the status of the preprocessing using `ctl.setStatus`.\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        const status = ctl.createStatus({\n          status: \"loading\",\n          text: \"Preprocessing.\",\n        });\n```\n\nYou can update the status at any time by calling `status.setState`.\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        status.setState({\n          status: \"done\",\n          text: \"Preprocessing done.\",\n        })\n```\n\nYou can even add sub status to the status:\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        const subStatus = status.addSubStatus({\n          status: \"loading\",\n          text: \"I am a sub status.\"\n        });\n```\n57:T408,\nGenerators are replacement for local LLMs. They act like a token source. When a plugin with a generator is used, LM Studio will no longer use the local model to generate text. The generator will be used instead.\n\nGenerators are useful for implementing adapters for external models, such as using a remote LM Studio instance or other online models.\n\nIf a plugin contains a generator, it will no longer show up in the plugins list. Instead, it will show up in the model dropdown and act as a model. If your plugins contains [Tools Provider](./tools-providers.md) or [Prompt Preprocessor](./prompt-preprocessors.md), they will be used when your generator is being selected.\n\n## Examples\n\nThe following are some plugins that make use of generators:\n\n- [lmstudio/remote-lmstudio](https://lmstudio.ai/lmstudio/remote-lmstudio)\n\n  Basic support for using a remote LM Studio instance to generate text.\n\n- [lmstudio/openai-comp"])</script><script>self.__next_f.push([1,"at-endpoint](https://lmstudio.ai/lmstudio/openai-compat-endpoint)\n\n  Use any OpenAI-compatible API in LM Studio.\n58:T63a,\nGenerators take in the the generator controller and the current conversation state, start the generation, and then report the generated text using the `ctl.fragmentGenerated` method.\n\nThe following is an example of a simple generator that echos back the last user message with 200 ms delay between each word:\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { Chat, GeneratorController } from \"@lmstudio/sdk\";\n\n        export async function generate(ctl: GeneratorController, chat: Chat) {\n          // Just echo back the last message\n          const lastMessage = chat.at(-1).getText();\n          // Split the last message into words\n          const words = lastMessage.split(/(?= )/);\n          for (const word of words) {\n            ctl.fragmentGenerated(word); // Send each word as a fragment\n            ctl.abortSignal.throwIfAborted(); // Allow for cancellation\n            await new Promise((resolve) =\u003e setTimeout(resolve, 200)); // Simulate some processing time\n          }\n        }\n```\n\n## Custom Configurations\n\nYou can access custom configurations via `ctl.getPluginConfig` and `ctl.getGlobalPluginConfig`. See [Custom Configurations](./configurations) for more details.\n\n## Handling Aborts\n\nA prediction may be aborted by the user while your generator is still running. In such cases, you should handle the abort gracefully by handling the `ctl.abortSignal`.\n\nYou can learn more about `AbortSignal` in the [MDN documentation](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal).\n59:Tba3,"])</script><script>self.__next_f.push([1,"\nTo enable tool use, it is slightly more involved. To see a comprehensive example that adapts OpenAI API, see the [openai-compat-endpoint plugin](https://lmstudio.ai/lmstudio/openai-compat-endpoint).\n\nYou can read the definition of tools available using `ctl.getToolDefinitions()`. For example, if you are making an online model adapter, you need to pass the tool definition to the model.\n\nOnce the model starts to make tool calls, you need to tell LM Studio about those calls.\n\nUse `ctl.toolCallGenerationStarted` to report the start of a tool call generation (i.e. the model starts to generate a tool call).\n\nUse `ctl.toolCallGenerationEnded` to report a successful generation of a tool call or use `ctl.toolCallGenerationFailed` to report a failed generation of a tool call.\n\nOptionally, you can also `ctl.toolCallGenerationNameReceived` to eagerly report the name of the tool being called once that is available. You can also use `ctl.toolCallGenerationArgumentFragmentGenerated` to report fragments of the tool call arguments as they are generated. These two methods are useful for providing better user experience, but are not strictly necessary.\n\nOverall, your generator must call these ctl methods in the following order:\n\n1. 0 - N calls to `ctl.fragmentGenerated` to report the generated non-tool-call text fragments.\n2. For each tool call:\n   1. Call `ctl.toolCallGenerationStarted` to indicate the start of a tool call generation.\n   2. (Optionally) Call `ctl.toolCallGenerationNameReceived` to report the name of the tool being called.\n   3. (Optionally) Call any times of `ctl.toolCallGenerationArgumentFragmentGenerated` to report the generated fragments of the tool call arguments.\n   4. Call either `ctl.toolCallGenerationEnded` to report a successful generation of the tool call or `ctl.toolCallGenerationFailed` to report a failed generation of the tool call.\n   5. If the model generates more text between/after the tool call, 0 - N calls to `ctl.fragmentGenerated` to report the generated non-tool-call text fragments. (This should not happen normally, but it is technically possible for some smaller models to do this. **Critically: this is not the same as model receiving the tool results and continuing the conversation. This is just model refusing to stop talking after made a tool request - the tool result is not available to the model yet.** When multi-round prediction happens, i.e. the model actually receives the tool call, your generator function will be called again with the updated conversation state.)\n\nSome API formats may report the tool name together with the beginning of the tool call generation, in which case you can call `ctl.toolCallGenerationNameReceived` immediately after `ctl.toolCallGenerationStarted`.\n\nSome API formats may not have incremental tool call updates (i.e. the entire tool call request is given at once), in which case you can just call `ctl.toolCallGenerationStarted` then immediately `ctl.toolCallGenerationEnded`.\n"])</script><script>self.__next_f.push([1,"5a:T641,\nLM Studio plugins support custom configurations. That is, you can define a configuration schema and LM Studio will present a UI to the user so they can configure your plugin without having to edit any code.\n\nThere are two types of configurations:\n\n- **Per-chat configuration**: tied to a specific chat. Different chats can have different configurations. Most configurations that affects the behavior of the plugin should be of this type.\n- **Global configuration**: apply to _all_ chats and are shared across the application. This is useful for global settings such as API keys.\n\n## Types of Configurations\n\nYou can define configurations in TypeScript using the `createConfigSchematics` function from the `@lmstudio/sdk` package. This function allows you to define fields with various types and options.\n\nSupported types include:\n\n- `string`: A text input field.\n- `numeric`: A number input field with optional validation and slider UI.\n- `boolean`: A checkbox or toggle input field.\n- `stringArray`: An array of string values with configurable constraints.\n- `select`: A dropdown selection field with predefined options.\n\nSee the [Defining New Fields](./custom-configuration/defining-new-fields) section for more details on how to define these fields.\n\n## Examples\n\nThe following are some plugins that make use of custom configurations\n\n- [lmstudio/wikipedia](https://lmstudio.ai/lmstudio/wikipedia)\n\n  Gives the LLM tools to search and read Wikipedia articles.\n\n- [lmstudio/openai-compat-endpoint](https://lmstudio.ai/lmstudio/openai-compat-endpoint)\n\n  Use any OpenAI-compatible API in LM Studio.\n5b:Ta12,"])</script><script>self.__next_f.push([1,"\nBy default, the plugin scaffold will create a `config.ts` file in the `src/` directory which will contain the schematics of the configurations. If the files does not exist, you can create it manually:\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { createConfigSchematics } from \"@lmstudio/sdk\";\n\n        export const configSchematics = createConfigSchematics()\n          .field(\n            \"myCustomField\", // The key of the field.\n            \"numeric\", // Type of the field.\n            // Options for the field. Different field types will have different options.\n            {\n              displayName: \"My Custom Field\",\n              hint: \"This is my custom field. Doesn't do anything special.\",\n              slider: { min: 0, max: 100, step: 1 }, // Add a slider to the field.\n            },\n            80, // Default Value\n          )\n          // You can add more fields by chaining the field method.\n          // For example:\n          //   .field(\"anotherField\", ...)\n          .build();\n\n        export const globalConfigSchematics = createConfigSchematics()\n          .field(\n            \"myGlobalCustomField\", // The key of the field.\n            \"string\",\n            {\n              displayName: \"My Global Custom Field\",\n              hint: \"This is my global custom field. Doesn't do anything special.\",\n            },\n            \"default value\", // Default Value\n          )\n          // You can add more fields by chaining the field method.\n          // For example:\n          //  .field(\"anotherGlobalField\", ...)\n          .build();\n```\n\nIf you've added your config schematics manual, you will also need to register the configurations in your plugin's `index.ts` file.\n\nThis is done by calling `context.withConfigSchematics(configSchematics)` and `context.withGlobalConfigSchematics(globalConfigSchematics)` in the `main` function of your plugin.\n\n```lms_code_snippet\n  title: \"src/index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        // ... other imports ...\n        import { toolsProvider } from \"./toolsProvider\";\n\n        export async function main(context: PluginContext) {\n          // ... other plugin setup code ...\n\n          // Register the configuration schematics.\n          context.withConfigSchematics(configSchematics);\n          // Register the global configuration schematics.\n          context.withGlobalConfigSchematics(globalConfigSchematics);\n\n          // ... other plugin setup code ...\n        }\n```\n"])</script><script>self.__next_f.push([1,"5c:T447,\nYou can access the configuration using the method `ctl.getPluginConfig(configSchematics)` and `ctl.getGlobalConfig(globalConfigSchematics)` respectively.\n\nFor example, here is how to access the config within the promptPreprocessor:\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { type PreprocessorController, type ChatMessage } from \"@lmstudio/sdk\";\n        import { configSchematics } from \"./config\";\n\n        export async function preprocess(ctl: PreprocessorController, userMessage: ChatMessage) {\n          const pluginConfig = ctl.getPluginConfig(configSchematics);\n          const myCustomField = pluginConfig.get(\"myCustomField\");\n\n          const globalPluginConfig = ctl.getGlobalPluginConfig(configSchematics);\n          const globalMyCustomField = globalPluginConfig.get(\"myCustomField\");\n\n          return (\n            `${userMessage.getText()},` +\n            `myCustomField: ${myCustomField}, ` +\n            `globalMyCustomField: ${globalMyCustomField}`\n          );\n        }\n```\n5d:T1109,"])</script><script>self.__next_f.push([1,"\nWe support the following field types:\n\n- `string`: A text input field.\n\n  ```lms_code_snippet\n    variants:\n      TypeScript:\n        language: typescript\n        code: |\n          // ... other fields ...\n          .field(\n            \"stringField\", // The key of the field.\n            \"string\", // Type of the field.\n            {\n              displayName: \"A string field\",\n              subtitle: \"Subtitle\", // Optional subtitle for the field. (Show below the field)\n              hint: \"Hint\", // Optional hint for the field. (Show on hover)\n              isParagraph: false, // Whether to show a large text input area for this field.\n              isProtected: false, // Whether the value should be obscured in the UI (e.g., for passwords).\n              placeholder: \"Placeholder text\", // Optional placeholder text for the field.\n            },\n            \"default value\", // Default Value\n          )\n          // ... other fields ...\n  ```\n\n- `numeric`: A number input field with optional validation and slider UI.\n\n  ```lms_code_snippet\n    variants:\n      TypeScript:\n        language: typescript\n        code: |\n          // ... other fields ...\n          .field(\n            \"numberField\", // The key of the field.\n            \"numeric\", // Type of the field.\n            {\n              displayName: \"A number field\",\n              subtitle: \"Subtitle for\", // Optional subtitle for the field. (Show below the field)\n              hint: \"Hint for number field\", // Optional hint for the field. (Show on hover)\n              int: false, // Whether the field should accept only integer values.\n              min: 0, // Minimum value for the field.\n              max: 100, // Maximum value for the field.\n              slider: {\n                // If present, configurations for the slider UI\n                min: 0, // Minimum value for the slider.\n                max: 100, // Maximum value for the slider.\n                step: 1, // Step value for the slider.\n              },\n            },\n            42, // Default Value\n          )\n          // ... other fields ...\n  ```\n\n- `boolean`: A checkbox or toggle input field.\n\n  ```lms_code_snippet\n    variants:\n      TypeScript:\n        language: typescript\n        code: |\n          // ... other fields ...\n          .field(\n            \"booleanField\", // The key of the field.\n            \"boolean\", // Type of the field.\n            {\n              displayName: \"A boolean field\",\n              subtitle: \"Subtitle\", // Optional subtitle for the field. (Show below the field)\n              hint: \"Hint\", // Optional hint for the field. (Show on hover)\n            },\n            true, // Default Value\n          )\n          // ... other fields ...\n  ```\n\n- `stringArray`: An array of string values with configurable constraints.\n\n  ```lms_code_snippet\n    variants:\n      TypeScript:\n        language: typescript\n        code: |\n          // ... other fields ...\n          .field(\n            \"stringArrayField\",\n            \"stringArray\",\n            {\n              displayName: \"A string array field\",\n              subtitle: \"Subtitle\", // Optional subtitle for the field. (Show below the field)\n              hint: \"Hint\", // Optional hint for the field. (Show on hover)\n              allowEmptyStrings: true, // Whether to allow empty strings in the array.\n              maxNumItems: 5, // Maximum number of items in the array.\n            },\n            [\"default\", \"values\"], // Default Value\n          )\n          // ... other fields ...\n  ```\n\n- `select`: A dropdown selection field with predefined options.\n\n  ```lms_code_snippet\n    variants:\n      TypeScript:\n        language: typescript\n        code: |\n          // ... other fields ...\n          .field(\n            \"selectField\",\n            \"select\",\n            {\n              displayName: \"A select field\",\n              options: [\n                { value: \"option1\", displayName: \"Option 1\" },\n                { value: \"option2\", displayName: \"Option 2\" },\n                { value: \"option3\", displayName: \"Option 3\" },\n              ],\n              subtitle: \"Subtitle\", // Optional subtitle for the field. (Show below the field)\n              hint: \"Hint\", // Optional hint for the field. (Show on hover)\n            },\n            \"option1\", // Default Value\n          )\n          // ... other fields ...\n  ```\n"])</script><script>self.__next_f.push([1,"5e:T544,\nTo share publish your LM Studio plugin, open the plugin directory in a terminal and run:\n\n```bash\nlms push\n```\n\nThis command will package your plugin and upload it to the LM Studio Hub. You can use this command to create new plugins or update existing ones.\n\n### Changing Plugin Names\n\nIf you wish to change the name of the plugin, you can do so by editing the `manifest.json` file in the root of your plugin directory. Look for the `name` field and update it to your desired plugin name. Note the `name` must be kebab-case.\n\nWhen you `lms push` the plugin, it will be treated as a new plugin if the name has changed. You can delete the old plugin from the LM Studio Hub if you no longer need it.\n\n### Publishing Plugins to an Organization\n\nIf you are in an organization and wish to publish the plugin to the organization, you can do so by editing the `manifest.json` file in the root of your plugin directory. Look for the `owner` field and set it to the name of your organization. When you run `lms push`, the plugin will be published to the organization instead of your personal account.\n\n### Private Plugins\n\nIf your account supports private plugins, you can publish your plugins privately by using the `--private` flag when running `lms push`:\n\n```bash\nlms push --private\n```\n\nPrivate artifact is in test. Get in touch if you are interested.\n5f:Td2e,"])</script><script>self.__next_f.push([1,"\nModels use a tokenizer to internally convert text into \"tokens\" they can deal with more easily. LM Studio exposes this tokenizer for utility.\n\n## Tokenize\n\nYou can tokenize a string with a loaded LLM or embedding model using the SDK. In the below examples, `llm` can be replaced with an embedding model `emb`.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const model = await client.llm.model();\n\n        const tokens = await model.tokenize(\"Hello, world!\");\n\n        console.info(tokens); // Array of token IDs.\n```\n\n## Count tokens\n\nIf you only care about the number of tokens, you can use the `.countTokens` method instead.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        const tokenCount = await model.countTokens(\"Hello, world!\");\n        console.info(\"Token count:\", tokenCount);\n```\n\n### Example: Count Context\n\nYou can determine if a given conversation fits into a model's context by doing the following:\n\n1. Convert the conversation to a string using the prompt template.\n2. Count the number of tokens in the string.\n3. Compare the token count to the model's context length.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { Chat, type LLM, LMStudioClient } from \"@lmstudio/sdk\";\n\n        async function doesChatFitInContext(model: LLM, chat: Chat) {\n          // Convert the conversation to a string using the prompt template.\n          const formatted = await model.applyPromptTemplate(chat);\n          // Count the number of tokens in the string.\n          const tokenCount = await model.countTokens(formatted);\n          // Get the current loaded context length of the model\n          const contextLength = await model.getContextLength();\n          return tokenCount \u003c contextLength;\n        }\n\n        const client = new LMStudioClient();\n        const model = await client.llm.model();\n\n        const chat = Chat.from([\n          { role: \"user\", content: \"What is the meaning of life.\" },\n          { role: \"assistant\", content: \"The meaning of life is...\" },\n          // ... More messages\n        ]);\n\n        console.info(\"Fits in context:\", await doesChatFitInContext(model, chat));\n```\n\n\u003c!-- ### Context length comparisons\n\nThe below examples check whether a conversation is over a LLM's context length\n(replace `llm` with `emb` to check for an embedding model).\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient, Chat } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const llm = await client.llm.model();\n\n        // To check for a string, simply tokenize\n        var tokens = await llm.tokenize(\"Hello, world!\");\n\n        // To check for a Chat, apply the prompt template first\n        const chat = Chat.createEmpty().withAppended(\"user\", \"Hello, world!\");\n        const templatedChat = await llm.applyPromptTemplate(chat);\n        tokens = await llm.tokenize(templatedChat);\n\n        // If the prompt's length in tokens is less than the context length, you're good!\n        const contextLength = await llm.getContextLength()\n        const isOkay = (tokens.length \u003c contextLength)\n``` --\u003e\n"])</script><script>self.__next_f.push([1,"60:T5a6,\nYou can iterate through locally available models using the `listLocalModels` method.\n\n## Available Model on the Local Machine\n\n`listLocalModels` lives under the `system` namespace of the `LMStudioClient` object.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        console.info(await client.system.listDownloadedModels());\n```\n\nThis will give you results equivalent to using [`lms ls`](../../cli/ls) in the CLI.\n\n### Example output:\n\n```json\n[\n  {\n    \"type\": \"llm\",\n    \"modelKey\": \"qwen2.5-7b-instruct\",\n    \"format\": \"gguf\",\n    \"displayName\": \"Qwen2.5 7B Instruct\",\n    \"path\": \"lmstudio-community/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q4_K_M.gguf\",\n    \"sizeBytes\": 4683073952,\n    \"paramsString\": \"7B\",\n    \"architecture\": \"qwen2\",\n    \"vision\": false,\n    \"trainedForToolUse\": true,\n    \"maxContextLength\": 32768\n  },\n  {\n    \"type\": \"embedding\",\n    \"modelKey\": \"text-embedding-nomic-embed-text-v1.5@q4_k_m\",\n    \"format\": \"gguf\",\n    \"displayName\": \"Nomic Embed Text v1.5\",\n    \"path\": \"nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q4_K_M.gguf\",\n    \"sizeBytes\": 84106624,\n    \"architecture\": \"nomic-bert\",\n    \"maxContextLength\": 2048\n  }\n]\n```\n\n\u003c!-- Learn more about the `client.system` namespace in the [System API Reference](../api-reference/system-namespace). --\u003e\n61:T10ad,"])</script><script>self.__next_f.push([1,"\nAI models are huge. It can take a while to load them into memory. LM Studio's SDK allows you to precisely control this process.\n\n**Most commonly:**\n\n- Use `.model()` to get any currently loaded model\n- Use `.model(\"model-key\")` to use a specific model\n\n**Advanced (manual model management):**\n\n- Use `.load(\"model-key\")` to load a new instance of a model\n- Use `model.unload()` to unload a model from memory\n\n## Get the Current Model with `.model()`\n\nIf you already have a model loaded in LM Studio (either via the GUI or `lms load`), you can use it by calling `.model()` without any arguments.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model();\n```\n\n## Get a Specific Model with `.model(\"model-key\")`\n\nIf you want to use a specific model, you can provide the model key as an argument to `.model()`.\n\n#### Get if Loaded, or Load if not\n\nCalling `.model(\"model-key\")` will load the model if it's not already loaded, or return the existing instance if it is.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model(\"qwen/qwen3-4b-2507\");\n```\n\n\u003c!-- Learn more about the `.model()` method and the parameters it accepts in the [API Reference](../api-reference/model). --\u003e\n\n## Load a New Instance of a Model with `.load()`\n\nUse `load()` to load a new instance of a model, even if one already exists. This allows you to have multiple instances of the same or different models loaded at the same time.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const llama = await client.llm.load(\"qwen/qwen3-4b-2507\");\n        const another_llama = await client.llm.load(\"qwen/qwen3-4b-2507\", {\n          identifier: \"second-llama\"\n        });\n```\n\n\u003c!-- Learn more about the `.load()` method and the parameters it accepts in the [API Reference](../api-reference/load). --\u003e\n\n### Note about Instance Identifiers\n\nIf you provide an instance identifier that already exists, the server will throw an error.\nSo if you don't really care, it's safer to not provide an identifier, in which case\nthe server will generate one for you. You can always check in the server tab in LM Studio, too!\n\n## Unload a Model from Memory with `.unload()`\n\nOnce you no longer need a model, you can unload it by simply calling `unload()` on its handle.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model();\n        await model.unload();\n```\n\n## Set Custom Load Config Parameters\n\nYou can also specify the same load-time configuration options when loading a model, such as Context Length and GPU offload.\n\nSee [load-time configuration](../llm-prediction/parameters) for more.\n\n## Set an Auto Unload Timer (TTL)\n\nYou can specify a _time to live_ for a model you load, which is the idle time (in seconds)\nafter the last request until the model unloads. See [Idle TTL](/docs/api/ttl-and-auto-evict) for more on this.\n\n```lms_code_snippet\n  variants:\n    \"Using .load\":\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n\n        const model = await client.llm.load(\"qwen/qwen3-4b-2507\", {\n          ttl: 300, // 300 seconds\n        });\n    \"Using .model\":\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model(\"qwen/qwen3-4b-2507\", {\n          // Note: specifying ttl in `.model` will only set the TTL for the model if the model is\n          // loaded from this call. If the model was already loaded, the TTL will not be updated.\n          ttl: 300, // 300 seconds\n        });\n```\n"])</script><script>self.__next_f.push([1,"62:T1667,"])</script><script>self.__next_f.push([1,"\n### Parameters\n\n```lms_params\n- name: gpu\n  description: |\n    How to distribute the work to your GPUs. See {@link GPUSetting} for more information.\n  public: true\n  type: GPUSetting\n  optional: true\n\n- name: contextLength\n  description: |\n    The size of the context length in number of tokens. This will include both the prompts and the\n    responses. Once the context length is exceeded, the value set in\n    {@link LLMPredictionConfigBase#contextOverflowPolicy} is used to determine the behavior.\n\n    See {@link LLMContextOverflowPolicy} for more information.\n  type: number\n  optional: true\n\n- name: ropeFrequencyBase\n  description: |\n    Custom base frequency for rotary positional embeddings (RoPE).\n\n    This advanced parameter adjusts how positional information is embedded in the model's\n    representations. Increasing this value may enable better performance at high context lengths by\n    modifying how the model processes position-dependent information.\n  type: number\n  optional: true\n\n- name: ropeFrequencyScale\n  description: |\n    Scaling factor for RoPE (Rotary Positional Encoding) frequency.\n\n    This factor scales the effective context window by modifying how positional information is\n    encoded. Higher values allow the model to handle longer contexts by making positional encoding\n    more granular, which can be particularly useful for extending a model beyond its original\n    training context length.\n  type: number\n  optional: true\n\n- name: evalBatchSize\n  description: |\n    Number of input tokens to process together in a single batch during evaluation.\n\n    Increasing this value typically improves processing speed and throughput by leveraging\n    parallelization, but requires more memory. Finding the optimal batch size often involves\n    balancing between performance gains and available hardware resources.\n  type: number\n  optional: true\n\n- name: flashAttention\n  description: |\n    Enables Flash Attention for optimized attention computation.\n\n    Flash Attention is an efficient implementation that reduces memory usage and speeds up\n    generation by optimizing how attention mechanisms are computed. This can significantly\n    improve performance on compatible hardware, especially for longer sequences.\n  type: boolean\n  optional: true\n\n- name: keepModelInMemory\n  description: |\n    When enabled, prevents the model from being swapped out of system memory.\n\n    This option reserves system memory for the model even when portions are offloaded to GPU,\n    ensuring faster access times when the model needs to be used. Improves performance\n    particularly for interactive applications, but increases overall RAM requirements.\n  type: boolean\n  optional: true\n\n- name: seed\n  description: |\n    Random seed value for model initialization to ensure reproducible outputs.\n\n    Setting a specific seed ensures that random operations within the model (like sampling)\n    produce the same results across different runs, which is important for reproducibility\n    in testing and development scenarios.\n  type: number\n  optional: true\n\n- name: useFp16ForKVCache\n  description: |\n    When enabled, stores the key-value cache in half-precision (FP16) format.\n\n    This option significantly reduces memory usage during inference by using 16-bit floating\n    point numbers instead of 32-bit for the attention cache. While this may slightly reduce\n    numerical precision, the impact on output quality is generally minimal for most applications.\n  type: boolean\n  optional: true\n\n- name: tryMmap\n  description: |\n    Attempts to use memory-mapped (mmap) file access when loading the model.\n\n    Memory mapping can improve initial load times by mapping model files directly from disk to\n    memory, allowing the operating system to handle paging. This is particularly beneficial for\n    quick startup, but may reduce performance if the model is larger than available system RAM,\n    causing frequent disk access.\n  type: boolean\n  optional: true\n\n- name: numExperts\n  description: |\n    Specifies the number of experts to use for models with Mixture of Experts (MoE) architecture.\n\n    MoE models contain multiple \"expert\" networks that specialize in different aspects of the task.\n    This parameter controls how many of these experts are active during inference, affecting both\n    performance and quality of outputs. Only applicable for models designed with the MoE architecture.\n  type: number\n  optional: true\n\n- name: llamaKCacheQuantizationType\n  description: |\n    Quantization type for the Llama model's key cache.\n\n    This option determines the precision level used to store the key component of the attention\n    mechanism's cache. Lower precision values (e.g., 4-bit or 8-bit quantization) significantly\n    reduce memory usage during inference but may slightly impact output quality. The effect varies\n    between different models, with some being more robust to quantization than others.\n\n    Set to false to disable quantization and use full precision.\n  type: LLMLlamaCacheQuantizationType | false\n  optional: true\n\n- name: llamaVCacheQuantizationType\n  description: |\n    Quantization type for the Llama model's value cache.\n\n    Similar to the key cache quantization, this option controls the precision used for the value\n    component of the attention mechanism's cache. Reducing precision saves memory but may affect\n    generation quality. This option requires Flash Attention to be enabled to function properly.\n\n    Different models respond differently to value cache quantization, so experimentation may be\n    needed to find the optimal setting for a specific use case. Set to false to disable quantization.\n  type: LLMLlamaCacheQuantizationType | false\n  optional: true\n```\n"])</script><script>self.__next_f.push([1,"63:T184c,"])</script><script>self.__next_f.push([1,"\n### Fields\n\n```lms_params\n- name: \"maxTokens\"\n  type: \"number | false\"\n  optional: true\n  description: \"Number of tokens to predict at most. If set to false, the model will predict as many tokens as it wants.\\n\\nWhen the prediction is stopped because of this limit, the `stopReason` in the prediction stats will be set to `maxPredictedTokensReached`.\"\n\n- name: \"temperature\"\n  type: \"number\"\n  optional: true\n  description: \"The temperature parameter for the prediction model. A higher value makes the predictions more random, while a lower value makes the predictions more deterministic. The value should be between 0 and 1.\"\n\n- name: \"stopStrings\"\n  type: \"Array\u003cstring\u003e\"\n  optional: true\n  description: \"An array of strings. If the model generates one of these strings, the prediction will stop.\\n\\nWhen the prediction is stopped because of this limit, the `stopReason` in the prediction stats will be set to `stopStringFound`.\"\n\n- name: \"toolCallStopStrings\"\n  type: \"Array\u003cstring\u003e\"\n  optional: true\n  description: \"An array of strings. If the model generates one of these strings, the prediction will stop with the `stopReason` `toolCalls`.\"\n\n- name: \"contextOverflowPolicy\"\n  type: \"LLMContextOverflowPolicy\"\n  optional: true\n  description: \"The behavior for when the generated tokens length exceeds the context window size. The allowed values are:\\n\\n- `stopAtLimit`: Stop the prediction when the generated tokens length exceeds the context window size. If the generation is stopped because of this limit, the `stopReason` in the prediction stats will be set to `contextLengthReached`\\n- `truncateMiddle`: Keep the system prompt and the first user message, truncate middle.\\n- `rollingWindow`: Maintain a rolling window and truncate past messages.\"\n\n- name: \"structured\"\n  type: \"ZodType\u003cTStructuredOutputType\u003e | LLMStructuredPredictionSetting\"\n  optional: true\n  description: \"Configures the model to output structured JSON data that follows a specific schema defined using Zod.\\n\\nWhen you provide a Zod schema, the model will be instructed to generate JSON that conforms to that schema rather than free-form text.\\n\\nThis is particularly useful for extracting specific data points from model responses or when you need the output in a format that can be directly used by your application.\"\n\n- name: \"topKSampling\"\n  type: \"number\"\n  optional: true\n  description: \"Controls token sampling diversity by limiting consideration to the K most likely next tokens.\\n\\nFor example, if set to 40, only the 40 tokens with the highest probabilities will be considered for the next token selection. A lower value (e.g., 20) will make the output more focused and conservative, while a higher value (e.g., 100) allows for more creative and diverse outputs.\\n\\nTypical values range from 20 to 100.\"\n\n- name: \"repeatPenalty\"\n  type: \"number | false\"\n  optional: true\n  description: \"Applies a penalty to repeated tokens to prevent the model from getting stuck in repetitive patterns.\\n\\nA value of 1.0 means no penalty. Values greater than 1.0 increase the penalty. For example, 1.2 would reduce the probability of previously used tokens by 20%. This is particularly useful for preventing the model from repeating phrases or getting stuck in loops.\\n\\nSet to false to disable the penalty completely.\"\n\n- name: \"minPSampling\"\n  type: \"number | false\"\n  optional: true\n  description: \"Sets a minimum probability threshold that a token must meet to be considered for generation.\\n\\nFor example, if set to 0.05, any token with less than 5% probability will be excluded from consideration. This helps filter out unlikely or irrelevant tokens, potentially improving output quality.\\n\\nValue should be between 0 and 1. Set to false to disable this filter.\"\n\n- name: \"topPSampling\"\n  type: \"number | false\"\n  optional: true\n  description: \"Implements nucleus sampling by only considering tokens whose cumulative probabilities reach a specified threshold.\\n\\nFor example, if set to 0.9, the model will consider only the most likely tokens that together add up to 90% of the probability mass. This helps balance between diversity and quality by dynamically adjusting the number of tokens considered based on their probability distribution.\\n\\nValue should be between 0 and 1. Set to false to disable nucleus sampling.\"\n\n- name: \"xtcProbability\"\n  type: \"number | false\"\n  optional: true\n  description: \"Controls how often the XTC (Exclude Top Choices) sampling technique is applied during generation.\\n\\nXTC sampling can boost creativity and reduce clichÃ©s by occasionally filtering out common tokens. For example, if set to 0.3, there's a 30% chance that XTC sampling will be applied when generating each token.\\n\\nValue should be between 0 and 1. Set to false to disable XTC completely.\"\n\n- name: \"xtcThreshold\"\n  type: \"number | false\"\n  optional: true\n  description: \"Defines the lower probability threshold for the XTC (Exclude Top Choices) sampling technique.\\n\\nWhen XTC sampling is activated (based on xtcProbability), the algorithm identifies tokens with probabilities between this threshold and 0.5, then removes all such tokens except the least probable one. This helps introduce more diverse and unexpected tokens into the generation.\\n\\nOnly takes effect when xtcProbability is enabled.\"\n\n- name: \"cpuThreads\"\n  type: \"number\"\n  optional: true\n  description: \"Specifies the number of CPU threads to allocate for model inference.\\n\\nHigher values can improve performance on multi-core systems but may compete with other processes. For example, on an 8-core system, a value of 4-6 might provide good performance while leaving resources for other tasks.\\n\\nIf not specified, the system will use a default value based on available hardware.\"\n\n- name: \"draftModel\"\n  type: \"string\"\n  optional: true\n  description: \"The draft model to use for speculative decoding. Speculative decoding is a technique that can drastically increase the generation speed (up to 3x for larger models) by paring a main model with a smaller draft model.\\n\\nSee here for more information: https://lmstudio.ai/docs/advanced/speculative-decoding\\n\\nYou do not need to load the draft model yourself. Simply specifying its model key here is enough.\"\n```\n"])</script><script>self.__next_f.push([1,"64:T917,"])</script><script>self.__next_f.push([1,"\nLLMs and embedding models, due to their fundamental architecture, have a property called `context length`, and more specifically a **maximum** context length. Loosely speaking, this is how many tokens the models can \"keep in memory\" when generating text or embeddings. Exceeding this limit will result in the model behaving erratically.\n\n## Use the `getContextLength()` Function on the Model Object\n\nIt's useful to be able to check the context length of a model, especially as an extra check before providing potentially long input to the model.\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        const contextLength = await model.getContextLength();\n```\n\nThe `model` in the above code snippet is an instance of a loaded model you get from the `llm.model` method. See [Manage Models in Memory](../manage-models/loading) for more information.\n\n### Example: Check if the input will fit in the model's context window\n\nYou can determine if a given conversation fits into a model's context by doing the following:\n\n1. Convert the conversation to a string using the prompt template.\n2. Count the number of tokens in the string.\n3. Compare the token count to the model's context length.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { Chat, type LLM, LMStudioClient } from \"@lmstudio/sdk\";\n\n        async function doesChatFitInContext(model: LLM, chat: Chat) {\n          // Convert the conversation to a string using the prompt template.\n          const formatted = await model.applyPromptTemplate(chat);\n          // Count the number of tokens in the string.\n          const tokenCount = await model.countTokens(formatted);\n          // Get the current loaded context length of the model\n          const contextLength = await model.getContextLength();\n          return tokenCount \u003c contextLength;\n        }\n\n        const client = new LMStudioClient();\n        const model = await client.llm.model();\n\n        const chat = Chat.from([\n          { role: \"user\", content: \"What is the meaning of life.\" },\n          { role: \"assistant\", content: \"The meaning of life is...\" },\n          // ... More messages\n        ]);\n\n        console.info(\"Fits in context:\", await doesChatFitInContext(model, chat));\n```\n"])</script><script>self.__next_f.push([1,"65:Tc78,"])</script><script>self.__next_f.push([1,"\n## Install `lms`\n\n`lms` ships with LM Studio, so you don't need to do any additional installation steps if you have LM Studio installed.\n\nJust open a terminal window and run `lms`:\n\n```shell\nlms --help\n```\n\n## Open source\n\n`lms` is **MIT Licensed** and is developed in this repository on GitHub: https://github.com/lmstudio-ai/lms\n\n## Command quick links\n\n| Command | Syntax | Docs |\n| --- | --- | --- |\n| Chat in the terminal | `lms chat` | [Guide](/docs/cli/local-models/chat) |\n| Download models | `lms get` | [Guide](/docs/cli/local-models/get) |\n| List your models | `lms ls` | [Guide](/docs/cli/local-models/ls) |\n| See models loaded into memory | `lms ps` | [Guide](/docs/cli/local-models/ps) |\n| Control the server | `lms server start` | [Guide](/docs/cli/serve/server-start) |\n| Manage the inference runtime | `lms runtime` | [Guide](/docs/cli/runtime) |\n\n\n### Verify the installation\n\n```lms_info\nðŸ‘‰ You need to run LM Studio _at least once_ before you can use `lms`.\n```\n\nOpen a terminal window and run `lms`.\n\n```lms_terminal\n$ lms\n\nlms is LM Studio's CLI utility for your models, server, and inference runtime. (v0.0.47)\n\nUsage: lms [options] [command]\n\nLocal models\n   chat               Start an interactive chat with a model\n   get                Search and download models\n   load               Load a model\n   unload             Unload a model\n   ls                 List the models available on disk\n   ps                 List the models currently loaded in memory\n   import             Import a model file into LM Studio\n\nServe\n   server             Commands for managing the local server\n   log                Log incoming and outgoing messages\n\nRuntime\n   runtime            Manage and update the inference runtime\n\nDevelop \u0026 Publish (Beta)\n   clone              Clone an artifact from LM Studio Hub to a local folder\n   push               Uploads the artifact in the current folder to LM Studio Hub\n   dev                Starts a plugin dev server in the current folder\n   login              Authenticate with LM Studio\n\nLearn more:           https://lmstudio.ai/docs/developer\nJoin our Discord:     https://discord.gg/lmstudio\n```\n\n## Use `lms` to automate and debug your workflows\n\n### Start and stop the local server\n\n```bash\nlms server start\nlms server stop\n```\n\nLearn more about [`lms server`](/docs/cli/serve/server-start).\n\n### List the local models on the machine\n\n```bash\nlms ls\n```\n\nLearn more about [`lms ls`](/docs/cli/local-models/ls).\n\nThis will reflect the current LM Studio models directory, which you set in **ðŸ“‚ My Models** tab in the app.\n\n### List the currently loaded models\n\n```bash\nlms ps\n```\n\nLearn more about [`lms ps`](/docs/cli/local-models/ps).\n\n### Load a model (with options)\n\n```bash\nlms load [--gpu=max|auto|0.0-1.0] [--context-length=1-N]\n```\n\n`--gpu=1.0` means 'attempt to offload 100% of the computation to the GPU'.\n\n- Optionally, assign an identifier to your local LLM:\n\n```bash\nlms load openai/gpt-oss-20b --identifier=\"my-model-name\"\n```\n\nThis is useful if you want to keep the model identifier consistent.\n\n### Unload a model\n```\nlms unload [--all]\n```\n\nLearn more about [`lms load and unload`](/docs/cli/local-models/load).\n"])</script><script>self.__next_f.push([1,"66:T614,\nUse `lms chat` to talk to a local model directly in the terminal. This is handy for quick experiments or scripting.\n\n### Flags \n```lms_params\n- name: \"[model]\"\n  type: \"string\"\n  optional: true\n  description: \"Identifier of the model to use. If omitted, you will be prompted to pick one.\"\n- name: \"-p, --prompt\"\n  type: \"string\"\n  optional: true\n  description: \"Send a one-off prompt and print the response to stdout before exiting\"\n- name: \"-s, --system-prompt\"\n  type: \"string\"\n  optional: true\n  description: \"Custom system prompt for the chat\"\n- name: \"--stats\"\n  type: \"flag\"\n  optional: true\n  description: \"Show detailed prediction statistics after each response\"\n- name: \"--ttl\"\n  type: \"number\"\n  optional: true\n  description: \"Seconds to keep the model loaded after the chat ends (default: 3600)\"\n```\n\n### Start an interactive chat\n\n```shell\nlms chat\n```\n\nYou will be prompted to pick a model if one is not provided.\n\n### Chat with a specific model\n\n```shell\nlms chat my-model\n```\n\n### Send a single prompt and exit\n\nUse `-p` to print the response to stdout and exit instead of staying interactive:\n\n```shell\nlms chat my-model -p \"Summarize this release note\"\n```\n\n### Set a system prompt\n\n```shell\nlms chat my-model -s \"You are a terse assistant. Reply in two sentences.\"\n```\n\n### Keep the model loaded after chatting\n\n```shell\nlms chat my-model --ttl 600\n```\n\n### Pipe input from another command\n\n`lms chat` reads from stdin, so you can pipe content directly into a prompt:\n\n```shell\ncat my_file.txt | lms chat -p \"Summarize this, please\"\n```\n67:T75e,\nThe `lms get` command allows you to search and download models from online repositories. If no model is specified, it shows staff-picked recommendations.\n\nModels you download via `lms get` will be stored in your LM Studio model directory. \n\n### Flags \n```lms_params\n- name: \"[modelName]\"\n  type: \"string\"\n  optional: true\n  description: \"The model to download. If omitted, staff picks are shown. For models with multiple quantizations, append '@' (e.g., 'llama-3.1-8b@q4_k_m'"])</script><script>self.__next_f.push([1,").\"\n- name: \"--mlx\"\n  type: \"flag\"\n  optional: true\n  description: \"Include only MLX models in search results. If either '--mlx' or '--gguf' is set, only matching formats are shown; otherwise results match installed runtimes.\"\n- name: \"--gguf\"\n  type: \"flag\"\n  optional: true\n  description: \"Include only GGUF models in search results. If either '--mlx' or '--gguf' is set, only matching formats are shown; otherwise results match installed runtimes.\"\n- name: \"-n, --limit\"\n  type: \"number\"\n  optional: true\n  description: \"Limit the number of model options shown.\"\n- name: \"--always-show-all-results\"\n  type: \"flag\"\n  optional: true\n  description: \"Always prompt you to choose from search results, even when there's an exact match.\"\n- name: \"-a, --always-show-download-options\"\n  type: \"flag\"\n  optional: true\n  description: \"Always prompt you to choose a quantization, even when an exact match is auto-selected.\"\n```\n\n## Download a model\n\nDownload a model by name:\n\n```shell\nlms get llama-3.1-8b\n```\n\n### Specify quantization\n\nDownload a specific model quantization:\n\n```shell\nlms get llama-3.1-8b@q4_k_m\n```\n\n### Filter by format\n\nShow only MLX or GGUF models:\n\n```shell\nlms get --mlx\nlms get --gguf\n```\n\n### Control search results\n\nLimit the number of results:\n\n```shell\nlms get --limit 5\n```\n\nAlways show all options:\n\n```shell\nlms get --always-show-all-results\nlms get --always-show-download-options\n```\n68:T101c,"])</script><script>self.__next_f.push([1,"\nThe `lms load` command loads a model into memory. You can optionally set parameters such as context length, GPU offload, and TTL. This guide also covers unloading models with `lms unload`.\n\n### Flags \n```lms_params\n- name: \"[path]\"\n  type: \"string\"\n  optional: true\n  description: \"The path of the model to load. If not provided, you will be prompted to select one\"\n- name: \"--ttl\"\n  type: \"number\"\n  optional: true\n  description: \"If provided, when the model is not used for this number of seconds, it will be unloaded\"\n- name: \"--gpu\"\n  type: \"string\"\n  optional: true\n  description: \"How much to offload to the GPU. Values: 0-1, off, max\"\n- name: \"--context-length\"\n  type: \"number\"\n  optional: true\n  description: \"The number of tokens to consider as context when generating text\"\n- name: \"--identifier\"\n  type: \"string\"\n  optional: true\n  description: \"The identifier to assign to the loaded model for API reference\"\n- name: \"--estimate-only\"\n  type: \"boolean\"\n  optional: true\n  description: \"Print a resource (memory) estimate and exit without loading the model\"\n```\n\n## Load a model\n\nLoad a model into memory by running the following command:\n\n```shell\nlms load \u003cmodel_key\u003e\n```\n\nYou can find the `model_key` by first running [`lms ls`](/docs/cli/local-models/ls) to list your locally downloaded models.\n\n### Set a custom identifier\n\nOptionally, you can assign a custom identifier to the loaded model for API reference:\n\n```shell\nlms load \u003cmodel_key\u003e --identifier \"my-custom-identifier\"\n```\n\nYou will then be able to refer to this model by the identifier `my_model` in subsequent commands and API calls (`model` parameter).\n\n### Set context length\n\nYou can set the context length when loading a model using the `--context-length` flag:\n\n```shell\nlms load \u003cmodel_key\u003e --context-length 4096\n```\n\nThis determines how many tokens the model will consider as context when generating text.\n\n### Set GPU offload\n\nControl GPU memory usage with the `--gpu` flag:\n\n```shell\nlms load \u003cmodel_key\u003e --gpu 0.5    # Offload 50% of layers to GPU\nlms load \u003cmodel_key\u003e --gpu max    # Offload all layers to GPU\nlms load \u003cmodel_key\u003e --gpu off    # Disable GPU offloading\n```\n\nIf not specified, LM Studio will automatically determine optimal GPU usage.\n\n### Set TTL\n\nSet an auto-unload timer with the `--ttl` flag (in seconds):\n\n```shell\nlms load \u003cmodel_key\u003e --ttl 3600   # Unload after 1 hour of inactivity\n```\n\n### Estimate resources without loading\n\nPreview memory requirements before loading a model using `--estimate-only`:\n\n```shell\nlms load --estimate-only \u003cmodel_key\u003e\n```\n\nOptional flags such as `--context-length` and `--gpu` are honored and reflected in the estimate. The estimator accounts for factors like context length, flash attention, and whether the model is visionâ€‘enabled.\n\nExample:\n\n```bash\n$ lms load --estimate-only gpt-oss-120b\nModel: openai/gpt-oss-120b\nEstimated GPU Memory:   65.68 GB\nEstimated Total Memory: 65.68 GB\n\nEstimate: This model may be loaded based on your resource guardrails settings.\n```\n\n## Unload models\n\nUse `lms unload` to remove models from memory.\n\n### Flags \n```lms_params\n- name: \"[model_key]\"\n  type: \"string\"\n  optional: true\n  description: \"The key of the model to unload. If not provided, you will be prompted to select one\"\n- name: \"--all\"\n  type: \"flag\"\n  optional: true\n  description: \"Unload all currently loaded models\"\n- name: \"--host\"\n  type: \"string\"\n  optional: true\n  description: \"The host address of a remote LM Studio instance to connect to\"\n```\n\n### Unload a specific model\n\n```shell\nlms unload \u003cmodel_key\u003e\n```\n\nIf no model key is provided, you will be prompted to select from currently loaded models.\n\n### Unload all models\n\n```shell\nlms unload --all\n```\n\n### Unload from a remote LM Studio instance\n\n```shell\nlms unload \u003cmodel_key\u003e --host \u003chost\u003e\n```\n\n## Operate on a remote LM Studio instance\n\n`lms load` supports the `--host` flag to connect to a remote LM Studio instance. \n\n```shell\nlms load \u003cmodel_key\u003e --host \u003chost\u003e\n```\n\nFor this to work, the remote LM Studio instance must be running and accessible from your local machine, e.g. be accessible on the same subnet.\n"])</script><script>self.__next_f.push([1,"69:T836,"])</script><script>self.__next_f.push([1,"\nThe `lms ls` command displays a list of all models downloaded to your machine, including their size, architecture, and parameters.\n\n### Flags \n\n```lms_params\n- name: \"--llm\"\n  type: \"flag\"\n  optional: true\n  description: \"Show only LLMs. When not set, all models are shown\"\n- name: \"--embedding\"\n  type: \"flag\"\n  optional: true\n  description: \"Show only embedding models\"\n- name: \"--json\"\n  type: \"flag\"\n  optional: true\n  description: \"Output the list in JSON format\"\n- name: \"--detailed\"\n  type: \"flag\"\n  optional: true\n  description: \"Show detailed information about each model\"\n```\n\n## List all models\n\nShow all downloaded models:\n\n```shell\nlms ls\n```\n\nExample output:\n\n```\nYou have 47 models, taking up 160.78 GB of disk space.\n\nLLMs (Large Language Models)                       PARAMS      ARCHITECTURE           SIZE\nlmstudio-community/meta-llama-3.1-8b-instruct          8B         Llama            4.92 GB\nhugging-quants/llama-3.2-1b-instruct                   1B         Llama            1.32 GB\nmistral-7b-instruct-v0.3                                         Mistral           4.08 GB\nzeta                                                   7B         Qwen2            4.09 GB\n\n... (abbreviated in this example) ...\n\nEmbedding Models                                   PARAMS      ARCHITECTURE           SIZE\ntext-embedding-nomic-embed-text-v1.5@q4_k_m                     Nomic BERT        84.11 MB\ntext-embedding-bge-small-en-v1.5                     33M           BERT           24.81 MB\n```\n\n### Filter by model type\n\nList only LLM models:\n\n```shell\nlms ls --llm\n```\n\nList only embedding models:\n\n```shell\nlms ls --embedding\n```\n\n### Additional output formats\n\nGet detailed information about models:\n\n```shell\nlms ls --detailed\n```\n\nOutput in JSON format:\n\n```shell\nlms ls --json\n```\n\n## Operate on a remote LM Studio instance\n\n`lms ls` supports the `--host` flag to connect to a remote LM Studio instance:\n\n```shell\nlms ls --host \u003chost\u003e\n```\n\nFor this to work, the remote LM Studio instance must be running and accessible from your local machine, e.g. be accessible on the same subnet.\n"])</script><script>self.__next_f.push([1,"6a:T644,\nUse `lms import` to bring an existing model file into LM Studio without downloading it.\n\n### Flags \n```lms_params\n- name: \"\u003cfile-path\u003e\"\n  type: \"string\"\n  optional: false\n  description: \"Path to the model file to import\"\n- name: \"--user-repo\"\n  type: \"string\"\n  optional: true\n  description: \"Set the target folder as \u003cuser\u003e/\u003crepo\u003e. Skips the categorization prompts.\"\n- name: \"-y, --yes\"\n  type: \"flag\"\n  optional: true\n  description: \"Skip confirmations and try to infer the model location from the file name\"\n- name: \"-c, --copy\"\n  type: \"flag\"\n  optional: true\n  description: \"Copy the file instead of moving it\"\n- name: \"-L, --hard-link\"\n  type: \"flag\"\n  optional: true\n  description: \"Create a hard link instead of moving or copying the file\"\n- name: \"-l, --symbolic-link\"\n  type: \"flag\"\n  optional: true\n  description: \"Create a symbolic link instead of moving or copying the file\"\n- name: \"--dry-run\"\n  type: \"flag\"\n  optional: true\n  description: \"Do not perform the import, just show what would be done\"\n```\n\nOnly one of `--copy`, `--hard-link`, or `--symbolic-link` can be used at a time. If none is provided, `lms import` moves the file by default.\n\n### Import a model file\n\n```shell\nlms import ~/Downloads/model.gguf\n```\n\n### Keep the original file\n\n```shell\nlms import ~/Downloads/model.gguf --copy\n```\n\n### Pick the target folder yourself\n\nUse `--user-repo` to skip prompts and place the model in the chosen namespace:\n\n```shell\nlms import ~/Downloads/model.gguf --user-repo my-user/custom-models\n```\n\n### Dry run before importing\n\n```shell\nlms import ~/Downloads/model.gguf --dry-run\n```\n6b:T41d,\nThe `lms server start` command launches the LM Studio local server, allowing you to interact with loaded models via HTTP API calls.\n\n### Flags \n```lms_params\n- name: \"--port\"\n  type: \"number\"\n  optional: true\n  description: \"Port to run the server on. If not provided, uses the last used port\"\n- name: \"--cors\"\n  type: \"flag\"\n  optional: true\n  description: \"Enable CORS support for web application development. When not set, C"])</script><script>self.__next_f.push([1,"ORS is disabled\"\n```\n\n## Start the server\n\nStart the server with default settings:\n\n```shell\nlms server start\n```\n\n### Specify a custom port\n\nRun the server on a specific port:\n\n```shell\nlms server start --port 3000\n```\n\n### Enable CORS support\n\nFor usage with web applications or some VS Code extensions, you may need to enable CORS support:\n\n```shell\nlms server start --cors\n```\n\nNote that enabling CORS may expose your server to security risks, so use it only when necessary.\n\n### Check the server status\n\nSee [`lms server status`](/docs/cli/serve/server-status) for more information on checking the status of the server.\n6c:T56b,\nThe `lms server status` command displays the current status of the LM Studio local server, including whether it's running and its configuration.\n\n### Flags \n```lms_params\n- name: \"--json\"\n  type: \"flag\"\n  optional: true\n  description: \"Output the status in JSON format\"\n- name: \"--verbose\"\n  type: \"flag\"\n  optional: true\n  description: \"Enable detailed logging output\"\n- name: \"--quiet\"\n  type: \"flag\"\n  optional: true\n  description: \"Suppress all logging output\"\n- name: \"--log-level\"\n  type: \"string\"\n  optional: true\n  description: \"The level of logging to use. Defaults to 'info'\"\n```\n\n## Check server status\n\nGet the basic status of the server:\n\n```shell\nlms server status\n```\n\nExample output:\n```\nThe server is running on port 1234.\n```\n\n### Example usage\n\n```console\nâžœ  ~ lms server start\nStarting server...\nWaking up LM Studio service...\nSuccess! Server is now running on port 1234\n\nâžœ  ~ lms server status\nThe server is running on port 1234.\n```\n\n### JSON output\n\nGet the status in machine-readable JSON format:\n\n```shell\nlms server status --json --quiet\n```\n\nExample output:\n```json\n{\"running\":true,\"port\":1234}\n```\n\n### Control logging output\n\nAdjust logging verbosity:\n\n```shell\nlms server status --verbose\nlms server status --quiet\nlms server status --log-level debug\n```\n\nYou can only use one logging control flag at a time (`--verbose`, `--quiet`, or `--log-level`).\n6d:T547,\n`lms log stream` le"])</script><script>self.__next_f.push([1,"ts you inspect the exact strings LM Studio sends to and receives from models, and (new in 0.3.26) stream server logs. This is useful for debugging prompt templates, model IO, and server operations.\n\n### Flags\n\n```lms_params\n- name: \"-s, --source\"\n  type: \"string\"\n  optional: true\n  description: \"Source of logs: model or server (default: model)\"\n- name: \"--stats\"\n  type: \"flag\"\n  optional: true\n  description: \"Print prediction stats when available\"\n- name: \"--filter\"\n  type: \"string\"\n  optional: true\n  description: \"Filter for model source: input, output, or both\"\n- name: \"--json\"\n  type: \"flag\"\n  optional: true\n  description: \"Output logs as JSON (newline separated)\"\n```\n\n### Quick start\n\nStream model IO (default):\n\n```shell\nlms log stream\n```\n\nStream server logs:\n\n```shell\nlms log stream --source server\n```\n\n### Filter model logs\n\n```bash\n# Only the formatted user input\nlms log stream --source model --filter input\n\n# Only the model output (emitted once the message completes)\nlms log stream --source model --filter output\n\n# Both directions\nlms log stream --source model --filter input,output\n```\n\n### JSON output and stats\n\nEmit JSON:\n\n  ```shell\n  lms log stream --source model --filter input,output --json\n  ```\n\nInclude prediction stats:\n\n  ```shell\n  lms log stream --source model --filter output --stats\n  ```\n6e:T4fa,\nRun `lms push` from inside a [plugin](/docs/typescript/plugins), [preset](/docs/app/presets), or [`model.yaml`](/docs/app/modelyaml) project to publish a new revision. If a `model.yaml` exists, the CLI will generate a `manifest.json` for you before pushing. \n\nFor plugins, the CLI will ask for confirmation unless you pass `-y`.\n\n### Publish the current folder\n\n```shell\nlms push\n```\n\n### Flags\n```lms_params\n- name: \"--description\"\n  type: \"string\"\n  optional: true\n  description: \"Override the artifact description for this push\"\n- name: \"--overrides\"\n  type: \"string\"\n  optional: true\n  description: \"JSON string to override manifest fields (parsed with JSON.parse)\"\n- name: \"-y, --yes\"\n  type: \"flag\"\n  o"])</script><script>self.__next_f.push([1,"ptional: true\n  description: \"Suppress confirmations and warnings\"\n- name: \"--private\"\n  type: \"flag\"\n  optional: true\n  description: \"Mark the artifact as private when first published\"\n- name: \"--write-revision\"\n  type: \"flag\"\n  optional: true\n  description: \"Write the returned revision number to manifest.json\"\n```\n\n\n### Advanced \n\n#### Publish quietly and keep the revision in manifest.json\n\n```shell\nlms push -y --write-revision\n```\n\n#### Override metadata for this upload\n\n```shell\nlms push --description \"New beta build\" --overrides '{\"tags\": [\"beta\"]}'\n```\n6f:T410,\nUse `lms login` to authenticate the CLI with LM Studio Hub.\n\n### Sign in with the browser\n\n```shell\nlms login\n```\n\nThe CLI opens a browser window for authentication. If a browser cannot be opened automatically, copy the printed URL into your browser.\n\n### \"CI style\" login with pre-authenticated keys\n\n```bash\nlms login --with-pre-authenticated-keys \\\n  --key-id \u003cKEY_ID\u003e \\\n  --public-key \u003cPUBLIC_KEY\u003e \\\n  --private-key \u003cPRIVATE_KEY\u003e \n```\n\n### Advanced Flags\n```lms_params\n- name: \"--with-pre-authenticated-keys\"\n  type: \"flag\"\n  optional: true\n  description: \"Authenticate using pre-generated keys (CI/CD). Requires --key-id, --public-key, and --private-key.\"\n- name: \"--key-id\"\n  type: \"string\"\n  optional: true\n  description: \"Key ID to use with --with-pre-authenticated-keys\"\n- name: \"--public-key\"\n  type: \"string\"\n  optional: true\n  description: \"Public key to use with --with-pre-authenticated-keys\"\n- name: \"--private-key\"\n  type: \"string\"\n  optional: true\n  description: \"Private key to use with --with-pre-authenticated-keys\"\n```75:{\"title\":\"Welcome to LM Studio Docs!\",\"description\":\"Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM Studio.\",\"index\":1}\n76:T96d,"])</script><script>self.__next_f.push([1,"\nTo get LM Studio, head over to the [Downloads page](/download) and download an installer for your operating system.\n\nLM Studio is available for macOS, Windows, and Linux.\n\n## What can I do with LM Studio?\n\n1. Download and run local LLMs like gpt-oss or Llama, Qwen\n2. Use a simple and flexible chat interface\n3. Connect MCP servers and use them with local models\n4. Search \u0026 download functionality (via Hugging Face ðŸ¤—)\n5. Serve local models on OpenAI-like endpoints, locally and on the network\n6. Manage your local models, prompts, and configurations\n\n## System requirements\n\nLM Studio generally supports Apple Silicon Macs, x64/ARM64 Windows PCs, and x64 Linux PCs.\n\nConsult the [System Requirements](app/system-requirements) page for more detailed information.\n\n## Run llama.cpp (GGUF) or MLX models\n\nLM Studio supports running LLMs on Mac, Windows, and Linux using [`llama.cpp`](https://github.com/ggerganov/llama.cpp).\n\nOn Apple Silicon Macs, LM Studio also supports running LLMs using Apple's [`MLX`](https://github.com/ml-explore/mlx).\n\nTo install or manage LM Runtimes, press `âŒ˜` `Shift` `R` on Mac or `Ctrl` `Shift` `R` on Windows/Linux.\n\n## LM Studio as an MCP client\n\nYou can install MCP servers in LM Studio and use them with your local models.\n\nSee the docs for more: [Use MCP server](/docs/app/plugins/mcp).\n\nIf you're develping an MCP server, check out [Add to LM Studio Button](/docs/app/plugins/mcp/deeplink).\n\n## Run an LLM like `gpt-oss`, `Llama`, `Qwen`, `Mistral`, or `DeepSeek R1` on your computer\n\nTo run an LLM on your computer you first need to download the model weights.\n\nYou can do this right within LM Studio! See [Download an LLM](app/basics/download-model) for guidance.\n\n## Chat with documents entirely offline on your computer\n\nYou can attach documents to your chat messages and interact with them entirely offline, also known as \"RAG\".\n\nRead more about how to use this feature in the [Chat with Documents](app/basics/rag) guide.\n\n## Use LM Studio's API from your own apps and scripts\n\nLM Studio provides a REST API that you can use to interact with your local models from your own apps and scripts.\n\n- [OpenAI Compatibility API](api/openai-api)\n- [LM Studio REST API (beta)](api/rest-api)\n\n\u003cbr /\u003e\n\n## Community\n\nJoin the LM Studio community on [Discord](https://discord.gg/aPQfnNkxGC) to ask questions, share knowledge, and get help from other users and the LM Studio team.\n"])</script><script>self.__next_f.push([1,"74:{\"metadata\":\"$75\",\"prettyName\":\"Welcome\",\"content\":\"$76\",\"pageRelUrl\":\"0_app/0_root/index.md\"}\n78:{\"title\":\"System Requirements\",\"description\":\"Supported CPU, GPU types for LM Studio on Mac (M1/M2/M3/M4), Windows (x64/ARM), and Linux (x64/ARM64)\",\"index\":3}\n79:T45c,\n## macOS\n\n- Chip: Apple Silicon (M1/M2/M3/M4).\n- macOS 13.4 or newer is required.\n  - For MLX models, macOS 14.0 or newer is required.\n- 16GB+ RAM recommended.\n  - You may still be able to use LM Studio on 8GB Macs, but stick to smaller models and modest context sizes.\n- Intel-based Macs are currently not supported. Chime in [here](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/9) if you are interested in this.\n\n## Windows\n\nLM Studio is supported on both x64 and ARM (Snapdragon X Elite) based systems.\n\n- CPU: AVX2 instruction set support is required (for x64)\n- RAM: LLMs can consume a lot of RAM. At least 16GB of RAM is recommended.\n- GPU: at least 4GB of dedicated VRAM is recommended.\n\n## Linux\n\nLM Studio is supported on both x64 and ARM64 (aarch64) based systems.\n\n- LM Studio for Linux is distributed as an AppImage.\n- Ubuntu 20.04 or newer is required\n- Ubuntu versions newer than 22 are not well tested. Let us know if you're running into issues by opening a bug [here](https://github.com/lmstudio-ai/lmstudio-bug-tracker).\n- CPU:\n  - On x64, LM Studio ships with AVX2 support by default\n77:{\"metadata\":\"$78\",\"prettyName\":\"System Requirements\",\"content\":\"$79\",\"pageRelUrl\":\"0_app/0_root/system-requirements.md\"}\n7b:{\"title\":\"Offline Operation\",\"description\":\"LM Studio can operate entirely offline, just make sure to get some model files first.\",\"index\":4}\n7c:Tb7f,"])</script><script>self.__next_f.push([1,"\n```lms_notice\nIn general, LM Studio does not require the internet in order to work. This includes core functions like chatting with models, chatting with documents, or running a local server, none of which require the internet.\n```\n\n### Operations that do NOT require connectivity\n\n#### Using downloaded LLMs\n\nOnce you have an LLM onto your machine, the model will run locally and you should be good to go entirely offline. Nothing you enter into LM Studio when chatting with LLMs leaves your device.\n\n#### Chatting with documents (RAG)\n\nWhen you drag and drop a document into LM Studio to chat with it or perform RAG, that document stays on your machine. All document processing is done locally, and nothing you upload into LM Studio leaves the application.\n\n#### Running a local server\n\nLM Studio can be used as a server to provide LLM inferencing on localhost or the local network. Requests to LM Studio use OpenAI endpoints and return OpenAI-like response objects, but stay local.\n\n### Operations that require connectivity\n\nSeveral operations, described below, rely on internet connectivity. Once you get an LLM onto your machine, you should be good to go entirely offline.\n\n#### Searching for models\n\nWhen you search for models in the Discover tab, LM Studio makes network requests (e.g. to huggingface.co). Search will not work without internet connection.\n\n#### Downloading new models\n\nIn order to download models you need a stable (and decently fast) internet connection. You can also 'sideload' models (use models that were procured outside the app). See instructions for [sideloading models](/docs/advanced/sideload).\n\n#### Discover tab's model catalog\n\nAny given version of LM Studio ships with an initial model catalog built-in. The entries in the catalog are typically the state of the online catalog near the moment we cut the release. However, in order to show stats and download options for each model, we need to make network requests (e.g. to huggingface.co).\n\n#### Downloading runtimes\n\n[LM Runtimes](advanced/lm-runtimes) are individually packaged software libraries, or LLM engines, that allow running certain formats of models (e.g. `llama.cpp`). As of LM Studio 0.3.0 (read the [announcement](https://lmstudio.ai/blog/lmstudio-v0.3.0)) it's easy to download and even hot-swap runtimes without a full LM Studio update. To check for available runtimes, and to download them, we need to make network requests.\n\n#### Checking for app updates\n\nOn macOS and Windows, LM Studio has a built-in app updater that's capable. The linux in-app updater [is in the works](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/89). When you open LM Studio, the app updater will make a network request to check if there are any new updates available. If there's a new version, the app will show you a notification to update now or later.\nWithout internet connectivity you will not be able to update the app via the in-app updater.\n"])</script><script>self.__next_f.push([1,"7a:{\"metadata\":\"$7b\",\"prettyName\":\"Offline Operation\",\"content\":\"$7c\",\"pageRelUrl\":\"0_app/0_root/offline.md\"}\n73:{\"\":\"$74\",\"system-requirements\":\"$77\",\"offline\":\"$7a\"}\n7f:{\"title\":\"Get started with LM Studio\",\"description\":\"Download and run Large Language Models like Qwen, Mistral, Gemma, or gpt-oss in LM Studio.\",\"index\":1}\n80:T795,\nDouble check computer meets the minimum [system requirements](/docs/system-requirements).\n\n```lms_info\nYou might sometimes see terms such as `open-source models` or `open-weights models`. Different models might be released under different licenses and varying degrees of 'openness'. In order to run a model locally, you need to be able to get access to its \"weights\", often distributed as one or more files that end with `.gguf`, `.safetensors` etc.\n```\n\n\u003chr\u003e\n\n## Getting up and running\n\nFirst, **install the latest version of LM Studio**. You can get it from [here](/download).\n\nOnce you're all set up, you need to **download your first LLM**.\n\n### 1. Download an LLM to your computer\n\nHead over to the Discover tab to download models. Pick one of the curated options or search for models by search query (e.g. `\"Llama\"`). See more in-depth information about downloading models [here](/docs/basics/download-models).\n\n\u003cimg src=\"/assets/docs/discover.png\" style=\"width: 500px; margin-top:30px\" data-caption=\"The Discover tab in LM Studio\" /\u003e\n\n### 2. Load a model to memory\n\nHead over to the **Chat** tab, and\n\n1. Open the model loader\n2. Select one of the models you downloaded (or [sideloaded](/docs/advanced/sideload)).\n3. Optionally, choose load configuration parameters.\n\n\u003cimg src=\"/assets/docs/loader.png\" data-caption=\"Quickly open the model loader with `cmd` + `L` on macOS or `ctrl` + `L` on Windows/Linux\" /\u003e\n\n##### What does loading a model mean?\n\nLoading a model typically means allocating memory to be able to accommodate the model's weights and other parameters in your computer's RAM.\n\n### 3. Chat!\n\nOnce the model is loaded, you can start a back-and-forth conversation with the model in the Chat ta"])</script><script>self.__next_f.push([1,"b.\n\n\u003cimg src=\"/assets/docs/chat.png\" data-caption=\"LM Studio on macOS\" /\u003e\n\n\u003chr\u003e\n\n### Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n7e:{\"metadata\":\"$7f\",\"prettyName\":\"Overview\",\"content\":\"$80\",\"pageRelUrl\":\"0_app/1_basics/index.md\",\"sectionKey\":\"basics\",\"sectionPrettyName\":\"Getting Started\"}\n82:{\"title\":\"Manage chats\",\"description\":\"Manage conversation threads with LLMs\",\"index\":2}\n83:T61d,\nLM Studio has a ChatGPT-like interface for chatting with local LLMs. You can create many different conversation threads and manage them in folders.\n\n\u003chr\u003e\n\n### Create a new chat\n\nYou can create a new chat by clicking the \"+\" button or by using a keyboard shortcut: `âŒ˜` + `N` on Mac, or `ctrl` + `N` on Windows / Linux.\n\n### Create a folder\n\nCreate a new folder by clicking the new folder button or by pressing: `âŒ˜` + `shift` + `N` on Mac, or `ctrl` + `shift` + `N` on Windows / Linux.\n\n### Drag and drop\n\nYou can drag and drop chats in and out of folders, and even drag folders into folders!\n\n### Duplicate chats\n\nYou can duplicate a whole chat conversation by clicking the `â€¢â€¢â€¢` menu and selecting \"Duplicate\". If the chat has any files in it, they will be duplicated too.\n\n## FAQ\n\n#### Where are chats stored in the file system?\n\nRight-click on a chat and choose \"Reveal in Finder\" / \"Show in File Explorer\".\nConversations are stored in JSON format. It is NOT recommended to edit them manually, nor to rely on their structure.\n\n#### Does the model learn from chats?\n\nThe model doesn't 'learn' from chats. The model only 'knows' the content that is present in the chat or is provided to it via configuration options such as the \"system prompt\".\n\n## Conversations folder filesystem path\n\nMac / Linux:\n\n```shell\n~/.lmstudio/conversations/\n```\n\nWindows:\n\n```ps\n%USERPROFILE%\\.lmstudio\\conversations\n```\n\n\u003chr\u003e\n\n### Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNk"])</script><script>self.__next_f.push([1,"xGC).\n81:{\"metadata\":\"$82\",\"prettyName\":\"Manage chats\",\"content\":\"$83\",\"pageRelUrl\":\"0_app/1_basics/chat.md\",\"sectionKey\":\"basics\",\"sectionPrettyName\":\"Getting Started\"}\n85:{\"title\":\"Download an LLM\",\"description\":\"Discover and download supported LLMs in LM Studio\",\"index\":3}\n86:T62e,\nLM Studio comes with a built-in model downloader that let's you download any supported model from [Hugging Face](https://huggingface.co).\n\n\u003cimg src=\"/assets/docs/discover.png\" style=\"width: 500px; margin-top:30px\" data-caption=\"Download models from the Discover tab in LM Studio\" /\u003e\n\n\u003chr\u003e\n\n### Searching for models\n\nYou can search for models by keyword (e.g. `llama`, `gemma`, `lmstudio`), or by providing a specific `user/model` string. You can even insert full Hugging Face URLs into the search bar!\n\n###### Pro tip: you can jump to the Discover tab from anywhere by pressing `âŒ˜` + `2` on Mac, or `ctrl` + `2` on Windows / Linux.\n\n### Which download option to choose?\n\nYou will often see several options for any given model named things like `Q3_K_S`, `Q_8` etc. These are all copies of the same model, provided in varying degrees of fidelity. The `Q` represents a technique called \"Quantization\", which roughly means compressing model files in size, while giving up some degree of quality.\n\nChoose a 4-bit option or higher if your machine is capable enough for running it.\n\n\u003cimg src=\"/assets/docs/search.png\" style=\"\" data-caption=\"Hugging Face search results in LM Studio\" /\u003e\n\n\u003chr\u003e\n\n`Advanced`\n\n### Changing the models directory\n\nYou can change the models directory by heading to My Models\n\n\u003cimg src=\"/assets/docs/change-models-dir.png\" style=\"width:80%\" data-caption=\"Manage your models directory in the My Models tab\"\u003e\n\n\u003chr\u003e\n\n### Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n84:{\"metadata\":\"$85\",\"prettyName\":\"Download an LLM\",\"content\":\"$86\",\"pageRelUrl\":\"0_app/1_basics/download-model.md\",\"sectionKey\":\"basics\",\"sectionPrettyName\":\"Getting Started\"}\n88:{"])</script><script>self.__next_f.push([1,"\"title\":\"Chat with Documents\",\"description\":\"How to provide local documents to an LLM as additional context\",\"index\":4}\n89:T61a,\nYou can attach document files (`.docx`, `.pdf`, `.txt`) to chat sessions in LM Studio.\n\nThis will provide additional context to LLMs you chat with through the app.\n\n\u003chr\u003e\n\n## Terminology\n\n- **Retrieval**: Identifying relevant portion of a long source document\n- **Query**: The input to the retrieval operation\n- **RAG**: Retrieval-Augmented Generation\\*\n- **Context**: the 'working memory' of an LLM. Has a maximum size\n\n###### \\* In this context, 'Generation' means the output of the LLM.\n###### Context sizes are measured in \"tokens\". One token is often about 3/4 of a word.\n\n## RAG vs. Full document 'in context'\n\nIf the document is short enough (i.e., if it fits in the model's context), LM Studio will add the file contents to the conversation in full. This is particularly useful for models that support longer context sizes such as Meta's Llama 3.1 and Mistral Nemo.\n\nIf the document is very long, LM Studio will opt into using \"Retrieval Augmented Generation\", frequently referred to as \"RAG\". RAG means attempting to fish out relevant bits of a very long document (or several documents) and providing them to the model for reference. This technique sometimes works really well, but sometimes it requires some tuning and experimentation.\n\n## Tip for successful RAG\n\nprovide as much context in your query as possible. Mention terms, ideas, and words you expect to be in the relevant source material. This will often increase the chance the system will provide useful context to the LLM. As always, experimentation is the best way to find what works best.\n87:{\"metadata\":\"$88\",\"prettyName\":\"Chat with Documents\",\"content\":\"$89\",\"pageRelUrl\":\"0_app/1_basics/rag.md\",\"sectionKey\":\"basics\",\"sectionPrettyName\":\"Getting Started\"}\n7d:{\"\":\"$7e\",\"chat\":\"$81\",\"download-model\":\"$84\",\"rag\":\"$87\"}\n8c:{\"title\":\"Use MCP Servers\",\"description\":\"Connect MCP servers to LM Studio\",\"index\":1}\n8d:Tb37,"])</script><script>self.__next_f.push([1,"\nStarting LM Studio 0.3.17, LM Studio acts as an **Model Context Protocol (MCP) Host**. This means you can connect MCP servers to the app and make them available to your models.\n\n### Be cautious\n\nNever install MCPs from untrusted sources.\n\n```lms_warning\nSome MCP servers can run arbitrary code, access your local files, and use your network connection. Always be cautious when installing and using MCP servers. If you don't trust the source, don't install it.\n```\n\n# Use MCP servers in LM Studio\n\nStarting 0.3.17 (b10), LM Studio supports both local and remote MCP servers. You can add MCPs by editing the app's `mcp.json` file or via the [\"Add to LM Studio\" Button](mcp/deeplink), when available. LM Studio currently follows Cursor's `mcp.json` notation.\n\n## Install new servers: `mcp.json`\n\nSwitch to the \"Program\" tab in the right hand sidebar. Click `Install \u003e Edit mcp.json`.\n\n\u003cimg src=\"/assets/docs/install-mcp.png\"  data-caption=\"\" style=\"width: 80%;\" className=\"\" /\u003e\n\nThis will open the `mcp.json` file in the in-app editor. You can add MCP servers by editing this file.\n\n\u003cimg src=\"/assets/docs/mcp-editor.png\"  data-caption=\"Edit mcp.json using the in-app editor\" style=\"width: 100%;\" className=\"\" /\u003e\n\n### Example MCP to try: Hugging Face MCP Server\n\nThis MCP server provides access to functions like model and dataset search.\n\n\u003cdiv className=\"w-fit\"\u003e\n  \u003ca style=\"background: rgb(255,255,255)\" href=\"https://lmstudio.ai/install-mcp?name=hf-mcp-server\u0026config=eyJ1cmwiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvL21jcCIsImhlYWRlcnMiOnsiQXV0aG9yaXphdGlvbiI6IkJlYXJlciA8WU9VUl9IRl9UT0tFTj4ifX0%3D\"\u003e\n    \u003cLightVariant\u003e\n      \u003cimg src=\"https://files.lmstudio.ai/deeplink/mcp-install-light.svg\" alt=\"Add MCP Server hf-mcp-server to LM Studio\" /\u003e\n    \u003c/LightVariant\u003e\n    \u003cDarkVariant\u003e\n      \u003cimg src=\"https://files.lmstudio.ai/deeplink/mcp-install-dark.svg\" alt=\"Add MCP Server hf-mcp-server to LM Studio\" /\u003e\n    \u003c/DarkVariant\u003e\n  \u003c/a\u003e\n\u003c/div\u003e\n\n```json\n{\n  \"mcpServers\": {\n    \"hf-mcp-server\": {\n      \"url\": \"https://huggingface.co/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer \u003cYOUR_HF_TOKEN\u003e\"\n      }\n    }\n  }\n}\n```\n\n###### You will need to replace `\u003cYOUR_HF_TOKEN\u003e` with your actual Hugging Face token. Learn more [here](https://huggingface.co/docs/hub/en/security-tokens).\n\nUse the [deeplink button](mcp/deeplink), or copy the JSON snippet above and paste it into your `mcp.json` file.\n\n---\n\n## Gotchas and Troubleshooting\n\n- Never install MCP servers from untrusted sources. Some MCPs can have far reaching access to your system.\n\n- Some MCP servers were designed to be used with Claude, ChatGPT, Gemini and might use excessive amounts of tokens.\n\n  - Watch out for this. It may quickly bog down your local model and trigger frequent context overflows.\n\n- When adding MCP servers manually, copy only the content after `\"mcpServers\": {` and before the closing `}`.\n"])</script><script>self.__next_f.push([1,"8b:{\"metadata\":\"$8c\",\"prettyName\":\"Use MCP Servers\",\"content\":\"$8d\",\"pageRelUrl\":\"0_app/2_mcp/index.md\",\"sectionKey\":\"mcp\",\"sectionPrettyName\":\"Model Context Protocol (MCP)\"}\n8f:{\"title\":\"`Add to LM Studio` Button\",\"description\":\"Add MCP servers to LM Studio using a deeplink\",\"index\":2}\n90:T4c0,\nYou can install MCP servers in LM Studio with one click using a deeplink.\n\nStarting with version 0.3.17 (10), LM Studio can act as an MCP host. Learn more about it [here](../mcp).\n\n---\n\n# Generate your own MCP install link\n\nEnter your MCP JSON entry to generate a deeplink for the `Add to LM Studio` button.\n\n```lms_mcp_deep_link_generator\n\n```\n\n## Try an example\n\nTry to copy and paste the following into the link generator above.\n\n```json\n{\n  \"hf-mcp-server\": {\n    \"url\": \"https://huggingface.co/mcp\",\n    \"headers\": {\n      \"Authorization\": \"Bearer \u003cYOUR_HF_TOKEN\u003e\"\n    }\n  }\n}\n```\n\n### Deeplink format\n\n```bash\nlmstudio://add_mcp?name=hf-mcp-server\u0026config=eyJ1cmwiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvL21jcCIsImhlYWRlcnMiOnsiQXV0aG9yaXphdGlvbiI6IkJlYXJlciA8WU9VUl9IRl9UT0tFTj4ifX0%3D\n```\n\n#### Parameters\n\n```lms_params\n- name: \"lmstudio://\"\n  type: \"protocol\"\n  description: \"The protocol scheme to open LM Studio\"\n- name: \"add_mcp\"\n  type: \"path\"\n  description: \"The action to install an MCP server\"\n- name: \"name\"\n  type: \"query parameter\"\n  description: \"The name of the MCP server to install\"\n- name: \"config\"\n  type: \"query parameter\"\n  description: \"Base64 encoded JSON configuration for the MCP server\"\n```\n8e:{\"metadata\":\"$8f\",\"prettyName\":\"`Add to LM Studio` Button\",\"content\":\"$90\",\"pageRelUrl\":\"0_app/2_mcp/deeplink.md\",\"sectionKey\":\"mcp\",\"sectionPrettyName\":\"Model Context Protocol (MCP)\"}\n8a:{\"\":\"$8b\",\"deeplink\":\"$8e\"}\n94:{\"url\":\"https://files.lmstudio.ai/modelyaml-card.jpg\",\"alt\":\"model.yaml logo\"}\n93:{\"title\":\"Introduction to `model.yaml`\",\"description\":\"Describe models with the cross-platform `model.yaml` specification.\",\"index\":5,\"socialCard\":\"$94\"}\n95:T11ab,"])</script><script>self.__next_f.push([1,"\n`Draft`\n\n[`model.yaml`](https://modelyaml.org) describes a model and all of its variants in a single portable file. Models in LM Studio's [model catalog](https://lmstudio.ai/models) are all implemented using model.yaml.\n\nThis allows abstracting away the underlying format (GGUF, MLX, etc) and presenting a single entry point for a given model. Furthermore, the model.yaml file supports baking in additional metadata, load and inference options, and even custom logic (e.g. enable/disable thinking).\n\n**You can clone existing model.yaml files on the LM Studio Hub and even [publish your own](./modelyaml/publish)!**\n\n## Core fields\n\n### `model`\n\nThe canonical identifier in the form `publisher/model`.\n\n```yaml\nmodel: qwen/qwen3-8b\n```\n\n### `base`\n\nPoints to the \"concrete\" model files or other virtual models. Each entry uses a unique `key` and one or more `sources` from which the file can be fetched.\n\nThe snippet below demonstrates a case where the model (`qwen/qwen3-8b`) can resolve to one of 3 different concrete models.\n\n```yaml\nmodel: qwen/qwen3-8b\nbase:\n  - key: lmstudio-community/qwen3-8b-gguf\n    sources:\n      - type: huggingface\n        user: lmstudio-community\n        repo: Qwen3-8B-GGUF\n  - key: lmstudio-community/qwen3-8b-mlx-4bit\n    sources:\n      - type: huggingface\n        user: lmstudio-community\n        repo: Qwen3-8B-MLX-4bit\n  - key: lmstudio-community/qwen3-8b-mlx-8bit\n    sources:\n      - type: huggingface\n        user: lmstudio-community\n        repo: Qwen3-8B-MLX-8bit\n```\n\nConcrete model files refer to the actual weights.\n\n### `metadataOverrides`\n\nOverrides the base model's metadata. This is useful for presentation purposes, for example in LM Studio's model catalog or in app model search. It is not used for any functional changes to the model.\n\n```yaml\nmetadataOverrides:\n  domain: llm\n  architectures:\n    - qwen3\n  compatibilityTypes:\n    - gguf\n    - safetensors\n  paramsStrings:\n    - 8B\n  minMemoryUsageBytes: 4600000000\n  contextLengths:\n    - 40960\n  vision: false\n  reasoning: true\n  trainedForToolUse: true\n```\n\n### `config`\n\nUse this to \"bake in\" default runtime settings (such as sampling parameters) and even load time options.\nThis works similarly to [Per Model Defaults](/docs/app/advanced/per-model).\n\n- `operation:` inference time parameters\n- `load:` load time parameters\n\n```yaml\nconfig:\n  operation:\n    fields:\n      - key: llm.prediction.topKSampling\n        value: 20\n      - key: llm.prediction.temperature\n        value: 0.7\n  load:\n    fields:\n      - key: llm.load.contextLength\n        value: 42690\n```\n\n### `customFields`\n\nDefine model-specific custom fields.\n\n```yaml\ncustomFields:\n  - key: enableThinking\n    displayName: Enable Thinking\n    description: Controls whether the model will think before replying\n    type: boolean\n    defaultValue: true\n    effects:\n      - type: setJinjaVariable\n        variable: enable_thinking\n```\n\nIn order for the above example to work, the jinja template needs to have a variable named `enable_thinking`.\n\n## Complete example\n\nTaken from https://lmstudio.ai/models/qwen/qwen3-8b\n\n```yaml\n# model.yaml is an open standard for defining cross-platform, composable AI models\n# Learn more at https://modelyaml.org\nmodel: qwen/qwen3-8b\nbase:\n  - key: lmstudio-community/qwen3-8b-gguf\n    sources:\n      - type: huggingface\n        user: lmstudio-community\n        repo: Qwen3-8B-GGUF\n  - key: lmstudio-community/qwen3-8b-mlx-4bit\n    sources:\n      - type: huggingface\n        user: lmstudio-community\n        repo: Qwen3-8B-MLX-4bit\n  - key: lmstudio-community/qwen3-8b-mlx-8bit\n    sources:\n      - type: huggingface\n        user: lmstudio-community\n        repo: Qwen3-8B-MLX-8bit\nmetadataOverrides:\n  domain: llm\n  architectures:\n    - qwen3\n  compatibilityTypes:\n    - gguf\n    - safetensors\n  paramsStrings:\n    - 8B\n  minMemoryUsageBytes: 4600000000\n  contextLengths:\n    - 40960\n  vision: false\n  reasoning: true\n  trainedForToolUse: true\nconfig:\n  operation:\n    fields:\n      - key: llm.prediction.topKSampling\n        value: 20\n      - key: llm.prediction.minPSampling\n        value:\n          checked: true\n          value: 0\ncustomFields:\n  - key: enableThinking\n    displayName: Enable Thinking\n    description: Controls whether the model will think before replying\n    type: boolean\n    defaultValue: true\n    effects:\n      - type: setJinjaVariable\n        variable: enable_thinking\n```\n\nThe [GitHub specification](https://github.com/modelyaml/modelyaml) contains further details and the latest schema.\n"])</script><script>self.__next_f.push([1,"92:{\"metadata\":\"$93\",\"prettyName\":\"Introduction to `model.yaml`\",\"content\":\"$95\",\"pageRelUrl\":\"0_app/3_modelyaml/index.md\",\"sectionKey\":\"modelyaml\",\"sectionPrettyName\":\"Models (model.yaml)\"}\n97:{\"title\":\"Publish a `model.yaml`\",\"description\":\"Upload your model definition to the LM Studio Hub.\",\"index\":7}\n98:T783,\nShare portable models by uploading a [`model.yaml`](./) to your page on the LM Studio Hub.\n\nAfter you publish a model.yaml to the LM Studio Hub, it will be available for other users to download with `lms get`.\n\n###### Note: `model.yaml` refers to metadata only. This means it does not include the actual model weights.\n\n# Quickstart\n\nThe easiest way to get started is by cloning an existing model, modifying it, and then running `lms push`.\n\nFor example, you can clone the Qwen 3 8B model:\n\n```shell\nlms clone qwen/qwen3-8b\n```\n\nThis will result in a local copy `model.yaml`, `README` and other metadata files. Importantly, this does NOT download the model weights.\n\n```lms_terminal\n$ ls\nREADME.md     manifest.json    model.yaml    thumbnail.png\n```\n\n## Change the publisher to your user\n\nThe first part in the `model:` field should be the username of the publisher. Change it to a username of a user or organization for which you have write access.\n\n```diff\n- model: qwen/qwen3-8b\n+ model: your-user-here/qwen3-8b\nbase:\n  - key: lmstudio-community/qwen3-8b-gguf\n    sources:\n# ... the rest of the file\n```\n\n## Sign in\n\nAuthenticate with the Hub from the command line:\n\n```shell\nlms login\n```\n\nThe CLI will print an authentication URL. After you approve access, the session token is saved locally so you can publish models.\n\n## Publish your model\n\nRun the push command in the directory containing `model.yaml`:\n\n```shell\nlms push\n```\n\nThe command packages the file, uploads it, and prints a revision number for the new version.\n\n### Override metadata at publish time\n\nUse `--overrides` to tweak fields without editing the file:\n\n```shell\nlms push --overrides '{\"description\": \"Qwen 3 8B model\"}'\n```\n\n## Downloading a model and usin"])</script><script>self.__next_f.push([1,"g it in LM Studio\n\nAfter publishing, the model appears under your user or organization profile on the LM Studio Hub.\n\nIt can then be downloaded with:\n\n```shell\nlms get my-user/my-model\n```\n96:{\"metadata\":\"$97\",\"prettyName\":\"Publish a `model.yaml`\",\"content\":\"$98\",\"pageRelUrl\":\"0_app/3_modelyaml/publish.md\",\"sectionKey\":\"modelyaml\",\"sectionPrettyName\":\"Models (model.yaml)\"}\n91:{\"\":\"$92\",\"publish\":\"$96\"}\n9b:{\"title\":\"Config Presets\",\"description\":\"Save your system prompts and other parameters as Presets for easy reuse across chats.\",\"index\":1}\n9c:Tac3,"])</script><script>self.__next_f.push([1,"\nPresets are a way to bundle together a system prompt and other parameters into a single configuration that can be easily reused across different chats.\n\nNew in 0.3.15: You can [import](/docs/app/presets/import) Presets from file or URL, and even [publish](/docs/app/presets/publish) your own Presets to share with others on to the LM Studio Hub.\n\u003chr\u003e\n\n## Saving, resetting, and deselecting Presets\n\nBelow is the anatomy of the Preset manager:\n\n\u003cimg src=\"/assets/docs/preset-widget-anatomy.png\" style=\"width:70%\" data-caption=\"The anatomy of the Preset manager in the settings sidebar.\"\u003e\n\n## Importing, Publishing, and Updating Downloaded Presets\n\nPresets are JSON files. You can share them by sending around the JSON, or you can share them by publishing them to the LM Studio Hub.\nYou can also import Presets from other users by URL. See the [Import](/docs/app/presets/import) and [Publish](/docs/app/presets/publish) sections for more details.\n\n## Example: Build your own Prompt Library\n\nYou can create your own prompt library by using Presets.\n\n\u003cvideo autoplay loop muted playsinline style=\"width:60vh;\" data-caption=\"Save collections of parameters as a Preset for easy reuse.\" class=\"border border-border\"\u003e\n  \u003csource src=\"https://files.lmstudio.ai/presets.mp4\" type=\"video/mp4\"\u003e\n  Your browser does not support the video tag.\n\u003c/video\u003e\n\nIn addition to system prompts, every parameter under the Advanced Configuration sidebar can be recorded in a named Preset.\n\nFor example, you might want to always use a certain Temperature, Top P, or Max Tokens for a particular use case. You can save these settings as a Preset (with or without a system prompt) and easily switch between them.\n\n#### The Use Case for Presets\n\n- Save your system prompts, inference parameters as a named `Preset`.\n- Easily switch between different use cases, such as reasoning, creative writing, multi-turn conversations, or brainstorming.\n\n## Where Presets are stored\n\nPresets are stored in the following directory:\n\n#### macOS or Linux\n\n```xml\n~/.lmstudio/config-presets\n```\n\n#### Windows\n\n```xml\n%USERPROFILE%\\.lmstudio\\config-presets\n```\n\n### Migration from LM Studio 0.2.\\* Presets\n\n- Presets you've saved in LM Studio 0.2.\\* are automatically readable in 0.3.3 with no migration step needed.\n- If you save **new changes** in a **legacy preset**, it'll be **copied** to a new format upon save.\n  - The old files are NOT deleted.\n- Notable difference: Load parameters are not included in the new preset format.\n  - Favor editing the model's default config in My Models. See [how to do it here](/docs/configuration/per-model).\n\n\u003chr\u003e\n\n### Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n"])</script><script>self.__next_f.push([1,"9a:{\"metadata\":\"$9b\",\"prettyName\":\"Overview\",\"content\":\"$9c\",\"pageRelUrl\":\"0_app/3_presets/index.md\",\"sectionKey\":\"presets\",\"sectionPrettyName\":\"Presets\"}\n9e:{\"title\":\"Importing and Sharing\",\"description\":\"You can import preset files directly from disk, or pull presets made by others via URL.\",\"index\":2}\n9f:T798,\nYou can import preset by file or URL. This is useful for sharing presets with others, or for importing presets from other users.\n\n\u003chr\u003e\n\n# Import Presets\n\nFirst, click the presets dropdown in the sidebar. You will see a list of your presets along with 2 buttons: `+ New Preset` and `Import`.\n\nClick the `Import` button to import a preset.\n\n\u003cimg src=\"/assets/docs/preset-import-button.png\" data-caption=\"Import Presets\" /\u003e\n\n## Import Presets from File\n\nOnce you click the Import button, you can select the source of the preset you want to import. You can either import from a file or from a URL.\n\u003cimg src=\"/assets/docs/import-preset-from-file.png\" data-caption=\"Import one or more Presets from file\" /\u003e\n\n## Import Presets from URL\n\nPresets that are [published](/docs/app/presets/publish) to the LM Studio Hub can be imported by providing their URL.\n\nImporting public presets does not require logging in within LM Studio.\n\n\u003cimg src=\"/assets/docs/import-preset-from-url.png\" data-caption=\"Import Presets by URL\" /\u003e\n\n### Using `lms` CLI\nYou can also use the CLI to import presets from URL. This is useful for sharing presets with others.\n\n```\nlms get {author}/{preset-name}\n```\n\nExample:\n```bash\nlms get neil/qwen3-thinking\n```\n\n\n### Find your config-presets directory\n\nLM Studio manages config presets on disk. Presets are local and private by default. You or others can choose to share them by sharing the file.\n\nClick on the `â€¢â€¢â€¢` button in the Preset dropdown and select \"Reveal in Finder\" (or \"Show in Explorer\" on Windows).\n\u003cimg src=\"/assets/docs/preset-reveal-in-finder.png\" data-caption=\"Reveal Preset in your local file system\" /\u003e\n\nThis will download the preset file and automatically surface it in the preset dropdown in t"])</script><script>self.__next_f.push([1,"he app. \n\n### Where Hub shared presets are stored\nPresets you share, and ones you download from the LM Studio Hub are saved in `~/.lmstudio/hub` on macOS and Linux, or `%USERPROFILE%\\.lmstudio\\hub` on Windows. 9d:{\"metadata\":\"$9e\",\"prettyName\":\"Importing and Sharing\",\"content\":\"$9f\",\"pageRelUrl\":\"0_app/3_presets/import.md\",\"sectionKey\":\"presets\",\"sectionPrettyName\":\"Presets\"}\na1:{\"title\":\"Publish Your Presets\",\"description\":\"Publish your Presets to the LM Studio Hub. Share your Presets with the community or with your colleagues.\",\"index\":3}\na2:T52d,\n`Feature In Preview`\n\nStarting LM Studio 0.3.15, you can publish your Presets to the LM Studio community. This allows you to share your Presets with others and import Presets from other users.\n\nThis feature is early and we would love to hear your feedback. Please report bugs and feedback to bugs@lmstudio.ai.\n\n---\n\n## Step 1: Click the Publish Button\n\nIdentify the Preset you want to publish in the Preset dropdown. Click the `â€¢â€¢â€¢` button and select \"Publish\" from the menu.\n\n\u003cimg src=\"/assets/docs/preset-publish-new.png\" data-caption=\"Click the Publish button to publish your Preset to the LM Studio Hub.\" /\u003e\n\n## Step 2: Set the Preset Details\n\nYou will be prompted to set the details of your Preset. This includes the name (slug) and optional description. \n\nCommunity presets are public and can be used by anyone on the internet!\n\n\u003cimg src=\"/assets/docs/preset-publish-details.png\" data-caption=\"Set the details of your Preset before publishing.\" /\u003e\n\n#### Privacy and Terms\nFor good measure, visit the [Privacy Policy](https://lmstudio.ai/hub-privacy) and [Terms of Service](https://lmstudio.ai/hub-terms) to understand what's suitable to share on the Hub, and how data is handled. Community presets are public and visible to everyone. Make sure you agree to what these documents say before publishing your Preset.a0:{\"metadata\":\"$a1\",\"prettyName\":\"Publish a Preset\",\"content\":\"$a2\",\"pageRelUrl\":\"0_app/3_presets/publish.md\",\"sectionKey\":\"presets\",\"sectionPrettyName\":\"Presets\"}\na4:"])</script><script>self.__next_f.push([1,"{\"title\":\"Pull Updates\",\"description\":\"How to pull the latest revisions of your Presets, or presets you have imported from others.\",\"index\":4}\na3:{\"metadata\":\"$a4\",\"prettyName\":\"Pull Updates\",\"content\":\"`Feature In Preview`\\n\\nYou can pull the latest revisions of your Presets, or presets you have imported from others. This is useful for keeping your Presets up to date with the latest changes.\\n\\n\u003chr\u003e\\n\\n## How to Pull Updates\\nClick the `â€¢â€¢â€¢` button in the Preset dropdown and select \\\"Pull\\\" from the menu.\\n\\n\u003cimg src=\\\"/assets/docs/preset-pull-latest.png\\\" data-caption=\\\"Pull the latest revisions of your or imported Presets.\\\" /\u003e\\n\\n## Your Presets vs Others'\\n\\nBoth your published Presets and other downloaded Presets can be pulled and updated the same way.\",\"pageRelUrl\":\"0_app/3_presets/pull.md\",\"sectionKey\":\"presets\",\"sectionPrettyName\":\"Presets\"}\na6:{\"title\":\"Push New Revisions\",\"description\":\"Publish new revisions of your Presets to the LM Studio Hub.\",\"index\":5}\na5:{\"metadata\":\"$a6\",\"prettyName\":\"Push New Revisions\",\"content\":\"\\n`Feature In Preview`\\n\\nStarting LM Studio 0.3.15, you can publish your Presets to the LM Studio community. This allows you to share your Presets with others and import Presets from other users.\\n\\nThis feature is early and we would love to hear your feedback. Please report bugs and feedback to bugs@lmstudio.ai.\\n\\n---\\n\\n## Published Presets\\n\\nPresets you share on the LM Studio Hub can be updated.\\n\\n\u003cimg src=\\\"/assets/docs/preset-cloud-indicator.png\\\" data-caption=\\\"Your shared Presets are marked with a cloud icon.\\\" /\u003e\\n\\n## Step 1: Make Changes and Commit\\n\\nMake any changes to your Preset, both in parameters that are already included in the Preset, or by adding new parameters.\\n\\n## Step 2: Click the Push Button\\nOnce changes are committed, you will see a `Push` button. Click it to push your changes to the Hub. \\n\\nPushing changes will result in a new revision of your Preset on the Hub.\\n\\n\u003cimg src=\\\"/assets/docs/preset-push-button.png\\\" data-caption=\\\"Click the Push but"])</script><script>self.__next_f.push([1,"ton to push your changes to the Hub.\\\" /\u003e\\n\",\"pageRelUrl\":\"0_app/3_presets/push.md\",\"sectionKey\":\"presets\",\"sectionPrettyName\":\"Presets\"}\n99:{\"\":\"$9a\",\"import\":\"$9d\",\"publish\":\"$a0\",\"pull\":\"$a3\",\"push\":\"$a5\"}\na9:{\"title\":\"Speculative Decoding\",\"description\":\"Speed up generation with a draft model\",\"index\":1}\naa:T138b,"])</script><script>self.__next_f.push([1,"\n`Advanced`\n\nSpeculative decoding is a technique that can substantially increase the generation speed of large language models (LLMs) without reducing response quality.\n\n\u003chr\u003e\n\n## What is Speculative Decoding\n\nSpeculative decoding relies on the collaboration of two models:\n\n- A larger, \"main\" model\n- A smaller, faster \"draft\" model\n\nDuring generation, the draft model rapidly proposes potential tokens (subwords), which the main model can verify faster than it would take it to generate them from scratch. To maintain quality, the main model only accepts tokens that match what it would have generated. After the last accepted draft token, the main model always generates one additional token.\n\nFor a model to be used as a draft model, it must have the same \"vocabulary\" as the main model.\n\n## How to enable Speculative Decoding\n\nOn `Power User` mode or higher, load a model, then select a `Draft Model` within the `Speculative Decoding` section of the chat sidebar:\n\n\u003cimg src=\"/assets/docs/speculative-decoding-setting.png\" style=\"width:80%; margin-top: 20px; border: 1px solid rgba(0,0,0,0.2);\" data-caption=\"The Speculative Decoding section of the chat sidebar\"\u003e\n\n### Finding compatible draft models\n\nYou might see the following when you open the dropdown:\n\n\u003cimg src=\"/assets/docs/speculative-decoding-no-compatible.png\" style=\"width:40%; margin-top: 20px; border: 1px solid rgba(0,0,0,0.2);\" data-caption=\"No compatible draft models\"\u003e\n\nTry to download a lower parameter variant of the model you have loaded, if it exists. If no smaller versions of your model exist, find a pairing that does.\n\nFor example:\n\n\u003ccenter style=\"margin: 20px;\"\u003e\n\n|          Main Model          |          Draft Model          |\n| :--------------------------: | :---------------------------: |\n|    Llama 3.1 8B Instruct     |     Llama 3.2 1B Instruct     |\n|    Qwen 2.5 14B Instruct     |    Qwen 2.5 0.5B Instruct     |\n| DeepSeek R1 Distill Qwen 32B | DeepSeek R1 Distill Qwen 1.5B |\n\n\u003c/center\u003e\n\nOnce you have both a main and draft model loaded, simply begin chatting to enable speculative decoding.\n\n## Key factors affecting performance\n\nSpeculative decoding speed-up is generally dependent on two things:\n\n1. How small and fast the _draft model_ is compared with the _main model_\n2. How often the draft model is able to make \"good\" suggestions\n\nIn simple terms, you want to choose a draft model that's much smaller than the main model. And some prompts will work better than others.\n\n### An important trade-off\n\nRunning a draft model alongside a main model to enable speculative decoding requires more **computation and resources** than running the main model on its own.\n\nThe key to faster generation of the main model is choosing a draft model that's both small and capable enough.\n\nHere are general guidelines for the **maximum** draft model size you should select based on main model size (in parameters):\n\n\u003ccenter style=\"margin: 20px;\"\u003e\n\n| Main Model Size | Max Draft Model Size to Expect Speed-Ups |\n| :-------------: | :--------------------------------------: |\n|       3B        |                    -                     |\n|       7B        |                    1B                    |\n|       14B       |                    3B                    |\n|       32B       |                    7B                    |\n\n\u003c/center\u003e\n\nGenerally, the larger the size difference is between the main model and the draft model, the greater the speed-up.\n\nNote: if the draft model is not fast enough or effective enough at making \"good\" suggestions to the main model, the generation speed will not increase, and could actually decrease.\n\n### Prompt dependent\n\nOne thing you will likely notice when using speculative decoding is that the generation speed is not consistent across all prompts.\n\nThe reason that the speed-up is not consistent across all prompts is because for some prompts, the draft model is less likely to make \"good\" suggestions to the main model.\n\nHere are some extreme examples that illustrate this concept:\n\n#### 1. Discrete Example: Mathematical Question\n\nPrompt: \"What is the quadratic equation formula?\"\n\nIn this case, both a 70B model and a 0.5B model are both very likely to give the standard formula `x = (-b Â± âˆš(bÂ² - 4ac))/(2a)`. So if the draft model suggested this formula as the next tokens, the target model would likely accept it, making this an ideal case for speculative decoding to work efficiently.\n\n#### 2. Creative Example: Story Generation\n\nPrompt: \"Write a story that begins: 'The door creaked open...'\"\n\nIn this case, the smaller model's draft tokens are likely be rejected more often by the larger model, as each next word could branch into countless valid possibilities.\n\nWhile \"4\" is the only reasonable answer to \"2+2\", this story could continue with \"revealing a monster\", \"as the wind howled\", \"and Sarah froze\", or hundreds of other perfectly valid continuations, making the smaller model's specific word predictions much less likely to match the larger\nmodel's choices.\n"])</script><script>self.__next_f.push([1,"a8:{\"metadata\":\"$a9\",\"prettyName\":\"Speculative Decoding\",\"content\":\"$aa\",\"pageRelUrl\":\"0_app/5_advanced/speculative-decoding.md\",\"sectionKey\":\"advanced\",\"sectionPrettyName\":\"Advanced\"}\nac:{\"title\":\"Import Models\",\"description\":\"Use model files you've downloaded outside of LM Studio\",\"index\":6}\nad:T4bf,\nYou can use compatible models you've downloaded outside of LM Studio by placing them in the expected directory structure.\n\n\u003chr\u003e\n\n### Use `lms import` (experimental)\n\nTo import a `GGUF` model you've downloaded outside of LM Studio, run the following command in your terminal:\n\n```bash\nlms import \u003cpath/to/model.gguf\u003e\n```\n\n###### Follow the interactive prompt to complete the import process.\n\n### LM Studio's expected models directory structure\n\n\u003cimg src=\"/assets/docs/reveal-models-dir.png\" style=\"width:80%\" data-caption=\"Manage your models directory in the My Models tab\"\u003e\n\nLM Studio aims to preserves the directory structure of models downloaded from Hugging Face. The expected directory structure is as follows:\n\n```xml\n~/.lmstudio/models/\nâ””â”€â”€ publisher/\n    â””â”€â”€ model/\n        â””â”€â”€ model-file.gguf\n```\n\nFor example, if you have a model named `ocelot-v1` published by `infra-ai`, the structure would look like this:\n\n```xml\n~/.lmstudio/models/\nâ””â”€â”€ infra-ai/\n    â””â”€â”€ ocelot-v1/\n        â””â”€â”€ ocelot-v1-instruct-q4_0.gguf\n```\n\n\u003chr\u003e\n\n### Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\nab:{\"metadata\":\"$ac\",\"prettyName\":\"Import Models\",\"content\":\"$ad\",\"pageRelUrl\":\"0_app/5_advanced/import-model.md\",\"sectionKey\":\"advanced\",\"sectionPrettyName\":\"Advanced\"}\naf:{\"title\":\"Per-model Defaults\",\"description\":\"You can set default settings for each model in LM Studio\",\"index\":null}\nb0:T7f7,\n`Advanced`\n\nYou can set default load settings for each model in LM Studio.\n\nWhen the model is loaded anywhere in the app (including through [`lms load`](/docs/cli#load-a-model-with-options)) these settings will be used.\n\n\u003chr\u003e\n\n### "])</script><script>self.__next_f.push([1,"Setting default parameters for a model\n\nHead to the My Models tab and click on the gear âš™ï¸ icon to edit the model's default parameters.\n\n\u003cimg src=\"/assets/docs/model-settings-gear.png\" style=\"width:80%\" data-caption=\"Click on the gear icon to edit the default load settings for a model.\"\u003e\n\nThis will open a dialog where you can set the default parameters for the model.\n\n\u003cvideo autoplay loop muted playsinline style=\"width:50%\" data-caption=\"You can set the default parameters for a model in this dialog.\"\u003e\n  \u003csource src=\"https://files.lmstudio.ai/default-params.mp4\" type=\"video/mp4\"\u003e\n  Your browser does not support the video tag.\n\u003c/video\u003e\n\nNext time you load the model, these settings will be used.\n\n\n```lms_protip\n#### Reasons to set default load parameters (not required, totally optional)\n\n- Set a particular GPU offload settings for a given model\n- Set a particular context size for a given model\n- Whether or not to utilize Flash Attention for a given model\n\n```\n\n\n\n\n## Advanced Topics\n\n### Changing load settings before loading a model\n\nWhen you load a model, you can optionally change the default load settings.\n\n\u003cimg src=\"/assets/docs/load-model.png\" style=\"width:80%\" data-caption=\"You can change the load settings before loading a model.\"\u003e\n\n### Saving your changes as the default settings for a model\n\nIf you make changes to load settings when you load a model, you can save them as the default settings for that model.\n\n\u003cimg src=\"/assets/docs/save-load-changes.png\" style=\"width:80%\" data-caption=\"If you make changes to load settings when you load a model, you can save them as the default settings for that model.\"\u003e\n\n\n\u003chr\u003e\n\n### Community\nChat with other LM Studio power users, discuss configs, models, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\nae:{\"metadata\":\"$af\",\"prettyName\":\"Per-model Defaults\",\"content\":\"$b0\",\"pageRelUrl\":\"0_app/5_advanced/per-model.md\",\"sectionKey\":\"advanced\",\"sectionPrettyName\":\"Advanced\"}\nb2:{\"title\":\"Prompt Template\",\"description\":\"Optionally set or modify"])</script><script>self.__next_f.push([1," the model's prompt template\",\"index\":null}\nb3:T6aa,\n`Advanced`\n\nBy default, LM Studio will automatically configure the prompt template based on the model file's metadata. \n\nHowever, you can customize the prompt template for any model.\n\n\u003chr\u003e\n\n\n### Overriding the Prompt Template for a Specific Model\n\nHead over to the My Models tab and click on the gear âš™ï¸ icon to edit the model's default parameters.\n###### Pro tip: you can jump to the My Models tab from anywhere by pressing `âŒ˜` + `3` on Mac, or `ctrl` + `3` on Windows / Linux.\n\n### Customize the Prompt Template\n\n###### ðŸ’¡ In most cases you don't need to change the prompt template\n\nWhen a model doesn't come with a prompt template information, LM Studio will surface the `Prompt Template` config box in the **ðŸ§ª Advanced Configuration** sidebar.\n\n\u003cimg src=\"/assets/docs/prompt-template.png\" style=\"width:80%\" data-caption=\"The Prompt Template config box in the chat sidebar\"\u003e\n\nYou can make this config box always show up by right clicking the sidebar and selecting **Always Show Prompt Template**.\n\n### Prompt template options\n\n#### Jinja Template\nYou can express the prompt template in Jinja.\n\n###### ðŸ’¡ [Jinja](https://en.wikipedia.org/wiki/Jinja_(template_engine)) is a templating engine used to encode the prompt template in several popular LLM model file formats.\n\n#### Manual\n\nYou can also express the prompt template manually by specifying message role prefixes and suffixes.\n\n\u003chr\u003e\n\n#### Reasons you might want to edit the prompt template:\n1. The model's metadata is incorrect, incomplete, or LM Studio doesn't recognize it\n2. The model does not have a prompt template in its metadata (e.g. custom or older models)\n3. You want to customize the prompt template for a specific use caseb1:{\"metadata\":\"$b2\",\"prettyName\":\"Prompt Template\",\"content\":\"$b3\",\"pageRelUrl\":\"0_app/5_advanced/prompt-template.md\",\"sectionKey\":\"advanced\",\"sectionPrettyName\":\"Advanced\"}\na7:{\"speculative-decoding\":\"$a8\",\"import-model\":\"$ab\",\"per-model\":\"$ae\",\"prompt-template\":\"$b1\"}\nb6:{\"title\":\"LM St"])</script><script>self.__next_f.push([1,"udio in your language\",\"description\":\"LM Studio is available in English, Chinese, Spanish, French, German, Korean, Russian, and 26+ more languages.\",\"index\":null}\nb7:T1070,"])</script><script>self.__next_f.push([1,"\nLM Studio is available in `English`, `Spanish`, `Japanese`, `Chinese`, `German`, `Norwegian`, `Turkish`, `Russian`, `Korean`, `Polish`, `Vietnamese`, `Czech`, `Ukrainian`, `Portuguese (BR,PT)` and many more languages thanks to incredible  community localizers.\n\n\u003chr\u003e\n\n### Selecting a Language\n\nYou can choose a language in the Settings tab.\n\nUse the dropdown menu under Preferences \u003e Language.\n\n```lms_protip\nYou can jump to Settings from anywhere in the app by pressing `cmd` + `,` on macOS or `ctrl` + `,` on Windows/Linux.\n```\n\n###### To get to the Settings page, you need to be on [Power User mode](/docs/modes) or higher.\n\n\u003chr\u003e\n\n#### Big thank you to community localizers ðŸ™\n\n- Spanish [@xtianpaiva](https://github.com/xtianpaiva), [@AlexisGross](https://github.com/AlexisGross), [@Tonband](https://github.com/Tonband)\n- Norwegian [@Exlo84](https://github.com/Exlo84)\n- German [@marcelMaier](https://github.com/marcelMaier), [@Goekdeniz-Guelmez](https://github.com/Goekdeniz-Guelmez)\n- Romanian (ro) [@alexandrughinea](https://github.com/alexandrughinea)\n- Turkish (tr) [@progesor](https://github.com/progesor), [@nossbar](https://github.com/nossbar)\n- Russian [@shelomitsky](https://github.com/shelomitsky), [@mlatysh](https://github.com/mlatysh), [@Adjacentai](https://github.com/Adjacentai), [@HostFly](https://github.com/HostFly), [@MotyaDev](https://github.com/MotyaDev), [@Autumn-Whisper](https://github.com/Autumn-Whisper), [@seropheem](https://github.com/seropheem)\n- Korean [@williamjeong2](https://github.com/williamjeong2)\n- Polish [@danieltechdev](https://github.com/danieltechdev)\n- Czech [@ladislavsulc](https://github.com/ladislavsulc)\n- Vietnamese [@trinhvanminh](https://github.com/trinhvanminh), [@godkyo98](https://github.com/godkyo98)\n- Portuguese (BR) [@Sm1g00l](https://github.com/Sm1g00l), [@altiereslima](https://github.com/altiereslima)\n- Portuguese (PT) [@catarino](https://github.com/catarino)\n- Chinese (zh-CN) [@neotan](https://github.com/neotan), [@SweetDream0256](https://github.com/SweetDream0256), [@enKl03B](https://github.com/enKl03B), [@evansrrr](https://github.com/evansrrr), [@xkonglong](https://github.com/xkonglong), [@shadow01a](https://github.com/shadow01a)\n- Chinese (zh-HK), (zh-TW) [@neotan](https://github.com/neotan), [ceshizhuanyong895](https://github.com/ceshizhuanyong895), [@BrassaiKao](https://github.com/BrassaiKao)\n- Chinese (zh-Hant) [@kywarai](https://github.com/kywarai), [ceshizhuanyong895](https://github.com/ceshizhuanyong895)\n- Ukrainian (uk) [@hmelenok](https://github.com/hmelenok)\n- Japanese (ja) [@digitalsp](https://github.com/digitalsp)\n- Dutch (nl) [@alaaf11](https://github.com/alaaf11)\n- Italian (it) [@fralapo](https://github.com/fralapo), [@Bl4ck-D0g](https://github.com/Bl4ck-D0g), [@nikypalma](https://github.com/nikypalma)\n- Indonesian (id) [@dwirx](https://github.com/dwirx)\n- Greek (gr) [@ilikecatgirls](https://github.com/ilikecatgirls)\n- Swedish (sv) [@reinew](https://github.com/reinew)\n- Catalan (ca) [@Gopro3010](https://github.com/Gopro3010)\n- French [@Plexi09](https://github.com/Plexi09)\n- Finnish (fi) [@divergentti](https://github.com/divergentti)\n- Bengali (bn) [@AbiruzzamanMolla](https://github.com/AbiruzzamanMolla)\n- Malayalam (ml) [@prasanthc41m](https://github.com/prasanthc41m)\n- Thai (th) [@gnoparus](https://github.com/gnoparus)\n- Bosnian (bs) [@0haris0](https://github.com/0haris0)\n- Bulgarian (bg) [@DenisZekiria](https://github.com/DenisZekiria)\n- Hindi (hi) [@suhailtajshaik](https://github.com/suhailtajshaik)\n- Hungarian (hu) [@Mekemoka](https://github.com/Mekemoka)\n- Persian (Farsi) (fa) [@mohammad007kh](https://github.com/mohammad007kh), [@darwindev](https://github.com/darwindev)\n- Arabic (ar) [@haqbany](https://github.com/haqbany)\n\nStill under development (due to lack of RTL support in LM Studio)\n\n- Hebrew: [@NHLOCAL](https://github.com/NHLOCAL)\n\n#### Contributing to LM Studio localization\n\nIf you want to improve existing translations or contribute new ones, you're more than welcome to jump in.\n\nLM Studio strings are maintained in https://github.com/lmstudio-ai/localization.\n\nSee instructions for contributing [here](https://github.com/lmstudio-ai/localization/blob/main/README.md).\n"])</script><script>self.__next_f.push([1,"b5:{\"metadata\":\"$b6\",\"prettyName\":\"Languages\",\"content\":\"$b7\",\"pageRelUrl\":\"0_app/6_user-interface/languages.md\",\"sectionKey\":\"user-interface\",\"sectionPrettyName\":\"User Interface\"}\nb9:{\"title\":\"User, Power User, or Developer\",\"description\":\"Hide or reveal advanced features\",\"index\":null}\nb8:{\"metadata\":\"$b9\",\"prettyName\":\"UI Modes\",\"content\":\"\\n### Selecting a UI Mode\\n\\nYou can configure LM Studio to run in increasing levels of configurability.\\n\\nSelect between User, Power User, and Developer.\\n\\n\u003cimg src=\\\"/assets/docs/modes.png\\\" style=\\\"width: 500px; margin-top:30px\\\" data-caption=\\\"Choose a mode at the bottom of the app\\\" /\u003e\\n\\n### Which mode should I choose?\\n\\n#### `User`\\n\\nShow only the chat interface, and auto-configure everything. This is the best choice for beginners or anyone who's happy with the default settings.\\n\\n#### `Power User`\\n\\nUse LM Studio in this mode if you want access to configurable [load](/docs/configuration/load) and [inference](/docs/configuration/inference) parameters as well as advanced chat features such as [insert, edit, \u0026amp; continue](/docs/advanced/context) (for either role, user or assistant).\\n\\n#### `Developer`\\n\\nFull access to all aspects in LM Studio. This includes keyboard shortcuts and development features. Check out the Developer section under Settings for more.\\n\",\"pageRelUrl\":\"0_app/6_user-interface/modes.md\",\"sectionKey\":\"user-interface\",\"sectionPrettyName\":\"User Interface\"}\nbb:{\"title\":\"Color Themes\",\"description\":\"Customize LM Studio's color theme\",\"index\":null}\nba:{\"metadata\":\"$bb\",\"prettyName\":\"Color Themes\",\"content\":\"\\n### Selecting a Theme\\n\\nPress `Cmd` + `K` then `T` (macOS) or `Ctrl` + `K` then `T` (Windows/Linux) to open the theme selector.\\n\\nYou can also choose a theme in the Settings tab (`Cmd` + `,` on macOS or `Ctrl` + `,` on Windows/Linux).\\n\\nChoosing the \\\"Auto\\\" option will automatically switch between Light and Dark themes based on your system settings.\\n\",\"pageRelUrl\":\"0_app/6_user-interface/themes.md\",\"sectionKey\":\"user-interface\",\"section"])</script><script>self.__next_f.push([1,"PrettyName\":\"User Interface\"}\nb4:{\"languages\":\"$b5\",\"modes\":\"$b8\",\"themes\":\"$ba\"}\n72:{\"\":\"$73\",\"basics\":\"$7d\",\"mcp\":\"$8a\",\"modelyaml\":\"$91\",\"presets\":\"$99\",\"advanced\":\"$a7\",\"user-interface\":\"$b4\"}\nbe:{\"title\":\"LM Studio Developer Docs\",\"description\":\"Build with LM Studio's local APIs and SDKs â€” TypeScript, Python, REST, and OpenAIâ€‘compatible endpoints.\",\"index\":1}\nbf:T7c1,\n```lms_hstack\n## Get to know the stack\n\n- TypeScript SDK: [lmstudio-js](/docs/typescript)\n- Python SDK: [lmstudio-python](/docs/python)\n- OpenAIâ€‘compatible: [Chat, Responses, Embeddings](/docs/developer/openai-compat)\n- LM Studio CLI: [`lms`](/docs/cli)\n\n:::split:::\n\n## What you can build\n\n- Chat and text generation with streaming\n- Structured output (JSON schema)\n- Tool calling and local agents\n- Embeddings and tokenization\n- Model management (JIT load, TTL, autoâ€‘evict)\n```\n\n## Super quick start\n\n### TypeScript (`lmstudio-js`)\n\n```bash\nnpm install @lmstudio/sdk\n```\n\n```ts\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\nconst model = await client.llm.model(\"openai/gpt-oss-20b\");\nconst result = await model.respond(\"Who are you, and what can you do?\");\n\nconsole.info(result.content);\n```\n\nFull docs: [lmstudio-js](/docs/typescript), Source: [GitHub](https://github.com/lmstudio-ai/lmstudio-js)\n\n### Python (`lmstudio-python`)\n\n```bash\npip install lmstudio\n```\n\n```python\nimport lmstudio as lms\n\nwith lms.Client() as client:\n    model = client.llm.model(\"openai/gpt-oss-20b\")\n    result = model.respond(\"Who are you, and what can you do?\")\n    print(result)\n```\n\nFull docs: [lmstudio-python](/docs/python), Source: [GitHub](https://github.com/lmstudio-ai/lmstudio-python)\n\n### Try a minimal HTTP request (OpenAIâ€‘compatible)\n\n```bash\nlms server start --port 1234\n```\n\n```bash\ncurl http://localhost:1234/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai/gpt-oss-20b\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Who are you, and what can you do?\"}]\n  }'\n```\n\nFull docs: [OpenA"])</script><script>self.__next_f.push([1,"Iâ€‘compatible endpoints](/docs/developer/openai-compat)\n\n## Helpful links\n\n- API Changelog: [/docs/developer/api-changelog](/docs/developer/api-changelog)\n- Local server basics: [/docs/developer/core](/docs/developer/core)\n- CLI reference: [/docs/cli](/docs/cli)\n- Community: [Discord](https://discord.gg/lmstudio)\nbd:{\"metadata\":\"$be\",\"prettyName\":\"Introduction\",\"content\":\"$bf\",\"pageRelUrl\":\"1_developer/index.md\"}\nc1:{\"title\":\"API Changelog\",\"description\":\"Updates and changes to the LM Studio API.\",\"index\":2}\nc2:T243a,"])</script><script>self.__next_f.push([1,"\n---\n\n###### LM Studio 0.3.29 â€¢Â 2025â€‘10â€‘06\n\n### OpenAI `/v1/responses` and variant listing\n\n- New OpenAIâ€‘compatible endpoint: `POST /v1/responses`.\n  - Stateful interactions via `previous_response_id`.\n  - Custom tool calling and Remote MCP support (optâ€‘in).\n  - Reasoning support with `reasoning.effort` for `openai/gptâ€‘ossâ€‘20b`.\n  - Streaming via SSE when `stream: true`.\n- CLI: `lms ls --variants` lists all variants for multiâ€‘variant models.\n- Docs: [/docs/developer/openai-compat](/docs/developer/openai-compat). Full release notes: [/blog/lmstudio-v0.3.29](/blog/lmstudio-v0.3.29).\n\n---\n\n###### LM Studio 0.3.27 â€¢Â 2025â€‘09â€‘24\n\n### CLI: model resource estimates, status, and interrupts\n\n- New: `lms load --estimate-only \u003cmodel\u003e` prints estimated GPU and total memory before loading. Honors `--context-length` and `--gpu`, and uses an improved estimator that now accounts for flash attention and vision models.\n- `lms chat`: press `Ctrl+C` to interrupt an ongoing prediction.\n- `lms ps --json` now reports each model's generation status and the number of queued prediction requests.\n- CLI color contrast improved for light mode.\n- See docs: [/docs/cli/local-models/load](/docs/cli/local-models/load). Full release notes: [/blog/lmstudio-v0.3.27](/blog/lmstudio-v0.3.27).\n\n---\n\n###### LM Studio 0.3.26 â€¢Â 2025â€‘09â€‘15\n\n### CLI log streaming: server + model\n\n- `lms log stream` now supports multiple sources and filters.\n  - `--source server` streams HTTP server logs (startup, endpoints, status)\n  - `--source model --filter input,output` streams formatted user input and model output\n  - Append `--json` for machineâ€‘readable logs; `--stats` adds tokens/sec and related metrics (model source)\n- See usage and examples: [/docs/cli/serve/log-stream](/docs/cli/serve/log-stream). Full release notes: [/blog/lmstudio-v0.3.26](/blog/lmstudio-v0.3.26).\n\n---\n\n###### LM Studio 0.3.25 â€¢Â 2025â€‘09â€‘04\n\n### New model support (API)\n\n- Added support for NVIDIA Nemotronâ€‘Nanoâ€‘v2 with toolâ€‘calling via the OpenAIâ€‘compatible endpoints [â€¡](/blog/lmstudio-v0.3.25).\n- Added support for Google EmbeddingGemma for the `/v1/embeddings` endpoint [â€¡](/blog/lmstudio-v0.3.25).\n\n---\n\n###### LM Studio 0.3.24 â€¢Â 2025â€‘08â€‘28\n\n### Seedâ€‘OSS toolâ€‘calling and template fixes\n\n- Added support for ByteDance/Seedâ€‘OSS including toolâ€‘calling and promptâ€‘template compatibility fixes in the OpenAIâ€‘compatible API [â€¡](/blog/lmstudio-v0.3.24).\n- Fixed cases where tool calls were not parsed for certain prompt templates [â€¡](/blog/lmstudio-v0.3.24).\n\n---\n\n###### LM Studio 0.3.23 â€¢Â 2025â€‘08â€‘12\n\n### Reasoning content and toolâ€‘calling reliability\n\n- For `gptâ€‘oss` on `POST /v1/chat/completions`, reasoning content moves out of `message.content` and into `choices.message.reasoning` (nonâ€‘streaming) and `choices.delta.reasoning` (streaming), aligning with `o3â€‘mini` [â€¡](/blog/lmstudio-v0.3.23).\n- Tool names are normalized (e.g., snake_case) before being provided to the model to improve toolâ€‘calling reliability [â€¡](/blog/lmstudio-v0.3.23).\n- Fixed errors for certain toolsâ€‘containing requests to `POST /v1/chat/completions` (e.g., \"reading 'properties'\") and nonâ€‘streaming toolâ€‘call failures [â€¡](/blog/lmstudio-v0.3.23).\n\n---\n\n###### LM Studio 0.3.19 â€¢Â 2025â€‘07â€‘21\n\n### Bug fixes for streaming and tool calls\n\n- Corrected usage statistics returned by OpenAIâ€‘compatible streaming responses [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.19#:~:text=,OpenAI%20streaming%20responses%20were%20incorrect).\n- Improved handling of parallel tool calls via the streaming API [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.19#:~:text=,API%20were%20not%20handled%20correctly).\n- Fixed parsing of correct tool calls for certain Mistral models [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.19#:~:text=,Ryzen%20AI%20PRO%20300%20series).\n\n---\n\n###### LM Studio 0.3.18 â€¢Â 2025â€‘07â€‘10\n\n### Streaming options and toolâ€‘calling improvements\n\n- Added support for the `stream_options` object on OpenAIâ€‘compatible endpoints. Setting `stream_options.include_usage` to `true` returns prompt and completion token usage during streaming [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=%2A%20Added%20support%20for%20%60,to%20support%20more%20prompt%20templates).\n- Errors returned from streaming endpoints now follow the correct format expected by OpenAI clients [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=,with%20proper%20chat%20templates).\n- Toolâ€‘calling support added for MistralÂ v13 tokenizer models, using proper chat templates [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=,with%20proper%20chat%20templates).\n- The `response_format.type` field now accepts `\"text\"` in chatâ€‘completion requests [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=,that%20are%20split%20across%20multiple).\n- Fixed bugs where parallel tool calls split across multiple chunks were dropped and where rootâ€‘level `$defs` in tool definitions were stripped [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=,being%20stripped%20in%20tool%20definitions).\n\n---\n\n###### LM Studio 0.3.17 â€¢Â 2025â€‘06â€‘25\n\n### Toolâ€‘calling reliability and tokenâ€‘count updates\n\n- Token counts now include the system prompt and tool definitions [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.17#:~:text=,have%20a%20URL%20in%20the). This makes usage reporting more accurate for both the UI and the API.\n- Toolâ€‘call argument tokens are streamed as they are generated [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.17#:~:text=Build%206), improving responsiveness when using streamed function calls.\n- Various fixes improve MCP and toolâ€‘calling reliability, including correct handling of tools that omit a `parameters` object and preventing hangs when an MCP server reloads [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.17#:~:text=,tool%20calls%20would%20hang%20indefinitely).\n\n---\n\n###### LM Studio 0.3.16 â€¢Â 2025â€‘05â€‘23\n\n### Model capabilities in `GETÂ /models`\n\n- The OpenAIâ€‘compatible REST API (`/api/v0`) now returns a `capabilities` array in the `GETÂ /models` response. Each model lists its supported capabilities (e.g. `\"tool_use\"`) [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.16#:~:text=,response) so clients can programmatically discover toolâ€‘enabled models.\n- Fixed a streaming bug where an empty function name string was appended after the first packet of streamed tool calls [â€¡](https://lmstudio.ai/blog/lmstudio-v0.3.16#:~:text=%2A%20Bugfix%3A%20%5BOpenAI,packet%20of%20streamed%20function%20calls).\n\n---\n\n###### [ðŸ‘¾ LM Studio 0.3.15](/blog/lmstudio-v0.3.15) â€¢ 2025-04-24\n\n### Improved Tool Use API Support\n\nOpenAI-like REST API now supports the `tool_choice` parameter:\n\n```json\n{\n  \"tool_choice\": \"auto\" // or \"none\", \"required\"\n}\n```\n\n- `\"tool_choice\": \"none\"` â€” Model will not call tools\n- `\"tool_choice\": \"auto\"` â€” Model decides\n- `\"tool_choice\": \"required\"` â€” Model must call tools (llama.cpp only)\n\nChunked responses now set `\"finish_reason\": \"tool_calls\"` when appropriate.\n\n---\n\n###### [ðŸ‘¾ LM Studio 0.3.14](/blog/lmstudio-v0.3.14) â€¢ 2025-03-27\n\n### [API/SDK] Preset Support\n\nRESTful API and SDKs support specifying presets in requests.\n\n_(example needed)_\n\n###### [ðŸ‘¾ LM Studio 0.3.10](/blog/lmstudio-v0.3.10) â€¢ 2025-02-18\n\n### Speculative Decoding API\n\nEnable speculative decoding in API requests with `\"draft_model\"`:\n\n```json\n{\n  \"model\": \"deepseek-r1-distill-qwen-7b\",\n  \"draft_model\": \"deepseek-r1-distill-qwen-0.5b\",\n  \"messages\": [ ... ]\n}\n```\n\nResponses now include a `stats` object for speculative decoding:\n\n```json\n\"stats\": {\n  \"tokens_per_second\": ...,\n  \"draft_model\": \"...\",\n  \"total_draft_tokens_count\": ...,\n  \"accepted_draft_tokens_count\": ...,\n  \"rejected_draft_tokens_count\": ...,\n  \"ignored_draft_tokens_count\": ...\n}\n```\n\n---\n\n###### [ðŸ‘¾ LM Studio 0.3.9](blog/lmstudio-v0.3.9) â€¢ 2025-01-30\n\n### Idle TTL and Auto Evict\n\nSet a TTL (in seconds) for models loaded via API requests (docs article: [Idle TTL and Auto-Evict](/docs/developer/core/ttl-and-auto-evict))\n\n```diff\ncurl http://localhost:1234/api/v0/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"deepseek-r1-distill-qwen-7b\",\n    \"messages\": [ ... ]\n+   \"ttl\": 300,\n}'\n```\n\nWith `lms`:\n\n```\nlms load --ttl \u003cseconds\u003e\n```\n\n### Separate `reasoning_content` in Chat Completion responses\n\nFor DeepSeek R1 models, get reasoning content in a separate field. See more [here](/blog/lmstudio-v0.3.9#separate-reasoningcontent-in-chat-completion-responses).\n\nTurn this on in App Settings \u003e Developer.\n\n---\n\n###### [ðŸ‘¾ LM Studio 0.3.6](blog/lmstudio-v0.3.6) â€¢ 2025-01-06\n\n### Tool and Function Calling API\n\nUse any LLM that supports Tool Use and Function Calling through the OpenAI-like API.\n\nDocs: [Tool Use and Function Calling](/docs/developer/core/tools).\n\n---\n\n###### [ðŸ‘¾ LM Studio 0.3.5](blog/lmstudio-v0.3.5) â€¢ 2024-10-22\n\n### Introducing `lms get`: download models from the terminal\n\nYou can now download models directly from the terminal using a keyword\n\n```bash\nlms get deepseek-r1\n```\n\nor a full Hugging Face URL\n\n```bash\nlms get \u003chugging face url\u003e\n```\n\nTo filter for MLX models only, add `--mlx` to the command.\n\n```bash\nlms get deepseek-r1 --mlx\n```\n"])</script><script>self.__next_f.push([1,"c0:{\"metadata\":\"$c1\",\"prettyName\":\"API Changelog\",\"content\":\"$c2\",\"pageRelUrl\":\"1_developer/api-changelog.md\"}\nc5:{\"title\":\"Idle TTL and Auto-Evict\",\"description\":\"Optionally auto-unload idle models after a certain amount of time (TTL)\",\"index\":1}\nc6:T1170,"])</script><script>self.__next_f.push([1,"\n## Background\n\n- `JIT loading` makes it easy to use your LM Studio models in other apps: you don't need to manually load the model first before being able to use it. However, this also means that models can stay loaded in memory even when they're not being used. `[Default: enabled]`\n\n- (New) `Idle TTL` (technically: Time-To-Live) defines how long a model can stay loaded in memory without receiving any requests. When the TTL expires, the model is automatically unloaded from memory. You can set a TTL using the `ttl` field in your request payload. `[Default: 60 minutes]`\n\n- (New) `Auto-Evict` is a feature that unloads previously JIT loaded models before loading new ones. This enables easy switching between models from client apps without having to manually unload them first. You can enable or disable this feature in Developer tab \u003e Server Settings. `[Default: enabled]`\n\n## Idle TTL\n\n**Use case**: imagine you're using an app like [Zed](https://github.com/zed-industries/zed/blob/main/crates/lmstudio/src/lmstudio.rs#L340), [Cline](https://github.com/cline/cline/blob/main/src/api/providers/lmstudio.ts), or [Continue.dev](https://docs.continue.dev/customize/model-providers/more/lmstudio) to interact with LLMs served by LM Studio. These apps leverage JIT to load models on-demand the first time you use them.\n\n**Problem**: When you're not actively using a model, you might don't want it to remain loaded in memory.\n\n**Solution**: Set a TTL for models loaded via API requests. The idle timer resets every time the model receives a request, so it won't disappear while you use it. A model is considered idle if it's not doing any work. When the idle TTL expires, the model is automatically unloaded from memory.\n\n### Set App-default Idle TTL\n\nBy default, JIT-loaded models have a TTL of 60 minutes. You can configure a default TTL value for any model loaded via JIT like so:\n\n\u003cimg src=\"/assets/docs/app-default-ttl.png\" style=\"width: 500px; \" data-caption=\"Set a default TTL value. Will be used for all JIT loaded models unless specified otherwise in the request payload\" /\u003e\n\n### Set per-model TTL-model in API requests\n\nWhen JIT loading is enabled, the **first request** to a model will load it into memory. You can specify a TTL for that model in the request payload.\n\nThis works for requests targeting both the [OpenAI compatibility API](/docs/developer/openai-api) and the [LM Studio's REST API](/docs/developer/rest):\n\n```diff\ncurl http://localhost:1234/api/v0/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"deepseek-r1-distill-qwen-7b\",\n+   \"ttl\": 300,\n    \"messages\": [ ... ]\n}'\n```\n\n###### This will set a TTL of 5 minutes (300 seconds) for this model if it is JIT loaded.\n\n### Set TTL for models loaded with `lms`\n\nBy default, models loaded with `lms load` do not have a TTL, and will remain loaded in memory until you manually unload them.\n\nYou can set a TTL for a model loaded with `lms` like so:\n\n```bash\nlms load \u003cmodel\u003e --ttl 3600\n```\n\n###### Load a `\u003cmodel\u003e` with a TTL of 1 hour (3600 seconds)\n\n### Specify TTL when loading models in the server tab\n\nYou can also set a TTL when loading a model in the server tab like so\n\n\u003cimg src=\"/assets/docs/ttl-server-model.png\" style=\"width: 100%;\" data-caption=\"Set a TTL value when loading a model in the server tab\" /\u003e\n\n## Configure Auto-Evict for JIT loaded models\n\nWith this setting, you can ensure new models loaded via JIT automatically unload previously loaded models first.\n\nThis is useful when you want to switch between models from another app without worrying about memory building up with unused models.\n\n\u003cimg src=\"/assets/docs/auto-evict-and-ttl.png\" style=\"width: 500px; margin-top:30px\" data-caption=\"Enable or disable Auto-Evict for JIT loaded models in the Developer tab \u003e Server Settings\" /\u003e\n\n**When Auto-Evict is ON** (default):\n\n- At most `1` model is kept loaded in memory at a time (when loaded via JIT)\n- Non-JIT loaded models are not affected\n\n**When Auto-Evict is OFF**:\n\n- Switching models from an external app will keep previous models loaded in memory\n- Models will remain loaded until either:\n  - Their TTL expires\n  - You manually unload them\n\nThis feature works in tandem with TTL to provide better memory management for your workflow.\n\n### Nomenclature\n\n`TTL`: Time-To-Live, is a term borrowed from networking protocols and cache systems. It defines how long a resource can remain allocated before it's considered stale and evicted.\n"])</script><script>self.__next_f.push([1,"c4:{\"metadata\":\"$c5\",\"prettyName\":\"Idle TTL and Auto-Evict\",\"content\":\"$c6\",\"pageRelUrl\":\"1_developer/0_core/ttl-and-auto-evict.md\",\"sectionKey\":\"core\",\"sectionPrettyName\":\"Core\"}\nc8:{\"title\":\"Run LM Studio as a service (headless)\",\"description\":\"GUI-less operation of LM Studio: run in the background, start on machine login, and load models on demand\",\"index\":2}\nc9:T95e,"])</script><script>self.__next_f.push([1,"\nLM Studio can be run as a service without the GUI. This is useful for running LM Studio on a server or in the background on your local machine. This works on Mac, Windows, and Linux machines with a graphical user interface.\n\n## Run LM Studio as a service\n\nRunning LM Studio as a service consists of several new features intended to make it more efficient to use LM Studio as a developer tool.\n\n1. The ability to run LM Studio without the GUI\n2. The ability to start the LM Studio LLM server on machine login, headlessly\n3. On-demand model loading\n\n## Run the LLM service on machine login\n\nTo enable this, head to app settings (`Cmd` / `Ctrl` + `,`) and check the box to run the LLM server on login.\n\n\u003cimg src=\"/assets/docs/headless-settings.png\" style=\"\" data-caption=\"Enable the LLM server to start on machine login\" /\u003e\n\nWhen this setting is enabled, exiting the app will minimize it to the system tray, and the LLM server will continue to run in the background.\n\n## Just-In-Time (JIT) model loading for OpenAI endpoints\n\nUseful when utilizing LM Studio as an LLM service with other frontends or applications.\n\n\u003cimg src=\"/assets/docs/jit-loading.png\" style=\"\" data-caption=\"Load models on demand\" /\u003e\n\n#### When JIT loading is ON:\n\n- Call to `/v1/models` will return all downloaded models, not only the ones loaded into memory\n- Calls to inference endpoints will load the model into memory if it's not already loaded\n\n#### When JIT loading is OFF:\n\n- Call to `/v1/models` will return only the models loaded into memory\n- You have to first load the model into memory before being able to use it\n\n##### What about auto unloading?\n\nAs of LM Studio 0.3.5, auto unloading is not yet in place. Models that are loaded via JIT loading will remain in memory until you unload them.\nWe expect to implement more sophisticated memory management in the near future. Let us know if you have any feedback or suggestions.\n\n## Auto Server Start\n\nYour last server state will be saved and restored on app or service launch.\n\nTo achieve this programmatically, you can use the following command:\n\n```bash\nlms server start\n```\n\n### Community\n\nChat with other LM Studio developers, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n\nPlease report bugs and issues in the [lmstudio-bug-tracker](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues) GitHub repository.\n"])</script><script>self.__next_f.push([1,"c7:{\"metadata\":\"$c8\",\"prettyName\":\"Headless Mode\",\"content\":\"$c9\",\"pageRelUrl\":\"1_developer/0_core/headless.md\",\"sectionKey\":\"core\",\"sectionPrettyName\":\"Core\"}\ncc:{\"title\":\"LM Studio as a Local LLM API Server\",\"description\":\"Run an LLM API server on `localhost` with LM Studio\",\"index\":1,\"fullPage\":false}\ncd:T411,\nYou can serve local LLMs from LM Studio's Developer tab, either on `localhost` or on the network.\n\nLM Studio's APIs can be used through [REST API](/docs/developer/rest), client libraries like [lmstudio-js](/docs/typescript) and [lmstudio-python](/docs/python), and [OpenAI compatibility endpoints](/docs/developer/openai-compat)\n\n\u003cimg src=\"/assets/docs/server.png\" style=\"\" data-caption=\"Load and serve LLMs from LM Studio\" /\u003e\n\n### Running the server\n\nTo run the server, go to the Developer tab in LM Studio, and toggle the \"Start server\" switch to start the API server.\n\n\u003cimg src=\"/assets/docs/server-start.png\" style=\"\" data-caption=\"Start the LM Studio API Server\" /\u003e\n\n\n\nAlternatively, you can use `lms` ([LM Studio's CLI](/docs/cli)) to start the server from your terminal:\n\n```bash\nlms server start\n```\n\n\n### API options\n\n- [LM Studio REST API](/docs/developer/rest)\n- [TypeScript SDK](/docs/typescript) - `lmstudio-js`\n- [Python SDK](/docs/python) - `lmstudio-python`\n- [OpenAI compatibility endpoints](/docs/developer/openai-compat)\ncb:{\"metadata\":\"$cc\",\"prettyName\":\"Running the Server\",\"content\":\"$cd\",\"pageRelUrl\":\"1_developer/0_core/0_server/index.md\",\"sectionKey\":\"core\",\"sectionPrettyName\":\"Core\"}\ncf:{\"title\":\"Server Settings\",\"description\":\"Configure server settings for LM Studio API Server\",\"index\":2,\"fullPage\":false}\nd0:T61a,\nYou can configure server settings, such as the port number, whether to allow other API clients to access the server and MCP features.\n\n\u003cimg src=\"/assets/docs/server-config.png\" style=\"\" data-caption=\"Configure LM Studio API Server settings\" /\u003e\n\n\n### Settings information\n\n```lms_params\n- name: Server Port\n  type: Integer\n  optional: false\n  description: Port number on which the LM Studi"])</script><script>self.__next_f.push([1,"o API server listens for incoming connections.\n  unstyledName: true\n- name: Serve on Local Network\n  type: Switch\n  description: Allow other devices on the same local network to access the API server. Learn more in the [Serve on Local Network](/docs/developer/core/server/serve-on-network) section.\n  unstyledName: true\n- name: Allow Per Request Remote MCPs\n  type: Switch\n  description: Enable sending requests to remote MCP (Model Control Protocol) servers on a per-request basis.\n  unstyledName: true\n- name: Enable CORS\n  type: Switch\n  description: Enable Cross-Origin Resource Sharing (CORS) to allow applications from different origins to access the API.\n  unstyledName: true\n- name: Just in Time Model Loading\n  type: Switch\n  description: Load models dynamically at request time to save memory.\n  unstyledName: true\n- name: Auto Unload Unused JIT Models\n  type: Switch\n  description: Automatically unload JIT-loaded models from memory when they are no longer in use.\n  unstyledName: true\n- name: Only Keep Last JIT Loaded Model\n  type: Switch\n  description: Keep only the most recently used JIT-loaded model in memory to minimize RAM usag\n  unstyledName: true\n```\nce:{\"metadata\":\"$cf\",\"prettyName\":\"Server Settings\",\"content\":\"$d0\",\"pageRelUrl\":\"1_developer/0_core/0_server/settings.md\",\"sectionKey\":\"core\",\"sectionPrettyName\":\"Core\"}\nd2:{\"title\":\"Serve on Local Network\",\"description\":\"Allow other devices in your network use this LM Studio API server\",\"index\":3,\"fullPage\":false}\nd1:{\"metadata\":\"$d2\",\"prettyName\":\"Serve on Local Network\",\"content\":\"\\n\\nEnabling the \\\"Serve on Local Network\\\" option allows the LM Studio API server running on your machine to be accessible by other devices connected to the same local network.\\n\\nThis is useful for scenarios where you want to:\\n- Use a local LLM on your other less powerful devices by connecting them to a more powerful machine running LM Studio.\\n- Let multiple people use a single LM Studio instance on the network.\\n- Use the API from IoT devices, edge computing units, or other ser"])</script><script>self.__next_f.push([1,"vices in your local setup.\\n\\nOnce enabled, the server will bind to your local network IP address instead of localhost. The API access URL will be updated accordingly which you can use in your applications.\\n\\n\u003cimg src=\\\"/assets/docs/serve-local-network.png\\\" style=\\\"\\\" data-caption=\\\"Serve LM Studio API Server on Local Network\\\" /\u003e\\n\",\"pageRelUrl\":\"1_developer/0_core/0_server/serve-on-network.md\",\"sectionKey\":\"core\",\"sectionPrettyName\":\"Core\"}\nca:{\"\":\"$cb\",\"settings\":\"$ce\",\"serve-on-network\":\"$d1\"}\nc3:{\"ttl-and-auto-evict\":\"$c4\",\"headless\":\"$c7\",\"server\":\"$ca\"}\nd5:{\"title\":\"REST API v0\",\"description\":\"The REST API includes enhanced stats such as Token / Second and Time To First Token (TTFT), as well as rich information about models such as loaded vs unloaded, max context, quantization, and more.\",\"index\":null}\nd6:T19a1,"])</script><script>self.__next_f.push([1,"\n##### Requires [LM Studio 0.3.6](/download) or newer.\n\nLM Studio now has its own REST API, in addition to OpenAI compatibility mode ([learn more](/docs/developer/openai-compat)).\n\nThe REST API includes enhanced stats such as Token / Second and Time To First Token (TTFT), as well as rich information about models such as loaded vs unloaded, max context, quantization, and more.\n\n#### Supported API Endpoints\n\n- [`GET /api/v0/models`](#get-apiv0models) - List available models\n- [`GET /api/v0/models/{model}`](#get-apiv0modelsmodel) - Get info about a specific model\n- [`POST /api/v0/chat/completions`](#post-apiv0chatcompletions) - Chat Completions (messages -\u003e assistant response)\n- [`POST /api/v0/completions`](#post-apiv0completions) - Text Completions (prompt -\u003e completion)\n- [`POST /api/v0/embeddings`](#post-apiv0embeddings) - Text Embeddings (text -\u003e embedding)\n\n---\n\n### Start the REST API server\n\nTo start the server, run the following command:\n\n```bash\nlms server start\n```\n\n```lms_protip\nYou can run LM Studio as a service and get the server to auto-start on boot without launching the GUI. [Learn about Headless Mode](/docs/developer/core/headless).\n```\n\n## Endpoints\n\n### `GET /api/v0/models`\n\nList all loaded and downloaded models\n\n**Example request**\n\n```bash\ncurl http://localhost:1234/api/v0/models\n```\n\n**Response format**\n\n```json\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"qwen2-vl-7b-instruct\",\n      \"object\": \"model\",\n      \"type\": \"vlm\",\n      \"publisher\": \"mlx-community\",\n      \"arch\": \"qwen2_vl\",\n      \"compatibility_type\": \"mlx\",\n      \"quantization\": \"4bit\",\n      \"state\": \"not-loaded\",\n      \"max_context_length\": 32768\n    },\n    {\n      \"id\": \"meta-llama-3.1-8b-instruct\",\n      \"object\": \"model\",\n      \"type\": \"llm\",\n      \"publisher\": \"lmstudio-community\",\n      \"arch\": \"llama\",\n      \"compatibility_type\": \"gguf\",\n      \"quantization\": \"Q4_K_M\",\n      \"state\": \"not-loaded\",\n      \"max_context_length\": 131072\n    },\n    {\n      \"id\": \"text-embedding-nomic-embed-text-v1.5\",\n      \"object\": \"model\",\n      \"type\": \"embeddings\",\n      \"publisher\": \"nomic-ai\",\n      \"arch\": \"nomic-bert\",\n      \"compatibility_type\": \"gguf\",\n      \"quantization\": \"Q4_0\",\n      \"state\": \"not-loaded\",\n      \"max_context_length\": 2048\n    }\n  ]\n}\n```\n\n---\n\n### `GET /api/v0/models/{model}`\n\nGet info about one specific model\n\n**Example request**\n\n```bash\ncurl http://localhost:1234/api/v0/models/qwen2-vl-7b-instruct\n```\n\n**Response format**\n\n```json\n{\n  \"id\": \"qwen2-vl-7b-instruct\",\n  \"object\": \"model\",\n  \"type\": \"vlm\",\n  \"publisher\": \"mlx-community\",\n  \"arch\": \"qwen2_vl\",\n  \"compatibility_type\": \"mlx\",\n  \"quantization\": \"4bit\",\n  \"state\": \"not-loaded\",\n  \"max_context_length\": 32768\n}\n```\n\n---\n\n### `POST /api/v0/chat/completions`\n\nChat Completions API. You provide a messages array and receive the next assistant response in the chat.\n\n**Example request**\n\n```bash\ncurl http://localhost:1234/api/v0/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"granite-3.0-2b-instruct\",\n    \"messages\": [\n      { \"role\": \"system\", \"content\": \"Always answer in rhymes.\" },\n      { \"role\": \"user\", \"content\": \"Introduce yourself.\" }\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": -1,\n    \"stream\": false\n  }'\n```\n\n**Response format**\n\n```json\n{\n  \"id\": \"chatcmpl-i3gkjwthhw96whukek9tz\",\n  \"object\": \"chat.completion\",\n  \"created\": 1731990317,\n  \"model\": \"granite-3.0-2b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"logprobs\": null,\n      \"finish_reason\": \"stop\",\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Greetings, I'm a helpful AI, here to assist,\\nIn providing answers, with no distress.\\nI'll keep it short and sweet, in rhyme you'll find,\\nA friendly companion, all day long you'll bind.\"\n      }\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 24,\n    \"completion_tokens\": 53,\n    \"total_tokens\": 77\n  },\n  \"stats\": {\n    \"tokens_per_second\": 51.43709529007664,\n    \"time_to_first_token\": 0.111,\n    \"generation_time\": 0.954,\n    \"stop_reason\": \"eosFound\"\n  },\n  \"model_info\": {\n    \"arch\": \"granite\",\n    \"quant\": \"Q4_K_M\",\n    \"format\": \"gguf\",\n    \"context_length\": 4096\n  },\n  \"runtime\": {\n    \"name\": \"llama.cpp-mac-arm64-apple-metal-advsimd\",\n    \"version\": \"1.3.0\",\n    \"supported_formats\": [\"gguf\"]\n  }\n}\n```\n\n---\n\n### `POST /api/v0/completions`\n\nText Completions API. You provide a prompt and receive a completion.\n\n**Example request**\n\n```bash\ncurl http://localhost:1234/api/v0/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"granite-3.0-2b-instruct\",\n    \"prompt\": \"the meaning of life is\",\n    \"temperature\": 0.7,\n    \"max_tokens\": 10,\n    \"stream\": false,\n    \"stop\": \"\\n\"\n  }'\n```\n\n**Response format**\n\n```json\n{\n  \"id\": \"cmpl-p9rtxv6fky2v9k8jrd8cc\",\n  \"object\": \"text_completion\",\n  \"created\": 1731990488,\n  \"model\": \"granite-3.0-2b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"text\": \" to find your purpose, and once you have\",\n      \"logprobs\": null,\n      \"finish_reason\": \"length\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"completion_tokens\": 9,\n    \"total_tokens\": 14\n  },\n  \"stats\": {\n    \"tokens_per_second\": 57.69230769230769,\n    \"time_to_first_token\": 0.299,\n    \"generation_time\": 0.156,\n    \"stop_reason\": \"maxPredictedTokensReached\"\n  },\n  \"model_info\": {\n    \"arch\": \"granite\",\n    \"quant\": \"Q4_K_M\",\n    \"format\": \"gguf\",\n    \"context_length\": 4096\n  },\n  \"runtime\": {\n    \"name\": \"llama.cpp-mac-arm64-apple-metal-advsimd\",\n    \"version\": \"1.3.0\",\n    \"supported_formats\": [\"gguf\"]\n  }\n}\n```\n\n---\n\n### `POST /api/v0/embeddings`\n\nText Embeddings API. You provide a text and a representation of the text as an embedding vector is returned.\n\n**Example request**\n\n```bash\ncurl http://localhost:1234/api/v0/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-nomic-embed-text-v1.5\",\n    \"input\": \"Some text to embed\"\n  }\n```\n\n**Example response**\n\n```json\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"embedding\": [\n        -0.016731496900320053,\n        0.028460891917347908,\n        -0.1407836228609085,\n        ... (truncated for brevity) ...,\n        0.02505224384367466,\n        -0.0037634256295859814,\n        -0.04341062530875206\n      ],\n      \"index\": 0\n    }\n  ],\n  \"model\": \"text-embedding-nomic-embed-text-v1.5@q4_k_m\",\n  \"usage\": {\n    \"prompt_tokens\": 0,\n    \"total_tokens\": 0\n  }\n}\n```\n\n---\n\nPlease report bugs by opening an issue on [Github](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues).\n"])</script><script>self.__next_f.push([1,"d4:{\"metadata\":\"$d5\",\"prettyName\":\"REST API v0\",\"content\":\"$d6\",\"pageRelUrl\":\"1_developer/2_rest/endpoints.md\",\"sectionKey\":\"rest\",\"sectionPrettyName\":\"LM Studio REST API\"}\nd3:{\"endpoints\":\"$d4\"}\nd9:{\"title\":\"OpenAI Compatibility Endpoints\",\"description\":\"Send requests to Responses, Chat Completions (text and images), Completions, and Embeddings endpoints.\",\"index\":1}\nda:T90a,"])</script><script>self.__next_f.push([1,"\n### Supported endpoints\n\n\u003ctable class=\"flexible-cols\"\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eEndpoint\u003c/th\u003e\n      \u003cth\u003eMethod\u003c/th\u003e\n      \u003cth\u003eDocs\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode\u003e/v1/models\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003capimethod method=\"GET\" /\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003ca href=\"/docs/developer/openai-compat/models\"\u003eModels\u003c/a\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode\u003e/v1/responses\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003capimethod method=\"POST\" /\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003ca href=\"/docs/developer/openai-compat/responses\"\u003eResponses\u003c/a\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode\u003e/v1/chat/completions\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003capimethod method=\"POST\" /\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003ca href=\"/docs/developer/openai-compat/chat-completions\"\u003eChat Completions\u003c/a\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode\u003e/v1/embeddings\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003capimethod method=\"POST\" /\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003ca href=\"/docs/developer/openai-compat/embeddings\"\u003eEmbeddings\u003c/a\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode\u003e/v1/completions\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003capimethod method=\"POST\" /\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003ca href=\"/docs/developer/openai-compat/completions\"\u003eCompletions\u003c/a\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003chr\u003e\n\n## Set the `base url` to point to LM Studio\n\nYou can reuse existing OpenAI clients (in Python, JS, C#, etc) by switching up the \"base URL\" property to point to your LM Studio instead of OpenAI's servers.\n\nNote: The following examples assume the server port is `1234`\n\n### Python Example\n\n```diff\nfrom openai import OpenAI\n\nclient = OpenAI(\n+    base_url=\"http://localhost:1234/v1\"\n)\n\n# ... the rest of your code ...\n```\n\n### Typescript Example\n\n```diff\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n+  baseUrl: \"http://localhost:1234/v1\"\n});\n\n// ... the rest of your code ...\n```\n\n### cURL Example\n\n```diff\n- curl https://api.openai.com/v1/chat/completions \\\n+ curl http://localhost:1234/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n-     \"model\": \"gpt-4o-mini\",\n+     \"model\": \"use the model identifier from LM Studio here\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n---\n\nOther OpenAI client libraries should have similar options to set the base URL.\n\nIf you're running into trouble, hop onto our [Discord](https://discord.gg/lmstudio) and enter the `#ðŸ”¨-developers` channel.\n"])</script><script>self.__next_f.push([1,"d8:{\"metadata\":\"$d9\",\"prettyName\":\"Overview\",\"content\":\"$da\",\"pageRelUrl\":\"1_developer/3_openai-compat/index.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"}\ndc:{\"title\":\"Structured Output\",\"description\":\"Enforce LLM response formats using JSON schemas.\",\"index\":2}\ndd:T122d,"])</script><script>self.__next_f.push([1,"\nYou can enforce a particular response format from an LLM by providing a JSON schema to the `/v1/chat/completions` endpoint, via LM Studio's REST API (or via any OpenAI client).\n\n\u003chr\u003e\n\n### Start LM Studio as a server\n\nTo use LM Studio programmatically from your own code, run LM Studio as a local server.\n\nYou can turn on the server from the \"Developer\" tab in LM Studio, or via the `lms` CLI:\n\n```\nlms server start\n```\n\n###### Install `lms` by running `npx lmstudio install-cli`\n\nThis will allow you to interact with LM Studio via the REST API. For an intro to LM Studio's REST API, see [REST API Overview](/docs/developer/rest).\n\n### Structured Output\n\nThe API supports structured JSON outputs through the `/v1/chat/completions` endpoint when given a [JSON schema](https://json-schema.org/overview/what-is-jsonschema). Doing this will cause the LLM to respond in valid JSON conforming to the schema provided.\n\nIt follows the same format as OpenAI's recently announced [Structured Output](https://platform.openai.com/docs/guides/structured-outputs) API and is expected to work via the OpenAI client SDKs.\n\n**Example using `curl`**\n\nThis example demonstrates a structured output request using the `curl` utility.\n\nTo run this example on Mac or Linux, use any terminal. On Windows, use [Git Bash](https://git-scm.com/download/win).\n\n```bash\ncurl http://localhost:1234/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"{{model}}\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful jokester.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Tell me a joke.\"\n      }\n    ],\n    \"response_format\": {\n      \"type\": \"json_schema\",\n      \"json_schema\": {\n        \"name\": \"joke_response\",\n        \"strict\": \"true\",\n        \"schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"joke\": {\n              \"type\": \"string\"\n            }\n          },\n          \"required\": [\"joke\"]\n        }\n      }\n    },\n    \"temperature\": 0.7,\n    \"max_tokens\": 50,\n    \"stream\": false\n  }'\n```\n\nAll parameters recognized by `/v1/chat/completions` will be honored, and the JSON schema should be provided in the `json_schema` field of `response_format`.\n\nThe JSON object will be provided in `string` form in the typical response field, `choices[0].message.content`, and will need to be parsed into a JSON object.\n\n**Example using `python`**\n\n```python\nfrom openai import OpenAI\nimport json\n\n# Initialize OpenAI client that points to the local LM Studio server\nclient = OpenAI(\n    base_url=\"http://localhost:1234/v1\",\n    api_key=\"lm-studio\"\n)\n\n# Define the conversation with the AI\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n    {\"role\": \"user\", \"content\": \"Create 1-3 fictional characters\"}\n]\n\n# Define the expected response structure\ncharacter_schema = {\n    \"type\": \"json_schema\",\n    \"json_schema\": {\n        \"name\": \"characters\",\n        \"schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"characters\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"name\": {\"type\": \"string\"},\n                            \"occupation\": {\"type\": \"string\"},\n                            \"personality\": {\"type\": \"string\"},\n                            \"background\": {\"type\": \"string\"}\n                        },\n                        \"required\": [\"name\", \"occupation\", \"personality\", \"background\"]\n                    },\n                    \"minItems\": 1,\n                }\n            },\n            \"required\": [\"characters\"]\n        },\n    }\n}\n\n# Get response from AI\nresponse = client.chat.completions.create(\n    model=\"your-model\",\n    messages=messages,\n    response_format=character_schema,\n)\n\n# Parse and display the results\nresults = json.loads(response.choices[0].message.content)\nprint(json.dumps(results, indent=2))\n```\n\n**Important**: Not all models are capable of structured output, particularly LLMs below 7B parameters.\n\nCheck the model card README if you are unsure if the model supports structured output.\n\n### Structured output engine\n\n- For `GGUF` models: utilize `llama.cpp`'s grammar-based sampling APIs.\n- For `MLX` models: using [Outlines](https://github.com/dottxt-ai/outlines).\n\nThe MLX implementation is available on Github: [lmstudio-ai/mlx-engine](https://github.com/lmstudio-ai/mlx-engine).\n\n\u003chr\u003e\n\n### Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n"])</script><script>self.__next_f.push([1,"db:{\"metadata\":\"$dc\",\"prettyName\":\"Structured Output\",\"content\":\"$dd\",\"pageRelUrl\":\"1_developer/3_openai-compat/structured-output.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"}\ndf:{\"title\":\"Tool Use\",\"description\":\"Enable LLMs to interact with external functions and APIs.\",\"index\":2}\ne0:Ta108,"])</script><script>self.__next_f.push([1,"\nTool use enables LLMs to request calls to external functions and APIs through the `/v1/chat/completions` and `v1/responses` endpoints ([Learn more](/docs/developer/openai-compat)), via LM Studio's REST API (or via any OpenAI client). This expands their functionality far beyond text output.\n\n\u003chr\u003e\n\n## Quick Start\n\n### 1. Start LM Studio as a server\n\nTo use LM Studio programmatically from your own code, run LM Studio as a local server.\n\nYou can turn on the server from the \"Developer\" tab in LM Studio, or via the `lms` CLI:\n\n```bash\nlms server start\n```\n\n###### Install `lms` by running `npx lmstudio install-cli`\n\nThis will allow you to interact with LM Studio via the REST API. For an intro to LM Studio's REST API, see [REST API Overview](/docs/developer/rest).\n\n### 2. Load a Model\n\nYou can load a model from the \"Chat\" or \"Developer\" tabs in LM Studio, or via the `lms` CLI:\n\n```bash\nlms load\n```\n\n### 3. Copy, Paste, and Run an Example!\n\n- `Curl`\n  - [Single Turn Tool Call Request](#example-using-curl)\n- `Python`\n  - [Single Turn Tool Call + Tool Use](#single-turn-example)\n  - [Multi-Turn Example](#multi-turn-example)\n  - [Advanced Agent Example](#advanced-agent-example)\n\n## Tool Use\n\n### What really is \"Tool Use\"?\n\nTool use describes:\n\n- LLMs output text requesting functions to be called (LLMs cannot directly execute code)\n- Your code executes those functions\n- Your code feeds the results back to the LLM.\n\n### High-level flow\n\n```xml\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ SETUP: LLM + Tool list   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚    Get user input        â”‚â—„â”€â”€â”€â”€â”\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n           â–¼                     â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\nâ”‚ LLM prompted w/messages  â”‚     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n           â–¼                     â”‚\n     Needs tools?                â”‚\n      â”‚         â”‚                â”‚\n    Yes         No               â”‚\n      â”‚         â”‚                â”‚\n      â–¼         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚\nâ”‚Tool Responseâ”‚              â”‚   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚\n       â–¼                     â”‚   â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚\nâ”‚Execute toolsâ”‚              â”‚   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚\n       â–¼                     â–¼   â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Add results  â”‚          â”‚  Normal   â”‚\nâ”‚to messages  â”‚          â”‚ response  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n       â”‚                       â–²\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### In-depth flow\n\nLM Studio supports tool use through the `/v1/chat/completions` endpoint when given function definitions in the `tools` parameter of the request body. Tools are specified as an array of function definitions that describe their parameters and usage, like:\n\nIt follows the same format as OpenAI's [Function Calling](https://platform.openai.com/docs/guides/function-calling) API and is expected to work via the OpenAI client SDKs.\n\nWe will use [lmstudio-community/Qwen2.5-7B-Instruct-GGUF](https://model.lmstudio.ai/download/lmstudio-community/Qwen2.5-7B-Instruct-GGUF) as the model in this example flow.\n\n1. You provide a list of tools to an LLM. These are the tools that the model can _request_ calls to.\n   For example:\n\n```json\n// the list of tools is model-agnostic\n[\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"get_delivery_date\",\n      \"description\": \"Get the delivery date for a customer's order\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"order_id\": {\n            \"type\": \"string\"\n          }\n        },\n        \"required\": [\"order_id\"]\n      }\n    }\n  }\n]\n```\n\nThis list will be injected into the `system` prompt of the model depending on the model's chat template. For `Qwen2.5-Instruct`, this looks like:\n\n```json\n\u003c|im_start|\u003esystem\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within \u003ctools\u003e\u003c/tools\u003e XML tags:\n\u003ctools\u003e\n{\"type\": \"function\", \"function\": {\"name\": \"get_delivery_date\", \"description\": \"Get the delivery date for a customer's order\", \"parameters\": {\"type\": \"object\", \"properties\": {\"order_id\": {\"type\": \"string\"}}, \"required\": [\"order_id\"]}}}\n\u003c/tools\u003e\n\nFor each function call, return a json object with function name and arguments within \u003ctool_call\u003e\u003c/tool_call\u003e XML tags:\n\u003ctool_call\u003e\n{\"name\": \u003cfunction-name\u003e, \"arguments\": \u003cargs-json-object\u003e}\n\u003c/tool_call\u003e\u003c|im_end|\u003e\n```\n\n**Important**: The model can only _request_ calls to these tools because LLMs _cannot_ directly call functions, APIs, or any other tools. They can only output text, which can then be parsed to programmatically call the functions.\n\n2. When prompted, the LLM can then decide to either:\n\n   - (a) Call one or more tools\n\n   ```xml\n   User: Get me the delivery date for order 123\n   Model: \u003ctool_call\u003e\n   {\"name\": \"get_delivery_date\", \"arguments\": {\"order_id\": \"123\"}}\n   \u003c/tool_call\u003e\n   ```\n\n   - (b) Respond normally\n\n   ```xml\n   User: Hi\n   Model: Hello! How can I assist you today?\n   ```\n\n3. LM Studio parses the text output from the model into an OpenAI-compliant `chat.completion` response object.\n\n   - If the model was given access to `tools`, LM Studio will attempt to parse the tool calls into the `response.choices[0].message.tool_calls` field of the `chat.completion` response object.\n   - If LM Studio cannot parse any **correctly formatted** tool calls, it will simply return the response to the standard `response.choices[0].message.content` field.\n   - **Note**: Smaller models and models that were not trained for tool use may output improperly formatted tool calls, resulting in LM Studio being unable to parse them into the `tool_calls` field. This is useful for troubleshooting when you do not receive `tool_calls` as expected. Example of an improperly formatting `Qwen2.5-Instruct` tool call:\n\n   ```xml\n   \u003ctool_call\u003e\n   [\"name\": \"get_delivery_date\", function: \"date\"]\n   \u003c/tool_call\u003e\n   ```\n\n   \u003e Note that the brackets are incorrect, and the call does not follow the `name, argument` format.\n\n4. Your code parses the `chat.completion` response to check for tool calls from the model, then calls the appropriate tools with the parameters specified by the model. Your code then adds both:\n\n   1. The model's tool call message\n   2. The result of the tool call\n\n   To the `messages` array to send back to the model\n\n   ```python\n   # pseudocode, see examples for copy-paste snippets\n   if response.has_tool_calls:\n       for each tool_call:\n           # Extract function name \u0026 args\n           function_to_call = tool_call.name     # e.g. \"get_delivery_date\"\n           args = tool_call.arguments            # e.g. {\"order_id\": \"123\"}\n\n           # Execute the function\n           result = execute_function(function_to_call, args)\n\n           # Add result to conversation\n           add_to_messages([\n               ASSISTANT_TOOL_CALL_MESSAGE,      # The request to use the tool\n               TOOL_RESULT_MESSAGE               # The tool's response\n           ])\n   else:\n       # Normal response without tools\n       add_to_messages(response.content)\n   ```\n\n5. The LLM is then prompted again with the updated messages array, but without access to tools. This is because:\n   - The LLM already has the tool results in the conversation history\n   - We want the LLM to provide a final response to the user, not call more tools\n   ```python\n   # Example messages\n   messages = [\n       {\"role\": \"user\", \"content\": \"When will order 123 be delivered?\"},\n       {\"role\": \"assistant\", \"function_call\": {\n           \"name\": \"get_delivery_date\",\n           \"arguments\": {\"order_id\": \"123\"}\n       }},\n       {\"role\": \"tool\", \"content\": \"2024-03-15\"},\n   ]\n   response = client.chat.completions.create(\n       model=\"lmstudio-community/qwen2.5-7b-instruct\",\n       messages=messages\n   )\n   ```\n   The `response.choices[0].message.content` field after this call may be something like:\n   ```xml\n   Your order #123 will be delivered on March 15th, 2024\n   ```\n6. The loop continues back at step 2 of the flow\n\nNote: This is the `pedantic` flow for tool use. However, you can certainly experiment with this flow to best fit your use case.\n\n## Supported Models\n\nThrough LM Studio, **all** models support at least some degree of tool use.\n\nHowever, there are currently two levels of support that may impact the quality of the experience: Native and Default.\n\nModels with Native tool use support will have a hammer badge in the app, and generally perform better in tool use scenarios.\n\n### Native tool use support\n\n\"Native\" tool use support means that both:\n\n1. The model has a chat template that supports tool use (usually means the model has been trained for tool use)\n   - This is what will be used to format the `tools` array into the system prompt and tell them model how to format tool calls\n   - Example: [Qwen2.5-Instruct chat template](https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit/blob/c26a38f6a37d0a51b4e9a1eb3026530fa35d9fed/tokenizer_config.json#L197)\n2. LM Studio supports that model's tool use format\n   - Required for LM Studio to properly input the chat history into the chat template, and parse the tool calls the model outputs into the `chat.completion` object\n\nModels that currently have native tool use support in LM Studio (subject to change):\n\n- Qwen\n  - `GGUF` [lmstudio-community/Qwen2.5-7B-Instruct-GGUF](https://model.lmstudio.ai/download/lmstudio-community/Qwen2.5-7B-Instruct-GGUF) (4.68 GB)\n  - `MLX` [mlx-community/Qwen2.5-7B-Instruct-4bit](https://model.lmstudio.ai/download/mlx-community/Qwen2.5-7B-Instruct-4bit) (4.30 GB)\n- Llama-3.1, Llama-3.2\n  - `GGUF` [lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF](https://model.lmstudio.ai/download/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF) (4.92 GB)\n  - `MLX` [mlx-community/Meta-Llama-3.1-8B-Instruct-8bit](https://model.lmstudio.ai/download/mlx-community/Meta-Llama-3.1-8B-Instruct-8bit) (8.54 GB)\n- Mistral\n  - `GGUF` [bartowski/Ministral-8B-Instruct-2410-GGUF](https://model.lmstudio.ai/download/bartowski/Ministral-8B-Instruct-2410-GGUF) (4.67 GB)\n  - `MLX` [mlx-community/Ministral-8B-Instruct-2410-4bit](https://model.lmstudio.ai/download/mlx-community/Ministral-8B-Instruct-2410-4bit) (4.67 GB GB)\n\n### Default tool use support\n\n\"Default\" tool use support means that **either**:\n\n1. The model does not have chat template that supports tool use (usually means the model has not been trained for tool use)\n2. LM Studio does not currently support that model's tool use format\n\nUnder the hood, default tool use works by:\n\n- Giving models a custom system prompt and a default tool call format to use\n- Converting `tool` role messages to the `user` role so that chat templates without the `tool` role are compatible\n- Converting `assistant` role `tool_calls` into the default tool call format\n\nResults will vary by model.\n\nYou can see the default format by running `lms log stream` in your terminal, then sending a chat completion request with `tools` to a model that doesn't have Native tool use support. The default format is subject to change.\n\n\u003cdetails\u003e\n\u003csummary\u003eExpand to see example of default tool use format\u003c/summary\u003e\n\n```bash\n-\u003e % lms log stream\nStreaming logs from LM Studio\n\ntimestamp: 11/13/2024, 9:35:15 AM\ntype: llm.prediction.input\nmodelIdentifier: gemma-2-2b-it\nmodelPath: lmstudio-community/gemma-2-2b-it-GGUF/gemma-2-2b-it-Q4_K_M.gguf\ninput: \"\u003cstart_of_turn\u003esystem\nYou are a tool-calling AI. You can request calls to available tools with this EXACT format:\n[TOOL_REQUEST]{\"name\": \"tool_name\", \"arguments\": {\"param1\": \"value1\"}}[END_TOOL_REQUEST]\n\nAVAILABLE TOOLS:\n{\n  \"type\": \"toolArray\",\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_delivery_date\",\n        \"description\": \"Get the delivery date for a customer's order\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"order_id\": {\n              \"type\": \"string\"\n            }\n          },\n          \"required\": [\n            \"order_id\"\n          ]\n        }\n      }\n    }\n  ]\n}\n\nRULES:\n- Only use tools from AVAILABLE TOOLS\n- Include all required arguments\n- Use one [TOOL_REQUEST] block per tool\n- Never use [TOOL_RESULT]\n- If you decide to call one or more tools, there should be no other text in your message\n\nExamples:\n\"Check Paris weather\"\n[TOOL_REQUEST]{\"name\": \"get_weather\", \"arguments\": {\"location\": \"Paris\"}}[END_TOOL_REQUEST]\n\n\"Send email to John about meeting and open browser\"\n[TOOL_REQUEST]{\"name\": \"send_email\", \"arguments\": {\"to\": \"John\", \"subject\": \"meeting\"}}[END_TOOL_REQUEST]\n[TOOL_REQUEST]{\"name\": \"open_browser\", \"arguments\": {}}[END_TOOL_REQUEST]\n\nRespond conversationally if no matching tools exist.\u003cend_of_turn\u003e\n\u003cstart_of_turn\u003euser\nGet me delivery date for order 123\u003cend_of_turn\u003e\n\u003cstart_of_turn\u003emodel\n\"\n```\n\nIf the model follows this format exactly to call tools, i.e:\n\n```\n[TOOL_REQUEST]{\"name\": \"get_delivery_date\", \"arguments\": {\"order_id\": \"123\"}}[END_TOOL_REQUEST]\n```\n\nThen LM Studio will be able to parse those tool calls into the `chat.completions` object, just like for natively supported models.\n\n\u003c/details\u003e\n\nAll models that don't have native tool use support will have default tool use support.\n\n## Example using `curl`\n\nThis example demonstrates a model requesting a tool call using the `curl` utility.\n\nTo run this example on Mac or Linux, use any terminal. On Windows, use [Git Bash](https://git-scm.com/download/win).\n\n```bash\ncurl http://localhost:1234/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"lmstudio-community/qwen2.5-7b-instruct\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What dell products do you have under $50 in electronics?\"}],\n    \"tools\": [\n      {\n        \"type\": \"function\",\n        \"function\": {\n          \"name\": \"search_products\",\n          \"description\": \"Search the product catalog by various criteria. Use this whenever a customer asks about product availability, pricing, or specifications.\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"query\": {\n                \"type\": \"string\",\n                \"description\": \"Search terms or product name\"\n              },\n              \"category\": {\n                \"type\": \"string\",\n                \"description\": \"Product category to filter by\",\n                \"enum\": [\"electronics\", \"clothing\", \"home\", \"outdoor\"]\n              },\n              \"max_price\": {\n                \"type\": \"number\",\n                \"description\": \"Maximum price in dollars\"\n              }\n            },\n            \"required\": [\"query\"],\n            \"additionalProperties\": false\n          }\n        }\n      }\n    ]\n  }'\n```\n\nAll parameters recognized by `/v1/chat/completions` will be honored, and the array of available tools should be provided in the `tools` field.\n\nIf the model decides that the user message would be best fulfilled with a tool call, an array of tool call request objects will be provided in the response field, `choices[0].message.tool_calls`.\n\nThe `finish_reason` field of the top-level response object will also be populated with `\"tool_calls\"`.\n\nAn example response to the above `curl` request will look like:\n\n```bash\n{\n  \"id\": \"chatcmpl-gb1t1uqzefudice8ntxd9i\",\n  \"object\": \"chat.completion\",\n  \"created\": 1730913210,\n  \"model\": \"lmstudio-community/qwen2.5-7b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"logprobs\": null,\n      \"finish_reason\": \"tool_calls\",\n      \"message\": {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n          {\n            \"id\": \"365174485\",\n            \"type\": \"function\",\n            \"function\": {\n              \"name\": \"search_products\",\n              \"arguments\": \"{\\\"query\\\":\\\"dell\\\",\\\"category\\\":\\\"electronics\\\",\\\"max_price\\\":50}\"\n            }\n          }\n        ]\n      }\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 263,\n    \"completion_tokens\": 34,\n    \"total_tokens\": 297\n  },\n  \"system_fingerprint\": \"lmstudio-community/qwen2.5-7b-instruct\"\n}\n```\n\nIn plain english, the above response can be thought of as the model saying:\n\n\u003e \"Please call the `search_products` function, with arguments:\n\u003e\n\u003e - 'dell' for the `query` parameter,\n\u003e - 'electronics' for the `category` parameter\n\u003e - '50' for the `max_price` parameter\n\u003e\n\u003e and give me back the results\"\n\nThe `tool_calls` field will need to be parsed to call actual functions/APIs. The below examples demonstrate how.\n\n## Examples using `python`\n\nTool use shines when paired with program languages like python, where you can implement the functions specified in the `tools` field to programmatically call them when the model requests.\n\n### Single-turn example\n\nBelow is a simple single-turn (model is only called once) example of enabling a model to call a function called `say_hello` that prints a hello greeting to the console:\n\n`single-turn-example.py`\n\n```python\nfrom openai import OpenAI\n\n# Connect to LM Studio\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\n# Define a simple function\ndef say_hello(name: str) -\u003e str:\n    print(f\"Hello, {name}!\")\n\n# Tell the AI about our function\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"say_hello\",\n            \"description\": \"Says hello to someone\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"name\": {\n                        \"type\": \"string\",\n                        \"description\": \"The person's name\"\n                    }\n                },\n                \"required\": [\"name\"]\n            }\n        }\n    }\n]\n\n# Ask the AI to use our function\nresponse = client.chat.completions.create(\n    model=\"lmstudio-community/qwen2.5-7b-instruct\",\n    messages=[{\"role\": \"user\", \"content\": \"Can you say hello to Bob the Builder?\"}],\n    tools=tools\n)\n\n# Get the name the AI wants to use a tool to say hello to\n# (Assumes the AI has requested a tool call and that tool call is say_hello)\ntool_call = response.choices[0].message.tool_calls[0]\nname = eval(tool_call.function.arguments)[\"name\"]\n\n# Actually call the say_hello function\nsay_hello(name) # Prints: Hello, Bob the Builder!\n\n```\n\nRunning this script from the console should yield results like:\n\n```xml\n-\u003e % python single-turn-example.py\nHello, Bob the Builder!\n```\n\nPlay around with the name in\n\n```python\nmessages=[{\"role\": \"user\", \"content\": \"Can you say hello to Bob the Builder?\"}]\n```\n\nto see the model call the `say_hello` function with different names.\n\n### Multi-turn example\n\nNow for a slightly more complex example.\n\nIn this example, we'll:\n\n1. Enable the model to call a `get_delivery_date` function\n2. Hand the result of calling that function back to the model, so that it can fulfill the user's request in plain text\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003ccode\u003emulti-turn-example.py\u003c/code\u003e (click to expand) \u003c/summary\u003e\n\n```python\nfrom datetime import datetime, timedelta\nimport json\nimport random\nfrom openai import OpenAI\n\n# Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\nmodel = \"lmstudio-community/qwen2.5-7b-instruct\"\n\n\ndef get_delivery_date(order_id: str) -\u003e datetime:\n    # Generate a random delivery date between today and 14 days from now\n    # in a real-world scenario, this function would query a database or API\n    today = datetime.now()\n    random_days = random.randint(1, 14)\n    delivery_date = today + timedelta(days=random_days)\n    print(\n        f\"\\nget_delivery_date function returns delivery date:\\n\\n{delivery_date}\",\n        flush=True,\n    )\n    return delivery_date\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_delivery_date\",\n            \"description\": \"Get the delivery date for a customer's order. Call this whenever you need to know the delivery date, for example when a customer asks 'Where is my package'\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"order_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The customer's order ID.\",\n                    },\n                },\n                \"required\": [\"order_id\"],\n                \"additionalProperties\": False,\n            },\n        },\n    }\n]\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful customer support assistant. Use the supplied tools to assist the user.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Give me the delivery date and time for order number 1017\",\n    },\n]\n\n# LM Studio\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    tools=tools,\n)\n\nprint(\"\\nModel response requesting tool call:\\n\", flush=True)\nprint(response, flush=True)\n\n# Extract the arguments for get_delivery_date\n# Note this code assumes we have already determined that the model generated a function call.\ntool_call = response.choices[0].message.tool_calls[0]\narguments = json.loads(tool_call.function.arguments)\n\norder_id = arguments.get(\"order_id\")\n\n# Call the get_delivery_date function with the extracted order_id\ndelivery_date = get_delivery_date(order_id)\n\nassistant_tool_call_request_message = {\n    \"role\": \"assistant\",\n    \"tool_calls\": [\n        {\n            \"id\": response.choices[0].message.tool_calls[0].id,\n            \"type\": response.choices[0].message.tool_calls[0].type,\n            \"function\": response.choices[0].message.tool_calls[0].function,\n        }\n    ],\n}\n\n# Create a message containing the result of the function call\nfunction_call_result_message = {\n    \"role\": \"tool\",\n    \"content\": json.dumps(\n        {\n            \"order_id\": order_id,\n            \"delivery_date\": delivery_date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        }\n    ),\n    \"tool_call_id\": response.choices[0].message.tool_calls[0].id,\n}\n\n# Prepare the chat completion call payload\ncompletion_messages_payload = [\n    messages[0],\n    messages[1],\n    assistant_tool_call_request_message,\n    function_call_result_message,\n]\n\n# Call the OpenAI API's chat completions endpoint to send the tool call result back to the model\n# LM Studio\nresponse = client.chat.completions.create(\n    model=model,\n    messages=completion_messages_payload,\n)\n\nprint(\"\\nFinal model response with knowledge of the tool call result:\\n\", flush=True)\nprint(response.choices[0].message.content, flush=True)\n\n```\n\n\u003c/details\u003e\n\nRunning this script from the console should yield results like:\n\n```xml\n-\u003e % python multi-turn-example.py\n\nModel response requesting tool call:\n\nChatCompletion(id='chatcmpl-wwpstqqu94go4hvclqnpwn', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='377278620', function=Function(arguments='{\"order_id\":\"1017\"}', name='get_delivery_date'), type='function')]))], created=1730916196, model='lmstudio-community/qwen2.5-7b-instruct', object='chat.completion', service_tier=None, system_fingerprint='lmstudio-community/qwen2.5-7b-instruct', usage=CompletionUsage(completion_tokens=24, prompt_tokens=223, total_tokens=247, completion_tokens_details=None, prompt_tokens_details=None))\n\nget_delivery_date function returns delivery date:\n\n2024-11-19 13:03:17.773298\n\nFinal model response with knowledge of the tool call result:\n\nYour order number 1017 is scheduled for delivery on November 19, 2024, at 13:03 PM.\n```\n\n### Advanced agent example\n\nBuilding upon the principles above, we can combine LM Studio models with locally defined functions to create an \"agent\" - a system that pairs a language model with custom functions to understand requests and perform actions beyond basic text generation.\n\nThe agent in the below example can:\n\n1. Open safe urls in your default browser\n2. Check the current time\n3. Analyze directories in your file system\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003ccode\u003eagent-chat-example.py\u003c/code\u003e (click to expand) \u003c/summary\u003e\n\n```python\nimport json\nfrom urllib.parse import urlparse\nimport webbrowser\nfrom datetime import datetime\nimport os\nfrom openai import OpenAI\n\n# Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\nmodel = \"lmstudio-community/qwen2.5-7b-instruct\"\n\n\ndef is_valid_url(url: str) -\u003e bool:\n\n    try:\n        result = urlparse(url)\n        return bool(result.netloc)  # Returns True if there's a valid network location\n    except Exception:\n        return False\n\n\ndef open_safe_url(url: str) -\u003e dict:\n    # List of allowed domains (expand as needed)\n    SAFE_DOMAINS = {\n        \"lmstudio.ai\",\n        \"github.com\",\n        \"google.com\",\n        \"wikipedia.org\",\n        \"weather.com\",\n        \"stackoverflow.com\",\n        \"python.org\",\n        \"docs.python.org\",\n    }\n\n    try:\n        # Add http:// if no scheme is present\n        if not url.startswith(('http://', 'https://')):\n            url = 'http://' + url\n\n        # Validate URL format\n        if not is_valid_url(url):\n            return {\"status\": \"error\", \"message\": f\"Invalid URL format: {url}\"}\n\n        # Parse the URL and check domain\n        parsed_url = urlparse(url)\n        domain = parsed_url.netloc.lower()\n        base_domain = \".\".join(domain.split(\".\")[-2:])\n\n        if base_domain in SAFE_DOMAINS:\n            webbrowser.open(url)\n            return {\"status\": \"success\", \"message\": f\"Opened {url} in browser\"}\n        else:\n            return {\n                \"status\": \"error\",\n                \"message\": f\"Domain {domain} not in allowed list\",\n            }\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": str(e)}\n\n\ndef get_current_time() -\u003e dict:\n    \"\"\"Get the current system time with timezone information\"\"\"\n    try:\n        current_time = datetime.now()\n        timezone = datetime.now().astimezone().tzinfo\n        formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n        return {\n            \"status\": \"success\",\n            \"time\": formatted_time,\n            \"timezone\": str(timezone),\n            \"timestamp\": current_time.timestamp(),\n        }\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": str(e)}\n\n\ndef analyze_directory(path: str = \".\") -\u003e dict:\n    \"\"\"Count and categorize files in a directory\"\"\"\n    try:\n        stats = {\n            \"total_files\": 0,\n            \"total_dirs\": 0,\n            \"file_types\": {},\n            \"total_size_bytes\": 0,\n        }\n\n        for entry in os.scandir(path):\n            if entry.is_file():\n                stats[\"total_files\"] += 1\n                ext = os.path.splitext(entry.name)[1].lower() or \"no_extension\"\n                stats[\"file_types\"][ext] = stats[\"file_types\"].get(ext, 0) + 1\n                stats[\"total_size_bytes\"] += entry.stat().st_size\n            elif entry.is_dir():\n                stats[\"total_dirs\"] += 1\n                # Add size of directory contents\n                for root, _, files in os.walk(entry.path):\n                    for file in files:\n                        try:\n                            stats[\"total_size_bytes\"] += os.path.getsize(os.path.join(root, file))\n                        except (OSError, FileNotFoundError):\n                            continue\n\n        return {\"status\": \"success\", \"stats\": stats, \"path\": os.path.abspath(path)}\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": str(e)}\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"open_safe_url\",\n            \"description\": \"Open a URL in the browser if it's deemed safe\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"url\": {\n                        \"type\": \"string\",\n                        \"description\": \"The URL to open\",\n                    },\n                },\n                \"required\": [\"url\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_time\",\n            \"description\": \"Get the current system time with timezone information\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_directory\",\n            \"description\": \"Analyze the contents of a directory, counting files and folders\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"path\": {\n                        \"type\": \"string\",\n                        \"description\": \"The directory path to analyze. Defaults to current directory if not specified.\",\n                    },\n                },\n                \"required\": [],\n            },\n        },\n    },\n]\n\n\ndef process_tool_calls(response, messages):\n    \"\"\"Process multiple tool calls and return the final response and updated messages\"\"\"\n    # Get all tool calls from the response\n    tool_calls = response.choices[0].message.tool_calls\n\n    # Create the assistant message with tool calls\n    assistant_tool_call_message = {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n            {\n                \"id\": tool_call.id,\n                \"type\": tool_call.type,\n                \"function\": tool_call.function,\n            }\n            for tool_call in tool_calls\n        ],\n    }\n\n    # Add the assistant's tool call message to the history\n    messages.append(assistant_tool_call_message)\n\n    # Process each tool call and collect results\n    tool_results = []\n    for tool_call in tool_calls:\n        # For functions with no arguments, use empty dict\n        arguments = (\n            json.loads(tool_call.function.arguments)\n            if tool_call.function.arguments.strip()\n            else {}\n        )\n\n        # Determine which function to call based on the tool call name\n        if tool_call.function.name == \"open_safe_url\":\n            result = open_safe_url(arguments[\"url\"])\n        elif tool_call.function.name == \"get_current_time\":\n            result = get_current_time()\n        elif tool_call.function.name == \"analyze_directory\":\n            path = arguments.get(\"path\", \".\")\n            result = analyze_directory(path)\n        else:\n            # llm tried to call a function that doesn't exist, skip\n            continue\n\n        # Add the result message\n        tool_result_message = {\n            \"role\": \"tool\",\n            \"content\": json.dumps(result),\n            \"tool_call_id\": tool_call.id,\n        }\n        tool_results.append(tool_result_message)\n        messages.append(tool_result_message)\n\n    # Get the final response\n    final_response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n    )\n\n    return final_response\n\n\ndef chat():\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that can open safe web links, tell the current time, and analyze directory contents. Use these capabilities whenever they might be helpful.\",\n        }\n    ]\n\n    print(\n        \"Assistant: Hello! I can help you open safe web links, tell you the current time, and analyze directory contents. What would you like me to do?\"\n    )\n    print(\"(Type 'quit' to exit)\")\n\n    while True:\n        # Get user input\n        user_input = input(\"\\nYou: \").strip()\n\n        # Check for quit command\n        if user_input.lower() == \"quit\":\n            print(\"Assistant: Goodbye!\")\n            break\n\n        # Add user message to conversation\n        messages.append({\"role\": \"user\", \"content\": user_input})\n\n        try:\n            # Get initial response\n            response = client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n            )\n\n            # Check if the response includes tool calls\n            if response.choices[0].message.tool_calls:\n                # Process all tool calls and get final response\n                final_response = process_tool_calls(response, messages)\n                print(\"\\nAssistant:\", final_response.choices[0].message.content)\n\n                # Add assistant's final response to messages\n                messages.append(\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": final_response.choices[0].message.content,\n                    }\n                )\n            else:\n                # If no tool call, just print the response\n                print(\"\\nAssistant:\", response.choices[0].message.content)\n\n                # Add assistant's response to messages\n                messages.append(\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": response.choices[0].message.content,\n                    }\n                )\n\n        except Exception as e:\n            print(f\"\\nAn error occurred: {str(e)}\")\n            exit(1)\n\n\nif __name__ == \"__main__\":\n    chat()\n\n```\n\n\u003c/details\u003e\n\nRunning this script from the console will allow you to chat with the agent:\n\n```xml\n-\u003e % python agent-example.py\nAssistant: Hello! I can help you open safe web links, tell you the current time, and analyze directory contents. What would you like me to do?\n(Type 'quit' to exit)\n\nYou: What time is it?\n\nAssistant: The current time is 14:11:40 (EST) as of November 6, 2024.\n\nYou: What time is it now?\n\nAssistant: The current time is 14:13:59 (EST) as of November 6, 2024.\n\nYou: Open lmstudio.ai\n\nAssistant: The link to lmstudio.ai has been opened in your default web browser.\n\nYou: What's in my current directory?\n\nAssistant: Your current directory at `/Users/matt/project` contains a total of 14 files and 8 directories. Here's the breakdown:\n\n- Files without an extension: 3\n- `.mjs` files: 2\n- `.ts` (TypeScript) files: 3\n- Markdown (`md`) file: 1\n- JSON files: 4\n- TOML file: 1\n\nThe total size of these items is 1,566,990,604 bytes.\n\nYou: Thank you!\n\nAssistant: You're welcome! If you have any other questions or need further assistance, feel free to ask.\n\nYou:\n```\n\n### Streaming\n\nWhen streaming through `/v1/chat/completions` (`stream=true`), tool calls are sent in chunks. Function names and arguments are sent in pieces via `chunk.choices[0].delta.tool_calls.function.name` and `chunk.choices[0].delta.tool_calls.function.arguments`.\n\nFor example, to call `get_current_weather(location=\"San Francisco\")`, the streamed `ChoiceDeltaToolCall` in each `chunk.choices[0].delta.tool_calls[0]` object will look like:\n\n```py\nChoiceDeltaToolCall(index=0, id='814890118', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_weather'), type='function')\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type=None)\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='location', name=None), type=None)\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":\"', name=None), type=None)\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='San Francisco', name=None), type=None)\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type=None)\n```\n\nThese chunks must be accumulated throughout the stream to form the complete function signature for execution.\n\nThe below example shows how to create a simple tool-enhanced chatbot through the `/v1/chat/completions` streaming endpoint (`stream=true`).\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003ccode\u003etool-streaming-chatbot.py\u003c/code\u003e (click to expand) \u003c/summary\u003e\n\n```python\nfrom openai import OpenAI\nimport time\n\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\nMODEL = \"lmstudio-community/qwen2.5-7b-instruct\"\n\nTIME_TOOL = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_time\",\n        \"description\": \"Get the current time, only if asked\",\n        \"parameters\": {\"type\": \"object\", \"properties\": {}},\n    },\n}\n\ndef get_current_time():\n    return {\"time\": time.strftime(\"%H:%M:%S\")}\n\ndef process_stream(stream, add_assistant_label=True):\n    \"\"\"Handle streaming responses from the API\"\"\"\n    collected_text = \"\"\n    tool_calls = []\n    first_chunk = True\n\n    for chunk in stream:\n        delta = chunk.choices[0].delta\n\n        # Handle regular text output\n        if delta.content:\n            if first_chunk:\n                print()\n                if add_assistant_label:\n                    print(\"Assistant:\", end=\" \", flush=True)\n                first_chunk = False\n            print(delta.content, end=\"\", flush=True)\n            collected_text += delta.content\n\n        # Handle tool calls\n        elif delta.tool_calls:\n            for tc in delta.tool_calls:\n                if len(tool_calls) \u003c= tc.index:\n                    tool_calls.append({\n                        \"id\": \"\", \"type\": \"function\",\n                        \"function\": {\"name\": \"\", \"arguments\": \"\"}\n                    })\n                tool_calls[tc.index] = {\n                    \"id\": (tool_calls[tc.index][\"id\"] + (tc.id or \"\")),\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": (tool_calls[tc.index][\"function\"][\"name\"] + (tc.function.name or \"\")),\n                        \"arguments\": (tool_calls[tc.index][\"function\"][\"arguments\"] + (tc.function.arguments or \"\"))\n                    }\n                }\n    return collected_text, tool_calls\n\ndef chat_loop():\n    messages = []\n    print(\"Assistant: Hi! I am an AI agent empowered with the ability to tell the current time (Type 'quit' to exit)\")\n\n    while True:\n        user_input = input(\"\\nYou: \").strip()\n        if user_input.lower() == \"quit\":\n            break\n\n        messages.append({\"role\": \"user\", \"content\": user_input})\n\n        # Get initial response\n        response_text, tool_calls = process_stream(\n            client.chat.completions.create(\n                model=MODEL,\n                messages=messages,\n                tools=[TIME_TOOL],\n                stream=True,\n                temperature=0.2\n            )\n        )\n\n        if not tool_calls:\n            print()\n\n        text_in_first_response = len(response_text) \u003e 0\n        if text_in_first_response:\n            messages.append({\"role\": \"assistant\", \"content\": response_text})\n\n        # Handle tool calls if any\n        if tool_calls:\n            tool_name = tool_calls[0][\"function\"][\"name\"]\n            print()\n            if not text_in_first_response:\n                print(\"Assistant:\", end=\" \", flush=True)\n            print(f\"**Calling Tool: {tool_name}**\")\n            messages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})\n\n            # Execute tool calls\n            for tool_call in tool_calls:\n                if tool_call[\"function\"][\"name\"] == \"get_current_time\":\n                    result = get_current_time()\n                    messages.append({\n                        \"role\": \"tool\",\n                        \"content\": str(result),\n                        \"tool_call_id\": tool_call[\"id\"]\n                    })\n\n            # Get final response after tool execution\n            final_response, _ = process_stream(\n                client.chat.completions.create(\n                    model=MODEL,\n                    messages=messages,\n                    stream=True\n                ),\n                add_assistant_label=False\n            )\n\n            if final_response:\n                print()\n                messages.append({\"role\": \"assistant\", \"content\": final_response})\n\nif __name__ == \"__main__\":\n    chat_loop()\n```\n\n\u003c/details\u003e\n\nYou can chat with the bot by running this script from the console:\n\n```xml\n-\u003e % python tool-streaming-chatbot.py\nAssistant: Hi! I am an AI agent empowered with the ability to tell the current time (Type 'quit' to exit)\n\nYou: Tell me a joke, then tell me the current time\n\nAssistant: Sure! Here's a light joke for you: Why don't scientists trust atoms? Because they make up everything.\n\nNow, let me get the current time for you.\n\n**Calling Tool: get_current_time**\n\nThe current time is 18:49:31. Enjoy your day!\n\nYou:\n```\n\n## Community\n\nChat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).\n"])</script><script>self.__next_f.push([1,"de:{\"metadata\":\"$df\",\"prettyName\":\"Tools and Function Calling\",\"content\":\"$e0\",\"pageRelUrl\":\"1_developer/3_openai-compat/tools.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"}\ne3:{\"method\":\"GET\"}\ne2:{\"title\":\"List Models\",\"description\":\"List available models via the OpenAI-compatible endpoint.\",\"index\":3,\"apiInfo\":\"$e3\"}\ne1:{\"metadata\":\"$e2\",\"prettyName\":\"List Models\",\"content\":\"\\n- Method: `GET`\\n- Returns the models visible to the server. The list may include all downloaded models when Justâ€‘Inâ€‘Time loading is enabled.\\n\\n##### cURL\\n\\n```bash\\ncurl http://localhost:1234/v1/models\\n```\\n\",\"pageRelUrl\":\"1_developer/3_openai-compat/models.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"}\ne6:{\"method\":\"POST\"}\ne5:{\"title\":\"Responses\",\"description\":\"Create responses with support for streaming, reasoning, prior response state, and optional Remote MCP tools.\",\"index\":3,\"apiInfo\":\"$e6\"}\ne7:T5f4,\n- Method: `POST`\n- See OpenAI docs: https://platform.openai.com/docs/api-reference/responses\n\n##### cURL (nonâ€‘streaming)\n\n```bash\ncurl http://localhost:1234/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai/gpt-oss-20b\",\n    \"input\": \"Provide a prime number less than 50\",\n    \"reasoning\": { \"effort\": \"low\" }\n  }'\n```\n\n##### Stateful followâ€‘up\n\nUse the `id` from a previous response as `previous_response_id`.\n\n```bash\ncurl http://localhost:1234/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai/gpt-oss-20b\",\n    \"input\": \"Multiply it by 2\",\n    \"previous_response_id\": \"resp_123\"\n  }'\n```\n\n##### Streaming\n\n```bash\ncurl http://localhost:1234/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai/gpt-oss-20b\",\n    \"input\": \"Hello\",\n    \"stream\": true\n  }'\n```\n\nYou will receive SSE events such as `response.created`, `response.output_text.delta`, and `response.completed`.\n\n##### Tools and Remote MCP (optâ€‘in)\n\nEnable Remote MCP in the app (Developer â†’ Settings). "])</script><script>self.__next_f.push([1,"Example payload using an MCP server tool:\n\n```bash\ncurl http://localhost:1234/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai/gpt-oss-20b\",\n    \"tools\": [{\n      \"type\": \"mcp\",\n      \"server_label\": \"tiktoken\",\n      \"server_url\": \"https://gitmcp.io/openai/tiktoken\",\n      \"allowed_tools\": [\"fetch_tiktoken_documentation\"]\n    }],\n    \"input\": \"What is the first sentence of the tiktoken documentation?\"\n  }'\n```\ne4:{\"metadata\":\"$e5\",\"prettyName\":\"Responses\",\"content\":\"$e7\",\"pageRelUrl\":\"1_developer/3_openai-compat/responses.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"}\nea:{\"method\":\"POST\"}\ne9:{\"title\":\"Chat Completions\",\"description\":\"Send a chat history and get the assistant's response.\",\"index\":4,\"apiInfo\":\"$ea\"}\ne8:{\"metadata\":\"$e9\",\"prettyName\":\"Chat Completions\",\"content\":\"\\n- Method: `POST`\\n- Prompt template is applied automatically for chatâ€‘tuned models\\n- Provide inference parameters (temperature, top_p, etc.) in the payload\\n- See OpenAI docs: https://platform.openai.com/docs/api-reference/chat\\n- Tip: keep a terminal open with [`lms log stream`](/docs/cli/serve/log-stream) to inspect model input\\n\\n##### Python example\\n\\n```python\\nfrom openai import OpenAI\\nclient = OpenAI(base_url=\\\"http://localhost:1234/v1\\\", api_key=\\\"lm-studio\\\")\\n\\ncompletion = client.chat.completions.create(\\n  model=\\\"model-identifier\\\",\\n  messages=[\\n    {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"Always answer in rhymes.\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Introduce yourself.\\\"}\\n  ],\\n  temperature=0.7,\\n)\\n\\nprint(completion.choices[0].message)\\n```\\n\\n### Supported payload parameters\\n\\nSee https://platform.openai.com/docs/api-reference/chat/create for parameter semantics.\\n\\n```py\\nmodel\\ntop_p\\ntop_k\\nmessages\\ntemperature\\nmax_tokens\\nstream\\nstop\\npresence_penalty\\nfrequency_penalty\\nlogit_bias\\nrepeat_penalty\\nseed\\n```\\n\",\"pageRelUrl\":\"1_developer/3_openai-compat/chat-completions.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI"])</script><script>self.__next_f.push([1," Compatible Endpoints\"}\ned:{\"method\":\"POST\"}\nec:{\"title\":\"Embeddings\",\"description\":\"Generate embedding vectors from input text.\",\"index\":5,\"apiInfo\":\"$ed\"}\neb:{\"metadata\":\"$ec\",\"prettyName\":\"Embeddings\",\"content\":\"\\n- Method: `POST`\\n- See OpenAI docs: https://platform.openai.com/docs/api-reference/embeddings\\n\\n##### Python example\\n\\n```python\\nfrom openai import OpenAI\\nclient = OpenAI(base_url=\\\"http://localhost:1234/v1\\\", api_key=\\\"lm-studio\\\")\\n\\ndef get_embedding(text, model=\\\"model-identifier\\\"):\\n   text = text.replace(\\\"\\\\n\\\", \\\" \\\")\\n   return client.embeddings.create(input=[text], model=model).data[0].embedding\\n\\nprint(get_embedding(\\\"Once upon a time, there was a cat.\\\"))\\n```\\n\",\"pageRelUrl\":\"1_developer/3_openai-compat/embeddings.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"}\nf0:{\"method\":\"POST\"}\nef:{\"title\":\"Completions (Legacy)\",\"description\":\"Text completion for base models (legacy OpenAI endpoint).\",\"index\":6,\"apiInfo\":\"$f0\"}\nee:{\"metadata\":\"$ef\",\"prettyName\":\"Completions (Legacy)\",\"content\":\"\\n```lms_warning\\nThis endpoint is no longer supported by OpenAI. LM Studio continues to support it.\\n\\nUsing this endpoint with chatâ€‘tuned models may produce unexpected tokens. Prefer base models.\\n```\\n\\n- Method: `POST`\\n- Prompt template is not applied\\n- See OpenAI docs: https://platform.openai.com/docs/api-reference/completions\\n\",\"pageRelUrl\":\"1_developer/3_openai-compat/completions.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"}\nd7:{\"\":\"$d8\",\"structured-output\":\"$db\",\"tools\":\"$de\",\"models\":\"$e1\",\"responses\":\"$e4\",\"chat-completions\":\"$e8\",\"embeddings\":\"$eb\",\"completions\":\"$ee\"}\nbc:{\"\":\"$bd\",\"api-changelog\":\"$c0\",\"core\":\"$c3\",\"rest\":\"$d3\",\"openai-compat\":\"$d7\"}\nf3:{\"title\":\"`lmstudio-python` (Python SDK)\",\"description\":\"Getting started with LM Studio's Python SDK\",\"index\":1}\nf4:T13ca,"])</script><script>self.__next_f.push([1,"\n`lmstudio-python` provides you a set APIs to interact with LLMs, embeddings models, and agentic flows.\n\n## Installing the SDK\n\n`lmstudio-python` is available as a PyPI package. You can install it using pip.\n\n```lms_code_snippet\n  variants:\n    pip:\n      language: bash\n      code: |\n        pip install lmstudio\n```\n\nFor the source code and open source contribution, visit [lmstudio-python](https://github.com/lmstudio-ai/lmstudio-python) on GitHub.\n\n## Features\n\n- Use LLMs to [respond in chats](./python/llm-prediction/chat-completion) or predict [text completions](./python/llm-prediction/completion)\n- Define functions as tools, and turn LLMs into [autonomous agents](./python/agent) that run completely locally\n- [Load](./python/manage-models/loading), [configure](./python/llm-prediction/parameters), and [unload](./python/manage-models/loading) models from memory\n- Generate embeddings for text, and more!\n\n## Quick Example: Chat with a Llama Model\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen/qwen3-4b-2507\")\n        result = model.respond(\"What is the meaning of life?\")\n\n        print(result)\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\"qwen/qwen3-4b-2507\")\n            result = model.respond(\"What is the meaning of life?\")\n\n            print(result)\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\"qwen/qwen3-4b-2507\")\n            result = await model.respond(\"What is the meaning of life?\")\n\n            print(result)\n```\n\n### Getting Local Models\n\nThe above code requires the [qwen3-4b-2507](https://lmstudio.ai/models/qwen/qwen3-4b-2507) model.\nIf you don't have the model, run the following command in the terminal to download it.\n\n```bash\nlms get qwen/qwen3-4b-2507\n```\n\nRead more about `lms get` in LM Studio's CLI [here](./cli/get).\n\n# Interactive Convenience, Deterministic Resource Management, or Structured Concurrency?\n\nAs shown in the example above, there are three distinct approaches for working\nwith the LM Studio Python SDK.\n\nThe first is the interactive convenience API (listed as \"Python (convenience API)\"\nin examples), which focuses on the use of a default LM Studio client instance for\nconvenient interactions at a synchronous Python prompt, or when using Jupyter notebooks.\n\nThe second is a synchronous scoped resource API (listed as \"Python (scoped resource API)\"\nin examples), which uses context managers to ensure that allocated resources\n(such as network connections) are released deterministically, rather than\npotentially remaining open until the entire process is terminated.\n\nThe last is an asynchronous structured concurrency API (listed as \"Python (asynchronous API)\" in\nexamples), which is designed for use in asynchronous programs that follow the design principles of\n[\"structured concurrency\"](https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/)\nin order to ensure the background tasks handling the SDK's connections to the API server host\nare managed correctly. Asynchronous applications which do not adhere to those design principles\nwill need to rely on threaded access to the synchronous scoped resource API rather than attempting\nto use the SDK's native asynchronous API. Python SDK version 1.5.0 is the first version to fully\nsupport the asynchronous API.\n\nSome examples are common between the interactive convenience API and the synchronous scoped\nresource API. These examples are listed as \"Python (synchronous API)\".\n\n## Timeouts in the synchronous API\n\n_Required Python SDK version_: **1.5.0**\n\nStarting in Python SDK version 1.5.0, the synchronous API defaults to timing out after 60 seconds\nwith no activity when waiting for a response or streaming event notification from the API server.\n\nThe number of seconds to wait for responses and event notifications can be adjusted using the\n`lmstudio.set_sync_api_timeout()` function. Setting the timeout to `None` disables the timeout\nentirely (restoring the behaviour of previous SDK versions).\n\nThe current synchronous API timeout can be queried using the `lmstudio.get_sync_api_timeout()`\nfunction.\n\n## Timeouts in the asynchronous API\n\n_Required Python SDK version_: **1.5.0**\n\nAs asynchronous coroutines support cancellation, there is no specific timeout support implemented\nin the asynchronous API. Instead, general purpose async timeout mechanisms, such as\n[`asyncio.wait_for()`](https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for) or\n[`anyio.move_on_after()`](https://anyio.readthedocs.io/en/stable/cancellation.html#timeouts),\nshould be used.\n"])</script><script>self.__next_f.push([1,"f2:{\"metadata\":\"$f3\",\"prettyName\":\"Introduction\",\"content\":\"$f4\",\"pageRelUrl\":\"1_python/index.md\"}\nf7:{\"title\":\"Project Setup\",\"description\":\"Set up your `lmstudio-python` app or script.\",\"index\":2}\nf8:T16fc,"])</script><script>self.__next_f.push([1,"\n`lmstudio` is a library published on PyPI that allows you to use `lmstudio-python` in your own projects.\nIt is open source and developed on GitHub.\nYou can find the source code [here](https://github.com/lmstudio-ai/lmstudio-python).\n\n## Installing `lmstudio-python`\n\nAs it is published to PyPI, `lmstudio-python` may be installed using `pip`\nor your preferred project dependency manager (`pdm` and `uv` are shown, but other\nPython project management tools offer similar dependency addition commands).\n\n```lms_code_snippet\n  variants:\n    pip:\n      language: bash\n      code: |\n        pip install lmstudio\n    pdm:\n      language: bash\n      code: |\n        pdm add lmstudio\n    uv:\n      language: bash\n      code: |\n        uv add lmstudio\n```\n\n## Customizing the server API host and TCP port\n\nAll of the examples in the documentation assume that the server API is running locally\non one of the default application ports (Note: in Python SDK versions prior to 1.5.0, the\nSDK also required that the optional HTTP REST server be enabled).\n\nThe network location of the server API can be overridden by\npassing a `\"host:port\"` string when creating the client instance.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n        SERVER_API_HOST = \"localhost:1234\"\n\n        # This must be the *first* convenience API interaction (otherwise the SDK\n        # implicitly creates a client that accesses the default server API host)\n        lms.configure_default_client(SERVER_API_HOST)\n\n        # Note: the dedicated configuration API was added in lmstudio-python 1.3.0\n        # For compatibility with earlier SDK versions, it is still possible to use\n        # lms.get_default_client(SERVER_API_HOST) to configure the default client\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n        SERVER_API_HOST = \"localhost:1234\"\n\n        # When using the scoped resource API, each client instance\n        # can be configured to use a specific server API host\n        with lms.Client(SERVER_API_HOST) as client:\n            model = client.llm.model()\n\n            for fragment in model.respond_stream(\"What is the meaning of life?\"):\n                print(fragment.content, end=\"\", flush=True)\n            print() # Advance to a new line at the end of the response\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n        SERVER_API_HOST = \"localhost:1234\"\n\n        # When using the asynchronous API, each client instance\n        # can be configured to use a specific server API host\n        async with lms.AsyncClient(SERVER_API_HOST) as client:\n            model = await client.llm.model()\n\n            for fragment in await model.respond_stream(\"What is the meaning of life?\"):\n                print(fragment.content, end=\"\", flush=True)\n            print() # Advance to a new line at the end of the response\n```\n\n### Checking a specified API server host is running\n\n*Required Python SDK version*: **1.5.0**\n\nWhile the most common connection pattern is to let the SDK raise an exception if it can't\nconnect to the specified API server host, the SDK also supports running the API check directly\nwithout creating an SDK client instance first:\n\n```lms_code_snippet\n  variants:\n    \"Python (synchronous API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n        SERVER_API_HOST = \"localhost:1234\"\n\n        if lms.Client.is_valid_api_host(SERVER_API_HOST):\n            print(f\"An LM Studio API server instance is available at {SERVER_API_HOST}\")\n        else:\n            print(\"No LM Studio API server instance found at {SERVER_API_HOST}\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n        SERVER_API_HOST = \"localhost:1234\"\n\n        if await lms.AsyncClient.is_valid_api_host(SERVER_API_HOST):\n            print(f\"An LM Studio API server instance is available at {SERVER_API_HOST}\")\n        else:\n            print(\"No LM Studio API server instance found at {SERVER_API_HOST}\")\n```\n\n\n### Determining the default local API server port\n\n*Required Python SDK version*: **1.5.0**\n\nWhen no API server host is specified, the SDK queries a number of ports on the local loopback\ninterface for a running API server instance. This scan is repeated for each new client instance\ncreated. Rather than letting the SDK perform this scan implicitly, the SDK also supports running\nthe scan explicitly, and passing in the reported API server details when creating clients:\n\n```lms_code_snippet\n  variants:\n    \"Python (synchronous API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        api_host = lms.Client.find_default_local_api_host()\n        if api_host is not None:\n            print(f\"An LM Studio API server instance is available at {api_host}\")\n          else:\n            print(\"No LM Studio API server instance found on any of the default local ports\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        api_host = await lms.AsyncClient.find_default_local_api_host()\n        if api_host is not None:\n            print(f\"An LM Studio API server instance is available at {api_host}\")\n          else:\n            print(\"No LM Studio API server instance found on any of the default local ports\")\n```\n"])</script><script>self.__next_f.push([1,"f6:{\"metadata\":\"$f7\",\"prettyName\":\"Project Setup\",\"content\":\"$f8\",\"pageRelUrl\":\"1_python/1_getting-started/project-setup.md\",\"sectionKey\":\"getting-started\",\"sectionPrettyName\":\"Getting Started\"}\nfa:{\"title\":\"Using `lmstudio-python` in REPL\",\"description\":\"You can use `lmstudio-python` in REPL (Read-Eval-Print Loop) to interact with LLMs, manage models, and more.\",\"index\":2}\nfb:Tec1,"])</script><script>self.__next_f.push([1,"\nTo simplify interactive use, `lmstudio-python` offers a convenience API which manages\nits resources via `atexit` hooks, allowing a default synchronous client session\nto be used across multiple interactive commands.\n\nThis convenience API is shown in the examples throughout the documentation as the\n`Python (convenience API)` tab (alongside the `Python (scoped resource API)` examples,\nwhich use `with` statements to ensure deterministic cleanup of network communication\nresources).\n\nThe convenience API allows the standard Python REPL, or more flexible alternatives like\nJuypter Notebooks, to be used to interact with AI models loaded into LM Studio. For\nexample:\n\n```lms_code_snippet\n  title: \"Python REPL\"\n  variants:\n    \"Interactive chat session\":\n      language: python\n      code: |\n        \u003e\u003e\u003e import lmstudio as lms\n        \u003e\u003e\u003e loaded_models = lms.list_loaded_models()\n        \u003e\u003e\u003e for idx, model in enumerate(loaded_models):\n        ...     print(f\"{idx:\u003e3} {model}\")\n        ...\n          0 LLM(identifier='qwen2.5-7b-instruct')\n        \u003e\u003e\u003e model = loaded_models[0]\n        \u003e\u003e\u003e chat = lms.Chat(\"You answer questions concisely\")\n        \u003e\u003e\u003e chat = lms.Chat(\"You answer questions concisely\")\n        \u003e\u003e\u003e chat.add_user_message(\"Tell me three fruits\")\n        UserMessage(content=[TextData(text='Tell me three fruits')])\n        \u003e\u003e\u003e print(model.respond(chat, on_message=chat.append))\n        Banana, apple, orange.\n        \u003e\u003e\u003e chat.add_user_message(\"Tell me three more fruits\")\n        UserMessage(content=[TextData(text='Tell me three more fruits')])\n        \u003e\u003e\u003e print(model.respond(chat, on_message=chat.append))\n        Mango, strawberry, avocado.\n        \u003e\u003e\u003e chat.add_user_message(\"How many fruits have you told me?\")\n        UserMessage(content=[TextData(text='How many fruits have you told me?')])\n        \u003e\u003e\u003e print(model.respond(chat, on_message=chat.append))\n        You asked for three initial fruits and three more, so I've listed a total of six fruits.\n\n```\n\nWhile not primarily intended for use this way, the SDK's asynchronous structured concurrency API\nis compatible with the asynchronous Python REPL that is launched by `python -m asyncio`.\nFor example:\n\n```lms_code_snippet\n  title: \"Python REPL\"\n  variants:\n    \"Asynchronous chat session\":\n      language: python\n      code: |\n        # Note: assumes use of the \"python -m asyncio\" asynchronous REPL (or equivalent)\n        # Requires Python SDK version 1.5.0 or later\n        \u003e\u003e\u003e from contextlib import AsyncExitStack\n        \u003e\u003e\u003e import lmstudio as lms\n        \u003e\u003e\u003e resources = AsyncExitStack()\n        \u003e\u003e\u003e client = await resources.enter_async_context(lms.AsyncClient())\n        \u003e\u003e\u003e loaded_models = await client.llm.list_loaded()\n        \u003e\u003e\u003e for idx, model in enumerate(loaded_models):\n        ...     print(f\"{idx:\u003e3} {model}\")\n        ...\n          0 AsyncLLM(identifier='qwen2.5-7b-instruct-1m')\n        \u003e\u003e\u003e model = loaded_models[0]\n        \u003e\u003e\u003e chat = lms.Chat(\"You answer questions concisely\")\n        \u003e\u003e\u003e chat.add_user_message(\"Tell me three fruits\")\n        UserMessage(content=[TextData(text='Tell me three fruits')])\n        \u003e\u003e\u003e print(await model.respond(chat, on_message=chat.append))\n        Apple, banana, and orange.\n        \u003e\u003e\u003e chat.add_user_message(\"Tell me three more fruits\")\n        UserMessage(content=[TextData(text='Tell me three more fruits')])\n        \u003e\u003e\u003e print(await model.respond(chat, on_message=chat.append))\n        Mango, strawberry, and pineapple.\n        \u003e\u003e\u003e chat.add_user_message(\"How many fruits have you told me?\")\n        UserMessage(content=[TextData(text='How many fruits have you told me?')])\n        \u003e\u003e\u003e print(await model.respond(chat, on_message=chat.append))\n        You asked for three fruits initially, then three more, so I've listed six fruits in total.\n\n```\n"])</script><script>self.__next_f.push([1,"f9:{\"metadata\":\"$fa\",\"prettyName\":\"REPL Usage\",\"content\":\"$fb\",\"pageRelUrl\":\"1_python/1_getting-started/repl.md\",\"sectionKey\":\"getting-started\",\"sectionPrettyName\":\"Getting Started\"}\nf5:{\"project-setup\":\"$f6\",\"repl\":\"$f9\"}\nfe:{\"title\":\"Chat Completions\",\"description\":\"APIs for a multi-turn chat conversations with an LLM\",\"index\":2}\nff:T3272,"])</script><script>self.__next_f.push([1,"\nUse `llm.respond(...)` to generate completions for a chat conversation.\n\n## Quick Example: Generate a Chat Response\n\nThe following snippet shows how to obtain the AI's response to a quick chat prompt.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n        print(model.respond(\"What is the meaning of life?\"))\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n            print(model.respond(\"What is the meaning of life?\"))\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model()\n            print(await model.respond(\"What is the meaning of life?\"))\n\n```\n\n## Streaming a Chat Response\n\nThe following snippet shows how to stream the AI's response to a chat prompt,\ndisplaying text fragments as they are received (rather than waiting for the\nentire response to be generated before displaying anything).\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n        model = lms.llm()\n\n        for fragment in model.respond_stream(\"What is the meaning of life?\"):\n            print(fragment.content, end=\"\", flush=True)\n        print() # Advance to a new line at the end of the response\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n\n            for fragment in model.respond_stream(\"What is the meaning of life?\"):\n                print(fragment.content, end=\"\", flush=True)\n            print() # Advance to a new line at the end of the response\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model()\n\n            async for fragment in model.respond_stream(\"What is the meaning of life?\"):\n                print(fragment.content, end=\"\", flush=True)\n            print() # Advance to a new line at the end of the response\n\n```\n\n## Cancelling a Chat Response\n\nSee the [Cancelling a Prediction](./cancelling-predictions) section for how to cancel a prediction in progress.\n\n## Obtain a Model\n\nFirst, you need to get a model handle.\nThis can be done using the top-level `llm` convenience API,\nor the `model` method in the `llm` namespace when using the scoped resource API.\nFor example, here is how to use Qwen2.5 7B Instruct.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen2.5-7b-instruct\")\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\"qwen2.5-7b-instruct\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\"qwen2.5-7b-instruct\")\n\n```\n\nThere are other ways to get a model handle. See [Managing Models in Memory](./../manage-models/loading) for more info.\n\n## Manage Chat Context\n\nThe input to the model is referred to as the \"context\".\nConceptually, the model receives a multi-turn conversation as input,\nand it is asked to predict the assistant's response in that conversation.\n\n```lms_code_snippet\n  variants:\n    \"Constructing a Chat object\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        # Create a chat with an initial system prompt.\n        chat = lms.Chat(\"You are a resident AI philosopher.\")\n\n        # Build the chat context by adding messages of relevant types.\n        chat.add_user_message(\"What is the meaning of life?\")\n        # ... continued in next example\n\n  \"From chat history data\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        # Create a chat object from a chat history dict\n        chat = lms.Chat.from_history({\n            \"messages\": [\n                { \"role\": \"system\", \"content\": \"You are a resident AI philosopher.\" },\n                { \"role\": \"user\", \"content\": \"What is the meaning of life?\" },\n            ]\n        })\n        # ... continued in next example\n\n```\n\nSee [Working with Chats](./working-with-chats) for more information on managing chat context.\n\n\u003c!-- , and [`Chat`](./../api-reference/chat) for API reference for the `Chat` class. --\u003e\n\n## Generate a response\n\nYou can ask the LLM to predict the next response in the chat context using the `respond()` method.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming (synchronous API)\":\n      language: python\n      code: |\n        # The `chat` object is created in the previous step.\n        result = model.respond(chat)\n\n        print(result)\n\n    \"Streaming (synchronous API)\":\n      language: python\n      code: |\n        # The `chat` object is created in the previous step.\n        prediction_stream = model.respond_stream(chat)\n\n        for fragment in prediction_stream:\n            print(fragment.content, end=\"\", flush=True)\n        print() # Advance to a new line at the end of the response\n\n    \"Non-streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        # The `chat` object is created in the previous step.\n        result = await model.respond(chat)\n\n        print(result)\n\n    \"Streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        # The `chat` object is created in the previous step.\n        prediction_stream = await model.respond_stream(chat)\n\n        async for fragment in prediction_stream:\n            print(fragment.content, end=\"\", flush=True)\n        print() # Advance to a new line at the end of the response\n\n```\n\n## Customize Inferencing Parameters\n\nYou can pass in inferencing parameters via the `config` keyword parameter on `.respond()`.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming (synchronous API)\":\n      language: python\n      code: |\n        result = model.respond(chat, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \"Streaming (synchronous API)\":\n      language: python\n      code: |\n        prediction_stream = model.respond_stream(chat, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \"Non-streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        result = await model.respond(chat, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \"Streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        prediction_stream = await model.respond_stream(chat, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n```\n\nSee [Configuring the Model](./parameters) for more information on what can be configured.\n\n## Print prediction stats\n\nYou can also print prediction metadata, such as the model used for generation, number of generated\ntokens, time to first token, and stop reason.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: python\n      code: |\n        # `result` is the response from the model.\n        print(\"Model used:\", result.model_info.display_name)\n        print(\"Predicted tokens:\", result.stats.predicted_tokens_count)\n        print(\"Time to first token (seconds):\", result.stats.time_to_first_token_sec)\n        print(\"Stop reason:\", result.stats.stop_reason)\n\n    \"Streaming\":\n      language: python\n      code: |\n        # After iterating through the prediction fragments,\n        # the overall prediction result may be obtained from the stream\n        result = prediction_stream.result()\n\n        print(\"Model used:\", result.model_info.display_name)\n        print(\"Predicted tokens:\", result.stats.predicted_tokens_count)\n        print(\"Time to first token (seconds):\", result.stats.time_to_first_token_sec)\n        print(\"Stop reason:\", result.stats.stop_reason)\n\n```\n\nBoth the non-streaming and streaming result access is consistent across the synchronous and\nasynchronous APIs, as `prediction_stream.result()` is a non-blocking API that raises an exception\nif no result is available (either because the prediction is still running, or because the\nprediction request failed). Prediction streams also offer a blocking (synchronous API) or\nawaitable (asynchronous API) `prediction_stream.wait_for_result()` method that internally handles\niterating the stream to completion before returning the result.\n\n## Example: Multi-turn Chat\n\n```lms_code_snippet\n  title: \"chatbot.py\"\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n        chat = lms.Chat(\"You are a task focused AI assistant\")\n\n        while True:\n            try:\n                user_input = input(\"You (leave blank to exit): \")\n            except EOFError:\n                print()\n                break\n            if not user_input:\n                break\n            chat.add_user_message(user_input)\n            prediction_stream = model.respond_stream(\n                chat,\n                on_message=chat.append,\n            )\n            print(\"Bot: \", end=\"\", flush=True)\n            for fragment in prediction_stream:\n                print(fragment.content, end=\"\", flush=True)\n            print()\n\n```\n\n### Progress Callbacks\n\nLong prompts will often take a long time to first token, i.e. it takes the model a long time to process your prompt.\nIf you want to get updates on the progress of this process, you can provide a float callback to `respond`\nthat receives a float from 0.0-1.0 representing prompt processing progress.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        llm = lms.llm()\n\n        response = llm.respond(\n            \"What is LM Studio?\",\n            on_prompt_processing_progress = (lambda progress: print(f\"{progress*100}% complete\")),\n        )\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            llm = client.llm.model()\n\n            response = llm.respond(\n                \"What is LM Studio?\",\n                on_prompt_processing_progress = (lambda progress: print(f\"{progress*100}% complete\")),\n            )\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            llm = await client.llm.model()\n\n            response = await llm.respond(\n                \"What is LM Studio?\",\n                on_prompt_processing_progress = (lambda progress: print(f\"{progress*100}% complete\")),\n            )\n\n\n```\n\nIn addition to `on_prompt_processing_progress`, the other available progress callbacks are:\n\n- `on_first_token`: called after prompt processing is complete and the first token is being emitted.\n  Does not receive any arguments (use the streaming iteration API or `on_prediction_fragment`\n  to process tokens as they are emitted).\n- `on_prediction_fragment`: called for each prediction fragment received by the client.\n  Receives the same prediction fragments as iterating over the stream iteration API.\n- `on_message`: called with an assistant response message when the prediction is complete.\n  Intended for appending received messages to a chat history instance.\n"])</script><script>self.__next_f.push([1,"fd:{\"metadata\":\"$fe\",\"prettyName\":\"Chat\",\"content\":\"$ff\",\"pageRelUrl\":\"1_python/1_llm-prediction/chat-completion.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n101:{\"title\":\"Image Input\",\"description\":\"API for passing images as input to the model\",\"index\":2}\n102:T1245,"])</script><script>self.__next_f.push([1,"\n*Required Python SDK version*: **1.1.0**\n\nSome models, known as VLMs (Vision-Language Models), can accept images as input. You can pass images to the model using the `.respond()` method.\n\n### Prerequisite: Get a VLM (Vision-Language Model)\n\nIf you don't yet have a VLM, you can download a model like `qwen2-vl-2b-instruct` using the following command:\n\n```bash\nlms get qwen2-vl-2b-instruct\n```\n\n## 1. Instantiate the Model\n\nConnect to LM Studio and obtain a handle to the VLM (Vision-Language Model) you want to use.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen2-vl-2b-instruct\")\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\"qwen2-vl-2b-instruct\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\"qwen2-vl-2b-instruct\")\n\n```\n\n## 2. Prepare the Image\n\nUse the `prepare_image()` function or `files` namespace method to\nget a handle to the image that can subsequently be passed to the model.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n        image_handle = lms.prepare_image(image_path)\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n            image_handle = client.files.prepare_image(image_path)\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n            image_handle = await client.files.prepare_image(image_path)\n\n```\n\nIf you only have the raw data of the image, you can supply the raw data directly as a bytes\nobject without having to write it to disk first. Due to this feature, binary filesystem\npaths are *not* supported (as they will be handled as malformed image data rather than as\nfilesystem paths).\n\nBinary IO objects are also accepted as local file inputs.\n\nThe LM Studio server supports JPEG, PNG, and WebP image formats.\n\n## 3. Pass the Image to the Model in `.respond()`\n\nGenerate a prediction by passing the image to the model in the `.respond()` method.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n        image_handle = lms.prepare_image(image_path)\n        model = lms.llm(\"qwen2-vl-2b-instruct\")\n        chat = lms.Chat()\n        chat.add_user_message(\"Describe this image please\", images=[image_handle])\n        prediction = model.respond(chat)\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n            image_handle = client.files.prepare_image(image_path)\n            model = client.llm.model(\"qwen2-vl-2b-instruct\")\n            chat = lms.Chat()\n            chat.add_user_message(\"Describe this image please\", images=[image_handle])\n            prediction = model.respond(chat)\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n            image_handle = client.files.prepare_image(image_path)\n            model = await client.llm.model(\"qwen2-vl-2b-instruct\")\n            chat = lms.Chat()\n            chat.add_user_message(\"Describe this image please\", images=[image_handle])\n            prediction = await model.respond(chat)\n\n```\n"])</script><script>self.__next_f.push([1,"100:{\"metadata\":\"$101\",\"prettyName\":\"Image Input\",\"content\":\"$102\",\"pageRelUrl\":\"1_python/1_llm-prediction/image-input.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n104:{\"title\":\"Cancelling Predictions\",\"description\":\"Stop an ongoing prediction in `lmstudio-python`\",\"index\":4}\n105:Td73,"])</script><script>self.__next_f.push([1,"\nOne benefit of using the streaming API is the ability to cancel the\nprediction request based on criteria that can't be represented using\nthe `stopStrings` or `maxPredictedTokens` configuration settings.\n\nThe following snippet illustrates cancelling the request in response\nto an application specification cancellation condition (such as polling\nan event set by another thread).\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n        model = lms.llm()\n\n        prediction_stream = model.respond_stream(\"What is the meaning of life?\")\n        cancelled = False\n        for fragment in prediction_stream:\n            if ...: # Cancellation condition will be app specific\n                cancelled = True\n                prediction_stream.cancel()\n                # Note: it is recommended to let the iteration complete,\n                # as doing so allows the partial result to be recorded.\n                # Breaking the loop *is* permitted, but means the partial result\n                # and final prediction stats won't be available to the client\n        # The stream allows the prediction result to be retrieved after iteration\n        if not cancelled:\n            print(prediction_stream.result())\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n\n            prediction_stream = model.respond_stream(\"What is the meaning of life?\")\n            cancelled = False\n            for fragment in prediction_stream:\n                if ...: # Cancellation condition will be app specific\n                    cancelled = True\n                    prediction_stream.cancel()\n                    # Note: it is recommended to let the iteration complete,\n                    # as doing so allows the partial result to be recorded.\n                    # Breaking the loop *is* permitted, but means the partial result\n                    # and final prediction stats won't be available to the client\n            # The stream allows the prediction result to be retrieved after iteration\n            if not cancelled:\n                print(prediction_stream.result())\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model()\n\n            prediction_stream = await model.respond_stream(\"What is the meaning of life?\")\n            cancelled = False\n            async for fragment in prediction_stream:\n                if ...: # Cancellation condition will be app specific\n                    cancelled = True\n                    await prediction_stream.cancel()\n                    # Note: it is recommended to let the iteration complete,\n                    # as doing so allows the partial result to be recorded.\n                    # Breaking the loop *is* permitted, but means the partial result\n                    # and final prediction stats won't be available to the client\n            # The stream allows the prediction result to be retrieved after iteration\n            if not cancelled:\n                print(prediction_stream.result())\n\n```\n"])</script><script>self.__next_f.push([1,"103:{\"metadata\":\"$104\",\"prettyName\":\"Cancelling Predictions\",\"content\":\"$105\",\"pageRelUrl\":\"1_python/1_llm-prediction/cancelling-predictions.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n107:{\"title\":\"Structured Response\",\"description\":\"Enforce a structured response from the model using Pydantic models or JSON Schema\",\"index\":4}\n108:T16f5,"])</script><script>self.__next_f.push([1,"\nYou can enforce a particular response format from an LLM by providing a JSON schema to the `.respond()` method.\nThis guarantees that the model's output conforms to the schema you provide.\n\nThe JSON schema can either be provided directly,\nor by providing an object that implements the `lmstudio.ModelSchema` protocol,\nsuch as `pydantic.BaseModel` or `lmstudio.BaseModel`.\n\nThe `lmstudio.ModelSchema` protocol is defined as follows:\n\n```python\n@runtime_checkable\nclass ModelSchema(Protocol):\n    \"\"\"Protocol for classes that provide a JSON schema for their model.\"\"\"\n\n    @classmethod\n    def model_json_schema(cls) -\u003e DictSchema:\n        \"\"\"Return a JSON schema dict describing this model.\"\"\"\n        ...\n\n```\n\nWhen a schema is provided, the prediction result's `parsed` field will contain a string-keyed dictionary that conforms\nto the given schema (for unstructured results, this field is a string field containing the same value as `content`).\n\n\n## Enforce Using a Class Based Schema Definition\n\nIf you wish the model to generate JSON that satisfies a given schema,\nit is recommended to provide a class based schema definition using a library\nsuch as [`pydantic`](https://docs.pydantic.dev/) or [`msgspec`](https://jcristharif.com/msgspec/).\n\nPydantic models natively implement the `lmstudio.ModelSchema` protocol,\nwhile `lmstudio.BaseModel` is a `msgspec.Struct` subclass that implements `.model_json_schema()` appropriately.\n\n#### Define a Class Based Schema\n\n```lms_code_snippet\n  variants:\n    \"pydantic.BaseModel\":\n      language: python\n      code: |\n        from pydantic import BaseModel\n\n        # A class based schema for a book\n        class BookSchema(BaseModel):\n            title: str\n            author: str\n            year: int\n\n    \"lmstudio.BaseModel\":\n      language: python\n      code: |\n        from lmstudio import BaseModel\n\n        # A class based schema for a book\n        class BookSchema(BaseModel):\n            title: str\n            author: str\n            year: int\n\n```\n\n#### Generate a Structured Response\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: python\n      code: |\n        result = model.respond(\"Tell me about The Hobbit\", response_format=BookSchema)\n        book = result.parsed\n\n        print(book)\n        #           ^\n        # Note that `book` is correctly typed as { title: string, author: string, year: number }\n\n    Streaming:\n      language: python\n      code: |\n        prediction_stream = model.respond_stream(\"Tell me about The Hobbit\", response_format=BookSchema)\n\n        # Optionally stream the response\n        # for fragment in prediction:\n        #   print(fragment.content, end=\"\", flush=True)\n        # print()\n        # Note that even for structured responses, the *fragment* contents are still only text\n\n        # Get the final structured result\n        result = prediction_stream.result()\n        book = result.parsed\n\n        print(book)\n        #           ^\n        # Note that `book` is correctly typed as { title: string, author: string, year: number }\n```\n\n## Enforce Using a JSON Schema\n\nYou can also enforce a structured response using a JSON schema.\n\n#### Define a JSON Schema\n\n```python\n# A JSON schema for a book\nschema = {\n  \"type\": \"object\",\n  \"properties\": {\n    \"title\": { \"type\": \"string\" },\n    \"author\": { \"type\": \"string\" },\n    \"year\": { \"type\": \"integer\" },\n  },\n  \"required\": [\"title\", \"author\", \"year\"],\n}\n```\n\n#### Generate a Structured Response\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: python\n      code: |\n        result = model.respond(\"Tell me about The Hobbit\", response_format=schema)\n        book = result.parsed\n\n        print(book)\n        #     ^\n        # Note that `book` is correctly typed as { title: string, author: string, year: number }\n\n    Streaming:\n      language: python\n      code: |\n        prediction_stream = model.respond_stream(\"Tell me about The Hobbit\", response_format=schema)\n\n        # Stream the response\n        for fragment in prediction:\n            print(fragment.content, end=\"\", flush=True)\n        print()\n        # Note that even for structured responses, the *fragment* contents are still only text\n\n        # Get the final structured result\n        result = prediction_stream.result()\n        book = result.parsed\n\n        print(book)\n        #     ^\n        # Note that `book` is correctly typed as { title: string, author: string, year: number }\n```\n\n\u003c!--\n\nTODO: Info about structured generation caveats\n\n ## Overview\n\nOnce you have [downloaded and loaded](/docs/basics/index) a large language model,\nyou can use it to respond to input through the API. This article covers getting JSON structured output, but you can also\n[request text completions](/docs/api/sdk/completion),\n[request chat responses](/docs/api/sdk/chat-completion), and\n[use a vision-language model to chat about images](/docs/api/sdk/image-input).\n\n### Usage\n\nCertain models are trained to output valid JSON data that conforms to\na user-provided schema, which can be used programmatically in applications\nthat need structured data. This structured data format is supported by both\n[`complete`](/docs/api/sdk/completion) and [`respond`](/docs/api/sdk/chat-completion)\nmethods, and relies on Pydantic in Python and Zod in TypeScript.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        const Book = z.object({\n          title: z.string(),\n          author: z.string(),\n          year: z.number().int()\n        })\n\n        const client = new LMStudioClient()\n        const llm = client.llm.model()\n\n        const response = llm.respond(\n          \"Tell me about The Hobbit.\",\n          { structured: Book },\n        )\n\n        console.log(response.content.title)\n``` --\u003e\n"])</script><script>self.__next_f.push([1,"106:{\"metadata\":\"$107\",\"prettyName\":\"Structured Response\",\"content\":\"$108\",\"pageRelUrl\":\"1_python/1_llm-prediction/structured-response.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n10a:{\"title\":\"Speculative Decoding\",\"description\":\"API to use a draft model in speculative decoding in `lmstudio-python`\",\"index\":5}\n10b:T70e,\n_Required Python SDK version_: **1.2.0**\n\nSpeculative decoding is a technique that can substantially increase the generation speed of large language models (LLMs) without reducing response quality. See [Speculative Decoding](./../../app/advanced/speculative-decoding) for more info.\n\nTo use speculative decoding in `lmstudio-python`, simply provide a `draftModel` parameter when performing the prediction. You do not need to load the draft model separately.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        main_model_key = \"qwen2.5-7b-instruct\"\n        draft_model_key = \"qwen2.5-0.5b-instruct\"\n\n        model = lms.llm(main_model_key)\n        result = model.respond(\n            \"What are the prime numbers between 0 and 100?\",\n            config={\n                \"draftModel\": draft_model_key,\n            }\n        )\n\n        print(result)\n        stats = result.stats\n        print(f\"Accepted {stats.accepted_draft_tokens_count}/{stats.predicted_tokens_count} tokens\")\n\n\n    Streaming:\n      language: python\n      code: |\n        import lmstudio as lms\n\n        main_model_key = \"qwen2.5-7b-instruct\"\n        draft_model_key = \"qwen2.5-0.5b-instruct\"\n\n        model = lms.llm(main_model_key)\n        prediction_stream = model.respond_stream(\n            \"What are the prime numbers between 0 and 100?\",\n            config={\n                \"draftModel\": draft_model_key,\n            }\n        )\n        for fragment in prediction_stream:\n            print(fragment.content, end=\"\", flush=True)\n        print() # Advance to a new line at the end of the response\n\n        stats = prediction_stream.result().stats\n      "])</script><script>self.__next_f.push([1,"  print(f\"Accepted {stats.accepted_draft_tokens_count}/{stats.predicted_tokens_count} tokens\")\n```\n109:{\"metadata\":\"$10a\",\"prettyName\":\"Speculative Decoding\",\"content\":\"$10b\",\"pageRelUrl\":\"1_python/1_llm-prediction/speculative-decoding.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n10d:{\"title\":\"Text Completions\",\"description\":\"Provide a string input for the model to complete\",\"index\":null}\n10e:T24f5,"])</script><script>self.__next_f.push([1,"\nUse `llm.complete(...)` to generate text completions from a loaded language model.\nText completions mean sending a non-formatted string to the model with the expectation that the model will complete the text.\n\nThis is different from multi-turn chat conversations. For more information on chat completions, see [Chat Completions](./chat-completion).\n\n## 1. Instantiate a Model\n\nFirst, you need to load a model to generate completions from.\nThis can be done using the top-level `llm` convenience API,\nor the `model` method in the `llm` namespace when using the scoped resource API.\nFor example, here is how to use Qwen2.5 7B Instruct.\n\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen2.5-7b-instruct\")\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\"qwen2.5-7b-instruct\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\"qwen2.5-7b-instruct\")\n\n```\n\n## 2. Generate a Completion\n\nOnce you have a loaded model, you can generate completions by passing a string to the `complete` method on the `llm` handle.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming (synchronous API)\":\n      language: python\n      code: |\n        # The `chat` object is created in the previous step.\n        result = model.complete(\"My name is\", config={\"maxTokens\": 100})\n\n        print(result)\n\n    \"Streaming (synchronous API)\":\n      language: python\n      code: |\n        # The `chat` object is created in the previous step.\n        prediction_stream = model.complete_stream(\"My name is\", config={\"maxTokens\": 100})\n\n        for fragment in prediction_stream:\n            print(fragment.content, end=\"\", flush=True)\n        print() # Advance to a new line at the end of the response\n\n    \"Non-streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        # The `chat` object is created in the previous step.\n        result = await model.complete(\"My name is\", config={\"maxTokens\": 100})\n\n        print(result)\n\n    \"Streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        # The `chat` object is created in the previous step.\n        prediction_stream = await model.complete_stream(\"My name is\", config={\"maxTokens\": 100})\n\n        async for fragment in prediction_stream:\n            print(fragment.content, end=\"\", flush=True)\n        print() # Advance to a new line at the end of the response\n\n```\n\n## 3. Print Prediction Stats\n\nYou can also print prediction metadata, such as the model used for generation, number of generated tokens, time to first token, and stop reason.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: python\n      code: |\n        # `result` is the response from the model.\n        print(\"Model used:\", result.model_info.display_name)\n        print(\"Predicted tokens:\", result.stats.predicted_tokens_count)\n        print(\"Time to first token (seconds):\", result.stats.time_to_first_token_sec)\n        print(\"Stop reason:\", result.stats.stop_reason)\n\n    \"Streaming\":\n      language: python\n      code: |\n        # After iterating through the prediction fragments,\n        # the overall prediction result may be obtained from the stream\n        result = prediction_stream.result()\n\n        print(\"Model used:\", result.model_info.display_name)\n        print(\"Predicted tokens:\", result.stats.predicted_tokens_count)\n        print(\"Time to first token (seconds):\", result.stats.time_to_first_token_sec)\n        print(\"Stop reason:\", result.stats.stop_reason)\n\n```\n\nBoth the non-streaming and streaming result access is consistent across the synchronous and\nasynchronous APIs, as `prediction_stream.result()` is a non-blocking API that raises an exception\nif no result is available (either because the prediction is still running, or because the\nprediction request failed). Prediction streams also offer a blocking (synchronous API) or\nawaitable (asynchronous API) `prediction_stream.wait_for_result()` method that internally handles\niterating the stream to completion before returning the result.\n\n## Example: Get an LLM to Simulate a Terminal\n\nHere's an example of how you might use the `complete` method to simulate a terminal.\n\n```lms_code_snippet\n  title: \"terminal-sim.py\"\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n        console_history = []\n\n        while True:\n            try:\n                user_command = input(\"$ \")\n            except EOFError:\n                print()\n                break\n            if user_command.strip() == \"exit\":\n                break\n            console_history.append(f\"$ {user_command}\")\n            history_prompt = \"\\n\".join(console_history)\n            prediction_stream = model.complete_stream(\n                history_prompt,\n                config={ \"stopStrings\": [\"$\"] },\n            )\n            for fragment in prediction_stream:\n                print(fragment.content, end=\"\", flush=True)\n            print()\n            console_history.append(prediction_stream.result().content)\n\n```\n\n## Customize Inferencing Parameters\n\nYou can pass in inferencing parameters via the `config` keyword parameter on `.complete()`.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming (synchronous API)\":\n      language: python\n      code: |\n        result = model.complete(initial_text, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \"Streaming (synchronous API)\":\n      language: python\n      code: |\n        prediction_stream = model.complete_stream(initial_text, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \"Non-streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        result = await model.complete(initial_text, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \"Streaming (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        prediction_stream = await model.complete_stream(initial_text, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n```\n\nSee [Configuring the Model](./parameters) for more information on what can be configured.\n\n### Progress Callbacks\n\nLong prompts will often take a long time to first token, i.e. it takes the model a long time to process your prompt.\nIf you want to get updates on the progress of this process, you can provide a float callback to `complete`\nthat receives a float from 0.0-1.0 representing prompt processing progress.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        llm = lms.llm()\n\n        completion = llm.complete(\n            \"My name is\",\n            on_prompt_processing_progress = (lambda progress: print(f\"{progress*100}% complete\")),\n        )\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            llm = client.llm.model()\n\n            completion = llm.complete(\n                \"My name is\",\n                on_prompt_processing_progress = (lambda progress: print(f\"{progress*100}% processed\")),\n            )\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            llm = await client.llm.model()\n\n            completion = await llm.complete(\n                \"My name is\",\n                on_prompt_processing_progress = (lambda progress: print(f\"{progress*100}% processed\")),\n            )\n\n```\n\nIn addition to `on_prompt_processing_progress`, the other available progress callbacks are:\n\n* `on_first_token`: called after prompt processing is complete and the first token is being emitted.\n  Does not receive any arguments (use the streaming iteration API or `on_prediction_fragment`\n  to process tokens as they are emitted).\n* `on_prediction_fragment`: called for each prediction fragment received by the client.\n  Receives the same prediction fragments as iterating over the stream iteration API.\n* `on_message`: called with an assistant response message when the prediction is complete.\n  Intended for appending received messages to a chat history instance.\n"])</script><script>self.__next_f.push([1,"10c:{\"metadata\":\"$10d\",\"prettyName\":\"Text Completions\",\"content\":\"$10e\",\"pageRelUrl\":\"1_python/1_llm-prediction/completion.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n110:{\"title\":\"Configuring the Model\",\"description\":\"APIs for setting inference-time and load-time parameters for your model\",\"index\":null}\n111:T1285,"])</script><script>self.__next_f.push([1,"\nYou can customize both inference-time and load-time parameters for your model. Inference parameters can be set on a per-request basis, while load parameters are set when loading the model.\n\n# Inference Parameters\n\nSet inference-time parameters such as `temperature`, `maxTokens`, `topP` and more.\n\n```lms_code_snippet\n  variants:\n    \".respond()\":\n      language: python\n      code: |\n        result = model.respond(chat, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n        })\n\n    \".complete()\":\n      language: python\n      code: |\n        result = model.complete(chat, config={\n            \"temperature\": 0.6,\n            \"maxTokens\": 50,\n            \"stopStrings\": [\"\\n\\n\"],\n          })\n\n```\n\nSee [`LLMPredictionConfigInput`](./../../typescript/api-reference/llm-prediction-config-input) in the\nTypescript SDK documentation for all configurable fields.\n\nNote that while `structured` can be set to a JSON schema definition as an inference-time configuration parameter\n(Zod schemas are not supported in the Python SDK), the preferred approach is to instead set the\n[dedicated `response_format` parameter](\u003c(./structured-responses)\u003e), which allows you to more rigorously\nenforce the structure of the output using a JSON or class based schema definition.\n\n# Load Parameters\n\nSet load-time parameters such as the context length, GPU offload ratio, and more.\n\n### Set Load Parameters with `.model()`\n\nThe `.model()` retrieves a handle to a model that has already been loaded, or loads a new one on demand (JIT loading).\n\n**Note**: if the model is already loaded, the given configuration will be **ignored**.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen2.5-7b-instruct\", config={\n            \"contextLength\": 8192,\n            \"gpu\": {\n              \"ratio\": 0.5,\n            }\n        })\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\n                \"qwen2.5-7b-instruct\",\n                config={\n                    \"contextLength\": 8192,\n                    \"gpu\": {\n                      \"ratio\": 0.5,\n                    }\n                }\n            )\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\n                \"qwen2.5-7b-instruct\",\n                config={\n                    \"contextLength\": 8192,\n                    \"gpu\": {\n                      \"ratio\": 0.5,\n                    }\n                }\n            )\n\n```\n\nSee [`LLMLoadModelConfig`](./../../typescript/api-reference/llm-load-model-config) in the\nTypescript SDK documentation for all configurable fields.\n\n### Set Load Parameters with `.load_new_instance()`\n\nThe `.load_new_instance()` method creates a new model instance and loads it with the specified configuration.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        client = lms.get_default_client()\n        model = client.llm.load_new_instance(\"qwen2.5-7b-instruct\", config={\n            \"contextLength\": 8192,\n            \"gpu\": {\n              \"ratio\": 0.5,\n            }\n        })\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.load_new_instance(\n                \"qwen2.5-7b-instruct\",\n                config={\n                    \"contextLength\": 8192,\n                    \"gpu\": {\n                      \"ratio\": 0.5,\n                    }\n                }\n            )\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.load_new_instance(\n                \"qwen2.5-7b-instruct\",\n                config={\n                    \"contextLength\": 8192,\n                    \"gpu\": {\n                      \"ratio\": 0.5,\n                    }\n                }\n            )\n\n```\n\nSee [`LLMLoadModelConfig`](./../../typescript/api-reference/llm-load-model-config) in the\nTypescript SDK documentation for all configurable fields.\n"])</script><script>self.__next_f.push([1,"10f:{\"metadata\":\"$110\",\"prettyName\":\"Configuration Parameters\",\"content\":\"$111\",\"pageRelUrl\":\"1_python/1_llm-prediction/parameters.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n113:{\"title\":\"Working with Chats\",\"description\":\"APIs for representing a chat conversation with an LLM\",\"index\":null}\n114:T88b,"])</script><script>self.__next_f.push([1,"\nSDK methods such as `llm.respond()`, `llm.applyPromptTemplate()`, or `llm.act()`\ntake in a chat parameter as an input.\nThere are a few ways to represent a chat when using the SDK.\n\n## Option 1: Input a Single String\n\nIf your chat only has one single user message, you can use a single string to represent the chat.\nHere is an example with the `.respond` method.\n\n```lms_code_snippet\nvariants:\n  \"Single string\":\n    language: python\n    code: |\n      prediction = llm.respond(\"What is the meaning of life?\")\n```\n\n## Option 2: Using the `Chat` Helper Class\n\nFor more complex tasks, it is recommended to use the `Chat` helper class.\nIt provides various commonly used methods to manage the chat.\nHere is an example with the `Chat` class, where the initial system prompt\nis supplied when initializing the chat instance, and then the initial user\nmessage is added via the corresponding method call.\n\n```lms_code_snippet\nvariants:\n  \"Simple chat\":\n    language: python\n    code: |\n      chat = Chat(\"You are a resident AI philosopher.\")\n      chat.add_user_message(\"What is the meaning of life?\")\n\n      prediction = llm.respond(chat)\n```\n\nYou can also quickly construct a `Chat` object using the `Chat.from_history` method.\n\n```lms_code_snippet\nvariants:\n  \"Chat history data\":\n    language: python\n    code: |\n      chat = Chat.from_history({\"messages\": [\n        { \"role\": \"system\", \"content\": \"You are a resident AI philosopher.\" },\n        { \"role\": \"user\", \"content\": \"What is the meaning of life?\" },\n      ]})\n\n  \"Single string\":\n    language: python\n    code: |\n      # This constructs a chat with a single user message\n      chat = Chat.from_history(\"What is the meaning of life?\")\n\n```\n\n## Option 3: Providing Chat History Data Directly\n\nAs the APIs that accept chat histories use `Chat.from_history` internally,\nthey also accept the chat history data format as a regular dictionary:\n\n```lms_code_snippet\nvariants:\n  \"Chat history data\":\n    language: python\n    code: |\n      prediction = llm.respond({\"messages\": [\n        { \"role\": \"system\", \"content\": \"You are a resident AI philosopher.\" },\n        { \"role\": \"user\", \"content\": \"What is the meaning of life?\" },\n      ]})\n```\n"])</script><script>self.__next_f.push([1,"112:{\"metadata\":\"$113\",\"prettyName\":\"Working with Chats\",\"content\":\"$114\",\"pageRelUrl\":\"1_python/1_llm-prediction/working-with-chats.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\nfc:{\"chat-completion\":\"$fd\",\"image-input\":\"$100\",\"cancelling-predictions\":\"$103\",\"structured-response\":\"$106\",\"speculative-decoding\":\"$109\",\"completion\":\"$10c\",\"parameters\":\"$10f\",\"working-with-chats\":\"$112\"}\n117:{\"title\":\"The `.act()` call\",\"description\":\"How to use the `.act()` call to turn LLMs into autonomous agents that can perform tasks on your local machine.\",\"index\":1}\n118:T22d6,"])</script><script>self.__next_f.push([1,"\n## Automatic tool calling\n\nWe introduce the concept of execution \"rounds\" to describe the combined process of running a tool, providing its output to the LLM, and then waiting for the LLM to decide what to do next.\n\n**Execution Round**\n\n```\n â€¢ run a tool -\u003e\n â†‘   â€¢ provide the result to the LLM -\u003e\n â”‚       â€¢ wait for the LLM to generate a response\n â”‚\n â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””âž” (return)\n```\n\nA model might choose to run tools multiple times before returning a final result. For example, if the LLM is writing code, it might choose to compile or run the program, fix errors, and then run it again, rinse and repeat until it gets the desired result.\n\nWith this in mind, we say that the `.act()` API is an automatic \"multi-round\" tool calling API.\n\n### Quick Example\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        def multiply(a: float, b: float) -\u003e float:\n            \"\"\"Given two numbers a and b. Returns the product of them.\"\"\"\n            return a * b\n\n        model = lms.llm(\"qwen2.5-7b-instruct\")\n        model.act(\n          \"What is the result of 12345 multiplied by 54321?\",\n          [multiply],\n          on_message=print,\n        )\n```\n\n### What does it mean for an LLM to \"use a tool\"?\n\nLLMs are largely text-in, text-out programs. So, you may ask \"how can an LLM use a tool?\". The answer is that some LLMs are trained to ask the human to call the tool for them, and expect the tool output to to be provided back in some format.\n\nImagine you're giving computer support to someone over the phone. You might say things like \"run this command for me ... OK what did it output? ... OK now click there and tell me what it says ...\". In this case you're the LLM! And you're \"calling tools\" vicariously through the person on the other side of the line.\n\n### Running multiple tool calls in parallel\n\nBy default, version 1.4.0 and later of the Python SDK will only run a single tool call request at a time,\neven if the model requests multiple tool calls in a single response message. This ensures the requests will\nbe processed correctly even if the tool implementations do not support multiple concurrent calls.\n\nWhen the tool implementations are known to be thread-safe, and are both slow and frequent enough to be worth\nrunning in parallel, the `max_parallel_tool_calls` option specifies the maximum number of tool call requests\nthat will be processed in parallel from a single model response. This value defaults to 1 (waiting for each\ntool call to complete before starting the next one). Setting this value to `None` will automatically scale\nthe maximum number of parallel tool calls to a multiple of the number of CPU cores available to the process.\n\n### Important: Model Selection\n\nThe model selected for tool use will greatly impact performance.\n\nSome general guidance when selecting a model:\n\n- Not all models are capable of intelligent tool use\n- Bigger is better (i.e., a 7B parameter model will generally perform better than a 3B parameter model)\n- We've observed [Qwen2.5-7B-Instruct](https://model.lmstudio.ai/download/lmstudio-community/Qwen2.5-7B-Instruct-GGUF) to perform well in a wide variety of cases\n- This guidance may change\n\n### Example: Multiple Tools\n\nThe following code demonstrates how to provide multiple tools in a single `.act()` call.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import math\n        import lmstudio as lms\n\n        def add(a: int, b: int) -\u003e int:\n            \"\"\"Given two numbers a and b, returns the sum of them.\"\"\"\n            return a + b\n\n        def is_prime(n: int) -\u003e bool:\n            \"\"\"Given a number n, returns True if n is a prime number.\"\"\"\n            if n \u003c 2:\n                return False\n            sqrt = int(math.sqrt(n))\n            for i in range(2, sqrt):\n                if n % i == 0:\n                    return False\n            return True\n\n        model = lms.llm(\"qwen2.5-7b-instruct\")\n        model.act(\n          \"Is the result of 12345 + 45668 a prime? Think step by step.\",\n          [add, is_prime],\n          on_message=print,\n        )\n```\n\n### Example: Chat Loop with Create File Tool\n\nThe following code creates a conversation loop with an LLM agent that can create files.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import readline # Enables input line editing\n        from pathlib import Path\n\n        import lmstudio as lms\n\n        def create_file(name: str, content: str):\n            \"\"\"Create a file with the given name and content.\"\"\"\n            dest_path = Path(name)\n            if dest_path.exists():\n                return \"Error: File already exists.\"\n            try:\n                dest_path.write_text(content, encoding=\"utf-8\")\n            except Exception as exc:\n                return \"Error: {exc!r}\"\n            return \"File created.\"\n\n        def print_fragment(fragment, round_index=0):\n            # .act() supplies the round index as the second parameter\n            # Setting a default value means the callback is also\n            # compatible with .complete() and .respond().\n            print(fragment.content, end=\"\", flush=True)\n\n        model = lms.llm()\n        chat = lms.Chat(\"You are a task focused AI assistant\")\n\n        while True:\n            try:\n                user_input = input(\"You (leave blank to exit): \")\n            except EOFError:\n                print()\n                break\n            if not user_input:\n                break\n            chat.add_user_message(user_input)\n            print(\"Bot: \", end=\"\", flush=True)\n            model.act(\n                chat,\n                [create_file],\n                on_message=chat.append,\n                on_prediction_fragment=print_fragment,\n            )\n            print()\n\n```\n\n### Progress Callbacks\n\nComplex interactions with a tool using agent may take some time to process.\n\nThe regular progress callbacks for any prediction request are available,\nbut the expected capabilities differ from those for single round predictions.\n\n* `on_prompt_processing_progress`: called during prompt processing for each\n  prediction round. Receives the progress ratio (as a float) and the round\n  index as positional arguments.\n* `on_first_token`: called after prompt processing is complete for each prediction round.\n  Receives the round index as its sole argument.\n* `on_prediction_fragment`: called for each prediction fragment received by the client.\n  Receives the prediction fragment and the round index as positional arguments.\n* `on_message`: called with an assistant response message when each prediction round is\n  complete, and with tool result messages as each tool call request is completed.\n  Intended for appending received messages to a chat history instance, and hence\n  does *not* receive the round index as an argument.\n\nThe following additional callbacks are available to monitor the prediction rounds:\n\n* `on_round_start`: called before submitting the prediction request for each round.\n  Receives the round index as its sole argument.\n* `on_prediction_completed`: called after the prediction for the round has been completed,\n  but before any requested tool calls have been initiated. Receives the round's prediction\n  result as its sole argument. A round prediction result is a regular prediction result\n  with an additional `round_index` attribute.\n* `on_round_end`: called after any tool call requests for the round have been resolved.\n\nFinally, applications may request notifications when agents emit invalid tool requests:\n\n* `handle_invalid_tool_request`: called when a tool request was unable to be processed.\n  Receives the exception that is about to be reported, as well as the original tool\n  request that resulted in the problem. When no tool request is given, this is\n  purely a notification of an unrecoverable error before the agent interaction raises\n  the given exception (allowing the application to raise its own exception instead).\n  When a tool request is given, it indicates that rather than being raised locally,\n  the text description of the exception is going to be passed back to the agent\n  as the result of that failed tool request. In these cases, the callback may either\n  return `None` to indicate that the error description should be sent to the agent,\n  raise the given exception (or a different exception) locally, or return a text\n  string that should be sent to the agent instead of the error description.\n\nFor additional details on defining tools, and an example of overriding the invalid\ntool request handling to raise all exceptions locally instead of passing them to\nback the agent, refer to [Tool Definition](./tools.md).\n"])</script><script>self.__next_f.push([1,"116:{\"metadata\":\"$117\",\"prettyName\":\"The `.act()` call\",\"content\":\"$118\",\"pageRelUrl\":\"1_python/2_agent/act.md\",\"sectionKey\":\"agent\",\"sectionPrettyName\":\"Agentic Flows\"}\n11a:{\"title\":\"Tool Definition\",\"description\":\"Define tools to be called by the LLM, and pass them to the model in the `act()` call.\",\"index\":2}\n11b:T18c7,"])</script><script>self.__next_f.push([1,"\nYou can define tools as regular Python functions and pass them to the model in the `act()` call.\nAlternatively, tools can be defined with `lmstudio.ToolFunctionDef` in order to control the\nname and description passed to the language model.\n\n## Anatomy of a Tool\n\nFollow one of the following examples to define functions as tools (the first approach\nis typically going to be the most convenient):\n\n```lms_code_snippet\n  variants:\n    \"Python function\":\n      language: python\n      code: |\n        # Type hinted functions with clear names and docstrings\n        # may be used directly as tool definitions\n        def add(a: int, b: int) -\u003e int:\n            \"\"\"Given two numbers a and b, returns the sum of them.\"\"\"\n            # The SDK ensures arguments are coerced to their specified types\n            return a + b\n\n        # Pass `add` directly to `act()` as a tool definition\n\n    \"ToolFunctionDef.from_callable\":\n      language: python\n      code: |\n        from lmstudio import ToolFunctionDef\n\n        def cryptic_name(a: int, b: int) -\u003e int:\n            return a + b\n\n        # Type hinted functions with cryptic names and missing or poor docstrings\n        # can be turned into clear tool definitions with `from_callable`\n        tool_def = ToolFunctionDef.from_callable(\n          cryptic_name,\n          name=\"add\",\n          description=\"Given two numbers a and b, returns the sum of them.\"\n        )\n        # Pass `tool_def` to `act()` as a tool definition\n\n    \"ToolFunctionDef\":\n      language: python\n      code: |\n        from lmstudio import ToolFunctionDef\n\n        def cryptic_name(a, b):\n            return a + b\n\n        # Functions without type hints can be used without wrapping them\n        # at runtime by defining a tool function directly.\n        tool_def = ToolFunctionDef(\n          name=\"add\",\n          description=\"Given two numbers a and b, returns the sum of them.\",\n          parameters={\n            \"a\": int,\n            \"b\": int,\n          },\n          implementation=cryptic_name,\n        )\n        # Pass `tool_def` to `act()` as a tool definition\n\n```\n\n**Important**: The tool name, description, and the parameter definitions are all passed to the model!\n\nThis means that your wording will affect the quality of the generation. Make sure to always provide a clear description of the tool so the model knows how to use it.\n\n## Tools with External Effects (like Computer Use or API Calls)\n\nTools can also have external effects, such as creating files or calling programs and even APIs. By implementing tools with external effects, you\ncan essentially turn your LLMs into autonomous agents that can perform tasks on your local machine.\n\n## Example: `create_file_tool`\n\n### Tool Definition\n\n```lms_code_snippet\n  title: \"create_file_tool.py\"\n  variants:\n    Python:\n      language: python\n      code: |\n        from pathlib import Path\n\n        def create_file(name: str, content: str):\n            \"\"\"Create a file with the given name and content.\"\"\"\n            dest_path = Path(name)\n            if dest_path.exists():\n                return \"Error: File already exists.\"\n            try:\n                dest_path.write_text(content, encoding=\"utf-8\")\n            except Exception as exc:\n                return \"Error: {exc!r}\"\n            return \"File created.\"\n\n```\n\n### Example code using the `create_file` tool:\n\n```lms_code_snippet\n  title: \"example.py\"\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n        from create_file_tool import create_file\n\n        model = lms.llm(\"qwen2.5-7b-instruct\")\n        model.act(\n          \"Please create a file named output.txt with your understanding of the meaning of life.\",\n          [create_file],\n        )\n```\n\n## Handling tool calling errors\n\nBy default, version 1.3.0 and later of the Python SDK will automatically convert\nexceptions raised by tool calls to text and report them back to the language model.\nIn many cases, when notified of an error in this way, a language model is able\nto either adjust its request to avoid the failure, or else accept the failure as\na valid response to its request (consider a prompt like `Attempt to divide 1 by 0\nusing the provided tool. Explain the result.`, where the expected\nresponse is an explanation of the `ZeroDivisionError` exception the Python\ninterpreter raises when instructed to divide by zero).\n\nThis error handling behaviour can be overridden using the `handle_invalid_tool_request`\ncallback. For example, the following code reverts the error handling back to raising\nexceptions locally in the client:\n\n```lms_code_snippet\n  title: \"example.py\"\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        def divide(numerator: float, denominator: float) -\u003e float:\n            \"\"\"Divide the given numerator by the given denominator. Return the result.\"\"\"\n            return numerator / denominator\n\n        model = lms.llm(\"qwen2.5-7b-instruct\")\n        chat = Chat()\n        chat.add_user_message(\n            \"Attempt to divide 1 by 0 using the tool. Explain the result.\"\n        )\n\n        def _raise_exc_in_client(\n            exc: LMStudioPredictionError, request: ToolCallRequest | None\n        ) -\u003e None:\n            raise exc\n\n        act_result = llm.act(\n            chat,\n            [divide],\n            handle_invalid_tool_request=_raise_exc_in_client,\n        )\n```\n\nWhen a tool request is passed in, the callback results are processed as follows:\n\n* `None`: the original exception text is passed to the LLM unmodified\n* a string: the returned string is passed to the LLM instead of the original\n  exception text\n* raising an exception (whether the passed in exception or a new exception):\n  the raised exception is propagated locally in the client, terminating the\n  prediction process\n\nIf no tool request is passed in, the callback invocation is a notification only,\nand the exception cannot be converted to text for passing pack to the LLM\n(although it can still be replaced with a different exception). These cases\nindicate failures in the expected communication with the server API that mean\nthe prediction process cannot reasonably continue, so if the callback doesn't\nraise an exception, the calling code will raise the original exception directly.\n"])</script><script>self.__next_f.push([1,"119:{\"metadata\":\"$11a\",\"prettyName\":\"Tool Definition\",\"content\":\"$11b\",\"pageRelUrl\":\"1_python/2_agent/tools.md\",\"sectionKey\":\"agent\",\"sectionPrettyName\":\"Agentic Flows\"}\n115:{\"act\":\"$116\",\"tools\":\"$119\"}\n11e:{\"title\":\"Embedding\",\"description\":\"Generate text embeddings from input text\",\"index\":1}\n11d:{\"metadata\":\"$11e\",\"prettyName\":\"Generating embedding vectors\",\"content\":\"\\nGenerate embeddings for input text. Embeddings are vector representations of text that capture semantic meaning. Embeddings are a building block for RAG (Retrieval-Augmented Generation) and other similarity-based tasks.\\n\\n### Prerequisite: Get an Embedding Model\\n\\nIf you don't yet have an embedding model, you can download a model like `nomic-ai/nomic-embed-text-v1.5` using the following command:\\n\\n```bash\\nlms get nomic-ai/nomic-embed-text-v1.5\\n```\\n\\n## Create Embeddings\\n\\nTo convert a string to a vector representation, pass it to the `embed` method on the corresponding embedding model handle.\\n\\n```lms_code_snippet\\n  title: \\\"example.py\\\"\\n  variants:\\n    \\\"Python (convenience API)\\\":\\n      language: python\\n      code: |\\n        import lmstudio as lms\\n\\n        model = lms.embedding_model(\\\"nomic-embed-text-v1.5\\\")\\n\\n        embedding = model.embed(\\\"Hello, world!\\\")\\n\\n```\\n\",\"pageRelUrl\":\"1_python/3_embedding/index.md\",\"sectionKey\":\"embedding\",\"sectionPrettyName\":\"Text Embedding\"}\n11c:{\"\":\"$11d\"}\n121:{\"title\":\"Tokenization\",\"description\":\"Tokenize text using a model's tokenizer\",\"index\":1}\n122:T8d2,"])</script><script>self.__next_f.push([1,"\nModels use a tokenizer to internally convert text into \"tokens\" they can deal with more easily. LM Studio exposes this tokenizer for utility.\n\n## Tokenize\n\nYou can tokenize a string with a loaded LLM or embedding model using the SDK.\nIn the below examples, the LLM reference can be replaced with an\nembedding model reference without requiring any other changes.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n\n        tokens = model.tokenize(\"Hello, world!\")\n\n        print(tokens) # Array of token IDs.\n```\n\n## Count tokens\n\nIf you only care about the number of tokens, simply check the length of the resulting array.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        token_count = len(model.tokenize(\"Hello, world!\"))\n        print(\"Token count:\", token_count)\n```\n\n### Example: count context\n\nYou can determine if a given conversation fits into a model's context by doing the following:\n\n1. Convert the conversation to a string using the prompt template.\n2. Count the number of tokens in the string.\n3. Compare the token count to the model's context length.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        def does_chat_fit_in_context(model: lms.LLM, chat: lms.Chat) -\u003e bool:\n            # Convert the conversation to a string using the prompt template.\n            formatted = model.apply_prompt_template(chat)\n            # Count the number of tokens in the string.\n            token_count = len(model.tokenize(formatted))\n            # Get the current loaded context length of the model\n            context_length = model.get_context_length()\n            return token_count \u003c context_length\n\n        model = lms.llm()\n\n        chat = lms.Chat.from_history({\n            \"messages\": [\n                { \"role\": \"user\", \"content\": \"What is the meaning of life.\" },\n                { \"role\": \"assistant\", \"content\": \"The meaning of life is...\" },\n                # ... More messages\n            ]\n        })\n\n        print(\"Fits in context:\", does_chat_fit_in_context(model, chat))\n\n```\n"])</script><script>self.__next_f.push([1,"120:{\"metadata\":\"$121\",\"prettyName\":\"Tokenizing text\",\"content\":\"$122\",\"pageRelUrl\":\"1_python/4_tokenization/index.md\",\"sectionKey\":\"tokenization\",\"sectionPrettyName\":\"Tokenization\"}\n11f:{\"\":\"$120\"}\n125:{\"title\":\"List Downloaded Models\",\"description\":\"APIs to list the available models in a given local environment\",\"index\":null}\n126:T7c6,\nYou can iterate through locally available models using the downloaded model listing methods.\n\nThe listing results offer `.model()` and `.load_new_instance()` methods, which allow the\ndownloaded model reference to be converted in the full SDK handle for a loaded model.\n\n## Available Models on the LM Studio Server\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        downloaded = lms.list_downloaded_models()\n        llm_only = lms.list_downloaded_models(\"llm\")\n        embedding_only = lms.list_downloaded_models(\"embedding\")\n\n        for model in downloaded:\n            print(model)\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            downloaded = client.list_downloaded_models()\n            llm_only = client.llm.list_downloaded()\n            embedding_only = client.embedding.list_downloaded()\n\n        for model in downloaded:\n            print(model)\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            downloaded = await client.list_downloaded_models()\n            llm_only = await client.llm.list_downloaded()\n            embedding_only = await client.embedding.list_downloaded()\n\n        for model in downloaded:\n            print(model)\n\n```\nThis will give you results equivalent to using [`lms ls`](../../cli/ls) in the CLI.\n\n\n### Example output:"])</script><script>self.__next_f.push([1,"\n\n```python\nDownloadedLlm(model_key='qwen2.5-7b-instruct-1m', display_name='Qwen2.5 7B Instruct 1M', architecture='qwen2', vision=False)\nDownloadedEmbeddingModel(model_key='text-embedding-nomic-embed-text-v1.5', display_name='Nomic Embed Text v1.5', architecture='nomic-bert')\n```\n124:{\"metadata\":\"$125\",\"prettyName\":\"List Downloaded Models\",\"content\":\"$126\",\"pageRelUrl\":\"1_python/5_manage-models/list-downloaded.md\",\"sectionKey\":\"manage-models\",\"sectionPrettyName\":\"Manage Models\"}\n128:{\"title\":\"List Loaded Models\",\"description\":\"Query which models are currently loaded\",\"index\":null}\n129:T5e0,\nYou can iterate through models loaded into memory using the functions and methods shown below.\n\nThe results are full SDK model handles, allowing access to all model functionality. \n\n\n## List Models Currently Loaded in Memory\n\nThis will give you results equivalent to using [`lms ps`](../../cli/ps) in the CLI.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        all_loaded_models = lms.list_loaded_models()\n        llm_only = lms.list_loaded_models(\"llm\")\n        embedding_only = lms.list_loaded_models(\"embedding\")\n\n        print(all_loaded_models)\n\n    Python (scoped resource API):\n      language: python\n      code: |\n        import lms\n\n        with lms.Client() as client:\n            all_loaded_models = client.list_loaded_models()\n            llm_only = client.llm.list_loaded()\n            embedding_only = client.embedding.list_loaded()\n\n            print(all_loaded_models)\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            all_loaded_models = await client.list_loaded_models()\n            llm_only = await client.llm.list_loaded()\n            embedding_only = await client.embedding.list"])</script><script>self.__next_f.push([1,"_loaded()\n\n            print(all_loaded_models)\n\n```\n127:{\"metadata\":\"$128\",\"prettyName\":\"List Loaded Models\",\"content\":\"$129\",\"pageRelUrl\":\"1_python/5_manage-models/list-loaded.md\",\"sectionKey\":\"manage-models\",\"sectionPrettyName\":\"Manage Models\"}\n12b:{\"title\":\"Manage Models in Memory\",\"description\":\"APIs to load, access, and unload models from memory\",\"index\":null}\n12c:T1bd7,"])</script><script>self.__next_f.push([1,"\nAI models are huge. It can take a while to load them into memory. LM Studio's SDK allows you to precisely control this process.\n\n**Model namespaces:**\n\n- LLMs are accessed through the `client.llm` namespace\n- Embedding models are accessed through the `client.embedding` namespace\n- `lmstudio.llm` is equivalent to `client.llm.model` on the default client\n- `lmstudio.embedding_model` is equivalent to `client.embedding.model` on the default client\n\n**Most commonly:**\n\n- Use `.model()` to get any currently loaded model\n- Use `.model(\"model-key\")` to use a specific model\n\n**Advanced (manual model management):**\n\n- Use `.load_new_instance(\"model-key\")` to load a new instance of a model\n- Use `.unload(\"model-key\")` or `model_handle.unload()` to unload a model from memory\n\n## Get the Current Model with `.model()`\n\nIf you already have a model loaded in LM Studio (either via the GUI or `lms load`),\nyou can use it by calling `.model()` without any arguments.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model()\n\n```\n\n## Get a Specific Model with `.model(\"model-key\")`\n\nIf you want to use a specific model, you can provide the model key as an argument to `.model()`.\n\n#### Get if Loaded, or Load if not\n\nCalling `.model(\"model-key\")` will load the model if it's not already loaded, or return the existing instance if it is.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen/qwen3-4b-2507\")\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\"qwen/qwen3-4b-2507\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\"qwen/qwen3-4b-2507\")\n\n```\n\n\u003c!--\nLearn more about the `.model()` method and the parameters it accepts in the [API Reference](../api-reference/model).\n--\u003e\n\n## Load a New Instance of a Model with `.load_new_instance()`\n\nUse `load_new_instance()` to load a new instance of a model, even if one already exists.\nThis allows you to have multiple instances of the same or different models loaded at the same time.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        client = lms.get_default_client()\n        model = client.llm.load_new_instance(\"qwen/qwen3-4b-2507\")\n        another_model = client.llm.load_new_instance(\"qwen/qwen3-4b-2507\", \"my-second-model\")\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.load_new_instance(\"qwen/qwen3-4b-2507\")\n            another_model = client.llm.load_new_instance(\"qwen/qwen3-4b-2507\", \"my-second-model\")\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.load_new_instance(\"qwen/qwen3-4b-2507\")\n            another_model = await client.llm.load_new_instance(\"qwen/qwen3-4b-2507\", \"my-second-model\")\n\n```\n\n\u003c!--\nLearn more about the `.load_new_instance()` method and the parameters it accepts in the [API Reference](../api-reference/load_new_instance).\n--\u003e\n\n### Note about Instance Identifiers\n\nIf you provide an instance identifier that already exists, the server will throw an error.\nSo if you don't really care, it's safer to not provide an identifier, in which case\nthe server will generate one for you. You can always check in the server tab in LM Studio, too!\n\n## Unload a Model from Memory with `.unload()`\n\nOnce you no longer need a model, you can unload it by simply calling `unload()` on its handle.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n        model.unload()\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n            model.unload()\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model()\n            await model.unload()\n\n```\n\n## Set Custom Load Config Parameters\n\nYou can also specify the same load-time configuration options when loading a model, such as Context Length and GPU offload.\n\nSee [load-time configuration](../llm-prediction/parameters) for more.\n\n## Set an Auto Unload Timer (TTL)\n\nYou can specify a _time to live_ for a model you load, which is the idle time (in seconds)\nafter the last request until the model unloads. See [Idle TTL](/docs/app/api/ttl-and-auto-evict) for more on this.\n\n```lms_protip\nIf you specify a TTL to `model()`, it will only apply if `model()` loads\na new instance, and will _not_ retroactively change the TTL of an existing instance.\n```\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm(\"qwen/qwen3-4b-2507\", ttl=3600)\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model(\"qwen/qwen3-4b-2507\", ttl=3600)\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model(\"qwen/qwen3-4b-2507\", ttl=3600)\n\n```\n\n\u003c!--\n(TODO?: Cover the JIT implications of setting a TTL, and the default TTL variations)\n--\u003e\n"])</script><script>self.__next_f.push([1,"12a:{\"metadata\":\"$12b\",\"prettyName\":\"Load and Access Models\",\"content\":\"$12c\",\"pageRelUrl\":\"1_python/5_manage-models/loading.md\",\"sectionKey\":\"manage-models\",\"sectionPrettyName\":\"Manage Models\"}\n123:{\"list-downloaded\":\"$124\",\"list-loaded\":\"$127\",\"loading\":\"$12a\"}\n12f:{\"title\":\"Get Context Length\",\"description\":\"API to get the maximum context length of a model.\",\"index\":null}\n130:T8e5,"])</script><script>self.__next_f.push([1,"\nLLMs and embedding models, due to their fundamental architecture, have a property called `context length`, and more specifically a **maximum** context length. Loosely speaking, this is how many tokens the models can \"keep in memory\" when generating text or embeddings. Exceeding this limit will result in the model behaving erratically.\n\n## Use the `get_context_length()` function on the model object\n\nIt's useful to be able to check the context length of a model, especially as an extra check before providing potentially long input to the model.\n\n```lms_code_snippet\n  title: \"example.py\"\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        context_length = model.get_context_length()\n```\n\nThe `model` in the above code snippet is an instance of a loaded model you get from the `llm.model` method. See [Manage Models in Memory](../manage-models/loading) for more information.\n\n### Example: Check if the input will fit in the model's context window\n\nYou can determine if a given conversation fits into a model's context by doing the following:\n\n1. Convert the conversation to a string using the prompt template.\n2. Count the number of tokens in the string.\n3. Compare the token count to the model's context length.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        def does_chat_fit_in_context(model: lms.LLM, chat: lms.Chat) -\u003e bool:\n            # Convert the conversation to a string using the prompt template.\n            formatted = model.apply_prompt_template(chat)\n            # Count the number of tokens in the string.\n            token_count = len(model.tokenize(formatted))\n            # Get the current loaded context length of the model\n            context_length = model.get_context_length()\n            return token_count \u003c context_length\n\n        model = lms.llm()\n\n        chat = lms.Chat.from_history({\n            \"messages\": [\n                { \"role\": \"user\", \"content\": \"What is the meaning of life.\" },\n                { \"role\": \"assistant\", \"content\": \"The meaning of life is...\" },\n                # ... More messages\n            ]\n        })\n\n        print(\"Fits in context:\", does_chat_fit_in_context(model, chat))\n\n```\n"])</script><script>self.__next_f.push([1,"12e:{\"metadata\":\"$12f\",\"prettyName\":\"Get Context Length\",\"content\":\"$130\",\"pageRelUrl\":\"1_python/6_model-info/get-context-length.md\",\"sectionKey\":\"model-info\",\"sectionPrettyName\":\"Model Info\"}\n132:{\"title\":\"Get Load Config\",\"description\":\"Get the load configuration of the model\",\"index\":null}\n133:T54c,\n*Required Python SDK version*: **1.2.0**\n\nLM Studio allows you to configure certain parameters when loading a model\n[through the server UI](/docs/advanced/per-model) or [through the API](/docs/api/sdk/load-model).\n\nYou can retrieve the config with which a given model was loaded using the SDK.\n\nIn the below examples, the LLM reference can be replaced with an\nembedding model reference without requiring any other changes.\n\n```lms_protip\nContext length is a special case that [has its own method](/docs/api/sdk/get-context-length).\n```\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n\n        print(model.get_load_config())\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n\n            print(model.get_load_config())\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.Client() as client:\n            model = await client.llm.model()\n\n            print(await model.get_load_config())\n\n```\n131:{\"metadata\":\"$132\",\"prettyName\":\"Get Load Config\",\"content\":\"$133\",\"pageRelUrl\":\"1_python/6_model-info/get-load-config.md\",\"sectionKey\":\"model-info\",\"sectionPrettyName\":\"Model Info\"}\n135:{\"title\":\"Get Model Info\",\"description\":\"Get information about the model\",\"index\":null}\n136:T637,\nYou can access general information and metadata about a model itself from a loaded\ninstance of that "])</script><script>self.__next_f.push([1,"model.\n\nIn the below examples, the LLM reference can be replaced with an\nembedding model reference without requiring any other changes.\n\n```lms_code_snippet\n  variants:\n    \"Python (convenience API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        model = lms.llm()\n\n        print(model.get_info())\n\n    \"Python (scoped resource API)\":\n      language: python\n      code: |\n        import lmstudio as lms\n\n        with lms.Client() as client:\n            model = client.llm.model()\n\n            print(model.get_info())\n\n    \"Python (asynchronous API)\":\n      language: python\n      code: |\n        # Note: assumes use of an async function or the \"python -m asyncio\" asynchronous REPL\n        # Requires Python SDK version 1.5.0 or later\n        import lmstudio as lms\n\n        async with lms.AsyncClient() as client:\n            model = await client.llm.model()\n\n            print(await model.get_info())\n\n```\n\n## Example output\n\n```python\nLlmInstanceInfo.from_dict({\n  \"architecture\": \"qwen2\",\n  \"contextLength\": 4096,\n  \"displayName\": \"Qwen2.5 7B Instruct 1M\",\n  \"format\": \"gguf\",\n  \"identifier\": \"qwen2.5-7b-instruct\",\n  \"instanceReference\": \"lpFZPBQjhSZPrFevGyY6Leq8\",\n  \"maxContextLength\": 1010000,\n  \"modelKey\": \"qwen2.5-7b-instruct-1m\",\n  \"paramsString\": \"7B\",\n  \"path\": \"lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF/Qwen2.5-7B-Instruct-1M-Q4_K_M.gguf\",\n  \"sizeBytes\": 4683073888,\n  \"trainedForToolUse\": true,\n  \"type\": \"llm\",\n  \"vision\": false\n})\n```\n134:{\"metadata\":\"$135\",\"prettyName\":\"Get Model Info\",\"content\":\"$136\",\"pageRelUrl\":\"1_python/6_model-info/get-model-info.md\",\"sectionKey\":\"model-info\",\"sectionPrettyName\":\"Model Info\"}\n12d:{\"get-context-length\":\"$12e\",\"get-load-config\":\"$131\",\"get-model-info\":\"$134\"}\nf1:{\"\":\"$f2\",\"getting-started\":\"$f5\",\"llm-prediction\":\"$fc\",\"agent\":\"$115\",\"embedding\":\"$11c\",\"tokenization\":\"$11f\",\"manage-models\":\"$123\",\"model-info\":\"$12d\"}\n139:{\"title\":\"`lmstudio-js` (TypeScript SDK)\",\"description\":\"Getting started with LM Studio's Typescript / JavaScript SDK\",\"index\""])</script><script>self.__next_f.push([1,":1}\n13a:T78f,\nThe SDK provides you a set of programmatic tools to interact with LLMs, embeddings models, and agentic flows.\n\n## Installing the SDK\n\n`lmstudio-js` is available as an npm package. You can install it using npm, yarn, or pnpm.\n\n```lms_code_snippet\n  variants:\n    npm:\n      language: bash\n      code: |\n        npm install @lmstudio/sdk --save\n    yarn:\n      language: bash\n      code: |\n        yarn add @lmstudio/sdk\n    pnpm:\n      language: bash\n      code: |\n        pnpm add @lmstudio/sdk\n```\n\nFor the source code and open source contribution, visit [lmstudio-js](https://github.com/lmstudio-ai/lmstudio-js) on GitHub.\n\n## Features\n\n- Use LLMs to [respond in chats](./typescript/llm-prediction/chat-completion) or predict [text completions](./typescript/llm-prediction/completion)\n- Define functions as tools, and turn LLMs into [autonomous agents](./typescript/agent/act) that run completely locally\n- [Load](./typescript/manage-models/loading), [configure](./typescript/llm-prediction/parameters), and [unload](./typescript/manage-models/loading) models from memory\n- Supports for both browser and any Node-compatible environments\n- Generate embeddings for text, and more!\n\n## Quick Example: Chat with a Llama Model\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model(\"qwen/qwen3-4b-2507\");\n        const result = await model.respond(\"What is the meaning of life?\");\n\n        console.info(result.content);\n```\n\n### Getting Local Models\n\nThe above code requires the [qwen3-4b-2507](https://lmstudio.ai/models/qwen/qwen3-4b-2507). If you don't have the model, run the following command in the terminal to download it.\n\n```bash\nlms get qwen/qwen3-4b-2507\n```\n\nRead more about `lms get` in LM Studio's CLI [here](./cli/get).\n138:{\"metadata\":\"$139\",\"prettyName\":\"Introduction\",\"content\":\"$13a\",\"pageRelUrl\":\"2_typescript/index"])</script><script>self.__next_f.push([1,".md\"}\n13c:{\"title\":\"Project Setup\",\"description\":\"Set up your `lmstudio-js` app or script.\",\"index\":2}\n13b:{\"metadata\":\"$13c\",\"prettyName\":\"Project Setup\",\"content\":\"\\n`@lmstudio/sdk` is a library published on npm that allows you to use `lmstudio-js` in your own projects. It is open source and it's developed on GitHub. You can find the source code [here](https://github.com/lmstudio-ai/lmstudio-js).\\n\\n## Creating a New `node` Project\\n\\nUse the following command to start an interactive project setup:\\n\\n```lms_code_snippet\\n  variants:\\n    TypeScript (Recommended):\\n      language: bash\\n      code: |\\n        lms create node-typescript\\n    Javascript:\\n      language: bash\\n      code: |\\n        lms create node-javascript\\n```\\n\\n## Add `lmstudio-js` to an Exiting Project\\n\\nIf you have already created a project and would like to use `lmstudio-js` in it, you can install it using npm, yarn, or pnpm.\\n\\n```lms_code_snippet\\n  variants:\\n    npm:\\n      language: bash\\n      code: |\\n        npm install @lmstudio/sdk --save\\n    yarn:\\n      language: bash\\n      code: |\\n        yarn add @lmstudio/sdk\\n    pnpm:\\n      language: bash\\n      code: |\\n        pnpm add @lmstudio/sdk\\n```\\n\",\"pageRelUrl\":\"2_typescript/project-setup.md\"}\n13f:{\"title\":\"Chat Completions\",\"description\":\"APIs for a multi-turn chat conversations with an LLM\",\"index\":2}\n140:T1d9d,"])</script><script>self.__next_f.push([1,"\nUse `llm.respond(...)` to generate completions for a chat conversation.\n\n## Quick Example: Generate a Chat Response\n\nThe following snippet shows how to stream the AI's response to quick chat prompt.\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model();\n\n        for await (const fragment of model.respond(\"What is the meaning of life?\")) {\n          process.stdout.write(fragment.content);\n        }\n```\n\n## Obtain a Model\n\nFirst, you need to get a model handle. This can be done using the `model` method in the `llm` namespace. For example, here is how to use Qwen2.5 7B Instruct.\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n```\n\nThere are other ways to get a model handle. See [Managing Models in Memory](./../manage-models/loading) for more info.\n\n## Manage Chat Context\n\nThe input to the model is referred to as the \"context\". Conceptually, the model receives a multi-turn conversation as input, and it is asked to predict the assistant's response in that conversation.\n\n```lms_code_snippet\n  variants:\n    \"Using an array of messages\":\n      language: typescript\n      code: |\n        import { Chat } from \"@lmstudio/sdk\";\n\n        // Create a chat object from an array of messages.\n        const chat = Chat.from([\n          { role: \"system\", content: \"You are a resident AI philosopher.\" },\n          { role: \"user\", content: \"What is the meaning of life?\" },\n        ]);\n    \"Constructing a Chat object\":\n      language: typescript\n      code: |\n        import { Chat } from \"@lmstudio/sdk\";\n\n        // Create an empty chat object.\n        const chat = Chat.empty();\n\n        // Build the chat context by appending messages.\n        chat.append(\"system\", \"You are a resident AI philosopher.\");\n        chat.append(\"user\", \"What is the meaning of life?\");\n```\n\nSee [Working with Chats](./working-with-chats) for more information on managing chat context.\n\n\u003c!-- , and [`Chat`](./../api-reference/chat) for API reference for the `Chat` class. --\u003e\n\n## Generate a response\n\nYou can ask the LLM to predict the next response in the chat context using the `respond()` method.\n\n```lms_code_snippet\n  variants:\n    Streaming:\n      language: typescript\n      code: |\n        // The `chat` object is created in the previous step.\n        const prediction = model.respond(chat);\n\n        for await (const { content } of prediction) {\n          process.stdout.write(content);\n        }\n\n        console.info(); // Write a new line to prevent text from being overwritten by your shell.\n\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        // The `chat` object is created in the previous step.\n        const result = await model.respond(chat);\n\n        console.info(result.content);\n```\n\n## Customize Inferencing Parameters\n\nYou can pass in inferencing parameters as the second parameter to `.respond()`.\n\n```lms_code_snippet\n  variants:\n    Streaming:\n      language: typescript\n      code: |\n        const prediction = model.respond(chat, {\n          temperature: 0.6,\n          maxTokens: 50,\n        });\n\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        const result = await model.respond(chat, {\n          temperature: 0.6,\n          maxTokens: 50,\n        });\n```\n\nSee [Configuring the Model](./parameters) for more information on what can be configured.\n\n## Print prediction stats\n\nYou can also print prediction metadata, such as the model used for generation, number of generated\ntokens, time to first token, and stop reason.\n\n```lms_code_snippet\n  variants:\n    Streaming:\n      language: typescript\n      code: |\n        // If you have already iterated through the prediction fragments,\n        // doing this will not result in extra waiting.\n        const result = await prediction.result();\n\n        console.info(\"Model used:\", result.modelInfo.displayName);\n        console.info(\"Predicted tokens:\", result.stats.predictedTokensCount);\n        console.info(\"Time to first token (seconds):\", result.stats.timeToFirstTokenSec);\n        console.info(\"Stop reason:\", result.stats.stopReason);\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        // `result` is the response from the model.\n        console.info(\"Model used:\", result.modelInfo.displayName);\n        console.info(\"Predicted tokens:\", result.stats.predictedTokensCount);\n        console.info(\"Time to first token (seconds):\", result.stats.timeToFirstTokenSec);\n        console.info(\"Stop reason:\", result.stats.stopReason);\n```\n\n## Example: Multi-turn Chat\n\n\u003c!-- TODO: Probably needs polish here: --\u003e\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { Chat, LMStudioClient } from \"@lmstudio/sdk\";\n        import { createInterface } from \"readline/promises\";\n\n        const rl = createInterface({ input: process.stdin, output: process.stdout });\n        const client = new LMStudioClient();\n        const model = await client.llm.model();\n        const chat = Chat.empty();\n\n        while (true) {\n          const input = await rl.question(\"You: \");\n          // Append the user input to the chat\n          chat.append(\"user\", input);\n\n          const prediction = model.respond(chat, {\n            // When the model finish the entire message, push it to the chat\n            onMessage: (message) =\u003e chat.append(message),\n          });\n          process.stdout.write(\"Bot: \");\n          for await (const { content } of prediction) {\n            process.stdout.write(content);\n          }\n          process.stdout.write(\"\\n\");\n        }\n```\n\n\u003c!-- ### Progress callbacks\n\nTODO: Cover onFirstToken callback (Python SDK has this now)\n\nLong prompts will often take a long time to first token, i.e. it takes the model a long time to process your prompt.\nIf you want to get updates on the progress of this process, you can provide a float callback to `respond`\nthat receives a float from 0.0-1.0 representing prompt processing progress.\n\n```lms_code_snippet\n  variants:\n    Python:\n      language: python\n      code: |\n        import lmstudio as lm\n\n        llm = lm.llm()\n\n        response = llm.respond(\n            \"What is LM Studio?\",\n            on_progress: lambda progress: print(f\"{progress*100}% complete\")\n        )\n\n    Python (with scoped resources):\n      language: python\n      code: |\n        import lmstudio\n\n        with lmstudio.Client() as client:\n            llm = client.llm.model()\n\n            response = llm.respond(\n                \"What is LM Studio?\",\n                on_progress: lambda progress: print(f\"{progress*100}% processed\")\n            )\n\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const llm = await client.llm.model();\n\n        const prediction = llm.respond(\n          \"What is LM Studio?\",\n          {onPromptProcessingProgress: (progress) =\u003e process.stdout.write(`${progress*100}% processed`)});\n```\n\n### Prediction configuration\n\nYou can also specify the same prediction configuration options as you could in the\nin-app chat window sidebar. Please consult your specific SDK to see exact syntax. --\u003e\n"])</script><script>self.__next_f.push([1,"13e:{\"metadata\":\"$13f\",\"prettyName\":\"Chat\",\"content\":\"$140\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/chat-completion.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n142:{\"title\":\"Working with Chats\",\"description\":\"APIs for representing a chat conversation with an LLM\",\"index\":3}\n143:Ta86,"])</script><script>self.__next_f.push([1,"\nSDK methods such as `model.respond()`, `model.applyPromptTemplate()`, or `model.act()`\ntakes in a chat parameter as an input. There are a few ways to represent a chat in the SDK.\n\n## Option 1: Array of Messages\n\nYou can use an array of messages to represent a chat. Here is an example with the `.respond()` method.\n\n```lms_code_snippet\nvariants:\n  \"Text-only\":\n    language: typescript\n    code: |\n      const prediction = model.respond([\n        { role: \"system\", content: \"You are a resident AI philosopher.\" },\n        { role: \"user\", content: \"What is the meaning of life?\" },\n      ]);\n  With Images:\n    language: typescript\n    code: |\n      const image = await client.files.prepareImage(\"/path/to/image.jpg\");\n\n      const prediction = model.respond([\n        { role: \"system\", content: \"You are a state-of-art object recognition system.\" },\n        { role: \"user\", content: \"What is this object?\", images: [image] },\n      ]);\n```\n\n## Option 2: Input a Single String\n\nIf your chat only has one single user message, you can use a single string to represent the chat. Here is an example with the `.respond` method.\n\n```lms_code_snippet\nvariants:\n  TypeScript:\n    language: typescript\n    code: |\n      const prediction = model.respond(\"What is the meaning of life?\");\n```\n\n## Option 3: Using the `Chat` Helper Class\n\nFor more complex tasks, it is recommended to use the `Chat` helper classes. It provides various commonly used methods to manage the chat. Here is an example with the `Chat` class.\n\n```lms_code_snippet\nvariants:\n  \"Text-only\":\n    language: typescript\n    code: |\n      const chat = Chat.empty();\n      chat.append(\"system\", \"You are a resident AI philosopher.\");\n      chat.append(\"user\", \"What is the meaning of life?\");\n\n      const prediction = model.respond(chat);\n  With Images:\n    language: typescript\n    code: |\n      const image = await client.files.prepareImage(\"/path/to/image.jpg\");\n\n      const chat = Chat.empty();\n      chat.append(\"system\", \"You are a state-of-art object recognition system.\");\n      chat.append(\"user\", \"What is this object?\", { images: [image] });\n\n      const prediction = model.respond(chat);\n```\n\nYou can also quickly construct a `Chat` object using the `Chat.from` method.\n\n```lms_code_snippet\nvariants:\n  \"Array of messages\":\n    language: typescript\n    code: |\n      const chat = Chat.from([\n        { role: \"system\", content: \"You are a resident AI philosopher.\" },\n        { role: \"user\", content: \"What is the meaning of life?\" },\n      ]);\n  \"Single string\":\n    language: typescript\n    code: |\n      // This constructs a chat with a single user message\n      const chat = Chat.from(\"What is the meaning of life?\");\n```\n"])</script><script>self.__next_f.push([1,"141:{\"metadata\":\"$142\",\"prettyName\":\"Working with Chats\",\"content\":\"$143\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/working-with-chats.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n145:{\"title\":\"Cancelling Predictions\",\"description\":\"Stop an ongoing prediction in `lmstudio-js`\",\"index\":4}\n146:T816,"])</script><script>self.__next_f.push([1,"\nSometimes you may want to halt a prediction before it finishes. For example, the user might change their mind or your UI may navigate away. `lmstudio-js` provides two simple ways to cancel a running prediction.\n\n## 1. Call `.cancel()` on the prediction\n\nEvery prediction method returns an `OngoingPrediction` instance. Calling `.cancel()` stops generation and causes the final `stopReason` to be `\"userStopped\"`. In the example below we schedule the cancel call on a timer:\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n\n        const prediction = model.respond(\"What is the meaning of life?\", {\n          maxTokens: 50,\n        });\n        setTimeout(() =\u003e prediction.cancel(), 1000); // cancel after 1 second\n\n        const result = await prediction.result();\n        console.info(result.stats.stopReason); // \"userStopped\"\n```\n\n## 2. Use an `AbortController`\n\nIf your application already uses an `AbortController` to propagate cancellation, you can pass its `signal` to the prediction method. Aborting the controller stops the prediction with the same `stopReason`:\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n\n        const controller = new AbortController();\n        const prediction = model.respond(\"What is the meaning of life?\", {\n          maxTokens: 50,\n          signal: controller.signal,\n        });\n        setTimeout(() =\u003e controller.abort(), 1000); // cancel after 1 second\n\n        const result = await prediction.result();\n        console.info(result.stats.stopReason); // \"userStopped\"\n```\n\nBoth approaches halt generation immediately, and the returned stats indicate that the prediction ended because you stopped it.\n"])</script><script>self.__next_f.push([1,"144:{\"metadata\":\"$145\",\"prettyName\":\"Cancelling Predictions\",\"content\":\"$146\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/cancelling-predictions.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n148:{\"title\":\"Image Input\",\"description\":\"API for passing images as input to the model\",\"index\":4}\n149:T776,\nSome models, known as VLMs (Vision-Language Models), can accept images as input. You can pass images to the model using the `.respond()` method.\n\n### Prerequisite: Get a VLM (Vision-Language Model)\n\nIf you don't yet have a VLM, you can download a model like `qwen2-vl-2b-instruct` using the following command:\n\n```bash\nlms get qwen2-vl-2b-instruct\n```\n\n## 1. Instantiate the Model\n\nConnect to LM Studio and obtain a handle to the VLM (Vision-Language Model) you want to use.\n\n```lms_code_snippet\n  variants:\n    Example:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model(\"qwen2-vl-2b-instruct\");\n```\n\n## 2. Prepare the Image\n\nUse the `client.files.prepareImage()` method to get a handle to the image that can be subsequently passed to the model.\n\n```lms_code_snippet\n  variants:\n    Example:\n      language: typescript\n      code: |\n        const imagePath = \"/path/to/image.jpg\"; // Replace with the path to your image\n        const image = await client.files.prepareImage(imagePath);\n\n```\n\nIf you only have the image in the form of a base64 string, you can use the `client.files.prepareImageBase64()` method instead.\n\n```lms_code_snippet\n  variants:\n    Example:\n      language: typescript\n      code: |\n        const imageBase64 = \"Your base64 string here\";\n        const image = await client.files.prepareImageBase64(imageBase64);\n```\n\nThe LM Studio server supports JPEG, PNG, and WebP image formats.\n\n## 3. Pass the Image to the Model in `.respond()`\n\nGenerate a prediction by passing the image to the model in the `.respond()` method.\n\n```lms_code_snippet\n  variants:\n    Example:\n     "])</script><script>self.__next_f.push([1," language: typescript\n      code: |\n        const prediction = model.respond([\n          { role: \"user\", content: \"Describe this image please\", images: [image] },\n        ]);\n```\n147:{\"metadata\":\"$148\",\"prettyName\":\"Image Input\",\"content\":\"$149\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/image-input.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n14b:{\"title\":\"Structured Response\",\"description\":\"Enforce a structured response from the model using Pydantic (Python), Zod (TypeScript), or JSON Schema\",\"index\":4}\n14c:T15ac,"])</script><script>self.__next_f.push([1,"\nYou can enforce a particular response format from an LLM by providing a schema (JSON or `zod`) to the `.respond()` method. This guarantees that the model's output conforms to the schema you provide.\n\n## Enforce Using a `zod` Schema\n\nIf you wish the model to generate JSON that satisfies a given schema, it is recommended to provide\nthe schema using [`zod`](https://zod.dev/). When a `zod` schema is provided, the prediction result will contain an extra field `parsed`, which contains parsed, validated, and typed result.\n\n#### Define a `zod` Schema\n\n```ts\nimport { z } from \"zod\";\n\n// A zod schema for a book\nconst bookSchema = z.object({\n  title: z.string(),\n  author: z.string(),\n  year: z.number().int(),\n});\n```\n\n#### Generate a Structured Response\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        const result = await model.respond(\"Tell me about The Hobbit.\",\n          { structured: bookSchema },\n          maxTokens: 100, // Recommended to avoid getting stuck\n        );\n\n        const book = result.parsed;\n        console.info(book);\n        //           ^\n        // Note that `book` is now correctly typed as { title: string, author: string, year: number }\n\n    Streaming:\n      language: typescript\n      code: |\n        const prediction = model.respond(\"Tell me about The Hobbit.\",\n          { structured: bookSchema },\n          maxTokens: 100, // Recommended to avoid getting stuck\n        );\n\n        for await (const { content } of prediction) {\n          process.stdout.write(content);\n        }\n        process.stdout.write(\"\\n\");\n\n        // Get the final structured result\n        const result = await prediction.result();\n        const book = result.parsed;\n\n        console.info(book);\n        //           ^\n        // Note that `book` is now correctly typed as { title: string, author: string, year: number }\n```\n\n## Enforce Using a JSON Schema\n\nYou can also enforce a structured response using a JSON schema.\n\n#### Define a JSON Schema\n\n```ts\n// A JSON schema for a book\nconst schema = {\n  type: \"object\",\n  properties: {\n    title: { type: \"string\" },\n    author: { type: \"string\" },\n    year: { type: \"integer\" },\n  },\n  required: [\"title\", \"author\", \"year\"],\n};\n```\n\n#### Generate a Structured Response\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        const result = await model.respond(\"Tell me about The Hobbit.\", {\n          structured: {\n            type: \"json\",\n            jsonSchema: schema,\n          },\n          maxTokens: 100, // Recommended to avoid getting stuck\n        });\n\n        const book = JSON.parse(result.content);\n        console.info(book);\n    Streaming:\n      language: typescript\n      code: |\n        const prediction = model.respond(\"Tell me about The Hobbit.\", {\n          structured: {\n            type: \"json\",\n            jsonSchema: schema,\n          },\n          maxTokens: 100, // Recommended to avoid getting stuck\n        });\n\n        for await (const { content } of prediction) {\n          process.stdout.write(content);\n        }\n        process.stdout.write(\"\\n\");\n\n        const result = await prediction.result();\n        const book = JSON.parse(result.content);\n\n        console.info(\"Parsed\", book);\n```\n\n```lms_warning\nStructured generation works by constraining the model to only generate tokens that conform to the provided schema. This ensures valid output in normal cases, but comes with two important limitations:\n\n1. Models (especially smaller ones) may occasionally get stuck in an unclosed structure (like an open bracket), when they \"forget\" they are in such structure and cannot stop due to schema requirements. Thus, it is recommended to always include a `maxTokens` parameter to prevent infinite generation.\n\n2. Schema compliance is only guaranteed for complete, successful generations. If generation is interrupted (by cancellation, reaching the `maxTokens` limit, or other reasons), the output will likely violate the schema. With `zod` schema input, this will raise an error; with JSON schema, you'll receive an invalid string that doesn't satisfy schema.\n```\n\n\u003c!-- ## Overview\n\nOnce you have [downloaded and loaded](/docs/basics/index) a large language model,\nyou can use it to respond to input through the API. This article covers getting JSON structured output, but you can also\n[request text completions](/docs/api/sdk/completion),\n[request chat responses](/docs/api/sdk/chat-completion), and\n[use a vision-language model to chat about images](/docs/api/sdk/image-input).\n\n### Usage\n\nCertain models are trained to output valid JSON data that conforms to\na user-provided schema, which can be used programmatically in applications\nthat need structured data. This structured data format is supported by both\n[`complete`](/docs/api/sdk/completion) and [`respond`](/docs/api/sdk/chat-completion)\nmethods, and relies on Pydantic in Python and Zod in TypeScript.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        const Book = z.object({\n          title: z.string(),\n          author: z.string(),\n          year: z.number().int()\n        })\n\n        const client = new LMStudioClient();\n        const llm = await client.llm.model();\n\n        const response = await llm.respond(\n          \"Tell me about The Hobbit.\",\n          { structured: Book },\n        )\n\n        console.log(response.content.title)\n``` --\u003e\n"])</script><script>self.__next_f.push([1,"14a:{\"metadata\":\"$14b\",\"prettyName\":\"Structured Response\",\"content\":\"$14c\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/structured-response.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n14e:{\"title\":\"Speculative Decoding\",\"description\":\"API to use a draft model in speculative decoding in `lmstudio-js`\",\"index\":5}\n14f:T755,\nSpeculative decoding is a technique that can substantially increase the generation speed of large language models (LLMs) without reducing response quality. See [Speculative Decoding](./../../app/advanced/speculative-decoding) for more info.\n\nTo use speculative decoding in `lmstudio-js`, simply provide a `draftModel` parameter when performing the prediction. You do not need to load the draft model separately.\n\n```lms_code_snippet\n  variants:\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n\n        const mainModelKey = \"qwen2.5-7b-instruct\";\n        const draftModelKey = \"qwen2.5-0.5b-instruct\";\n\n        const model = await client.llm.model(mainModelKey);\n        const result = await model.respond(\"What are the prime numbers between 0 and 100?\", {\n          draftModel: draftModelKey,\n        });\n\n        const { content, stats } = result;\n        console.info(content);\n        console.info(`Accepted ${stats.acceptedDraftTokensCount}/${stats.predictedTokensCount} tokens`);\n\n\n    Streaming:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n\n        const mainModelKey = \"qwen2.5-7b-instruct\";\n        const draftModelKey = \"qwen2.5-0.5b-instruct\";\n\n        const model = await client.llm.model(mainModelKey);\n        const prediction = model.respond(\"What are the prime numbers between 0 and 100?\", {\n          draftModel: draftModelKey,\n        });\n\n        for await (const { content } of prediction) {\n          process.stdout.write(content);\n        }\n        process.stdout.write("])</script><script>self.__next_f.push([1,"\"\\n\");\n\n        const { stats } = await prediction.result();\n        console.info(`Accepted ${stats.acceptedDraftTokensCount}/${stats.predictedTokensCount} tokens`);\n```\n14d:{\"metadata\":\"$14e\",\"prettyName\":\"Speculative Decoding\",\"content\":\"$14f\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/speculative-decoding.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n151:{\"title\":\"Generate Completions\",\"description\":\"Provide a string input for the model to complete\",\"index\":6}\n152:T148b,"])</script><script>self.__next_f.push([1,"\nUse `llm.complete(...)` to generate text completions from a loaded language model. Text completions mean sending an non-formatted string to the model with the expectation that the model will complete the text.\n\nThis is different from multi-turn chat conversations. For more information on chat completions, see [Chat Completions](./chat-completion).\n\n## 1. Instantiate a Model\n\nFirst, you need to load a model to generate completions from. This can be done using the `model` method on the `llm` handle.\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n```\n\n## 2. Generate a Completion\n\nOnce you have a loaded model, you can generate completions by passing a string to the `complete` method on the `llm` handle.\n\n```lms_code_snippet\n  variants:\n    Streaming:\n      language: typescript\n      code: |\n        const completion = model.complete(\"My name is\", {\n          maxTokens: 100,\n        });\n\n        for await (const { content } of completion) {\n          process.stdout.write(content);\n        }\n\n        console.info(); // Write a new line for cosmetic purposes\n\n    \"Non-streaming\":\n      language: typescript\n      code: |\n        const completion = await model.complete(\"My name is\", {\n          maxTokens: 100,\n        });\n\n        console.info(completion.content);\n```\n\n## 3. Print Prediction Stats\n\nYou can also print prediction metadata, such as the model used for generation, number of generated tokens, time to first token, and stop reason.\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        console.info(\"Model used:\", completion.modelInfo.displayName);\n        console.info(\"Predicted tokens:\", completion.stats.predictedTokensCount);\n        console.info(\"Time to first token (seconds):\", completion.stats.timeToFirstTokenSec);\n        console.info(\"Stop reason:\", completion.stats.stopReason);\n```\n\n## Example: Get an LLM to Simulate a Terminal\n\nHere's an example of how you might use the `complete` method to simulate a terminal.\n\n```lms_code_snippet\n  title: \"terminal-sim.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        import { createInterface } from \"node:readline/promises\";\n\n        const rl = createInterface({ input: process.stdin, output: process.stdout });\n        const client = new LMStudioClient();\n        const model = await client.llm.model();\n        let history = \"\";\n\n        while (true) {\n          const command = await rl.question(\"$ \");\n          history += \"$ \" + command + \"\\n\";\n\n          const prediction = model.complete(history, { stopStrings: [\"$\"] });\n          for await (const { content } of prediction) {\n            process.stdout.write(content);\n          }\n          process.stdout.write(\"\\n\");\n\n          const { content } = await prediction.result();\n          history += content;\n        }\n```\n\n\u003c!-- ## Advanced Usage\n\n### Prediction metadata\n\nPrediction responses are really returned as `PredictionResult` objects that contain additional dot-accessible metadata about the inference request.\nThis entails info about the model used, the configuration with which it was loaded, and the configuration for this particular prediction. It also provides\ninference statistics like stop reason, time to first token, tokens per second, and number of generated tokens.\n\nPlease consult your specific SDK to see exact syntax.\n\n### Progress callbacks\n\nTODO: TS has onFirstToken callback which Python does not\n\nLong prompts will often take a long time to first token, i.e. it takes the model a long time to process your prompt.\nIf you want to get updates on the progress of this process, you can provide a float callback to `complete`\nthat receives a float from 0.0-1.0 representing prompt processing progress.\n\n```lms_code_snippet\n  variants:\n    Python:\n      language: python\n      code: |\n        import lmstudio as lm\n\n        llm = lm.llm()\n\n        completion = llm.complete(\n            \"My name is\",\n            on_progress: lambda progress: print(f\"{progress*100}% complete\")\n        )\n\n    Python (with scoped resources):\n      language: python\n      code: |\n        import lmstudio\n\n        with lmstudio.Client() as client:\n            llm = client.llm.model()\n\n            completion = llm.complete(\n                \"My name is\",\n                on_progress: lambda progress: print(f\"{progress*100}% processed\")\n            )\n\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const llm = await client.llm.model();\n\n        const prediction = llm.complete(\n          \"My name is\",\n          {onPromptProcessingProgress: (progress) =\u003e process.stdout.write(`${progress*100}% processed`)});\n```\n\n### Prediction configuration\n\nYou can also specify the same prediction configuration options as you could in the\nin-app chat window sidebar. Please consult your specific SDK to see exact syntax. --\u003e\n"])</script><script>self.__next_f.push([1,"150:{\"metadata\":\"$151\",\"prettyName\":\"Generate Completions\",\"content\":\"$152\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/completion.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n154:{\"title\":\"Configuring the Model\",\"description\":\"APIs for setting inference-time and load-time parameters for your model\",\"index\":null}\n155:T90d,"])</script><script>self.__next_f.push([1,"\nYou can customize both inference-time and load-time parameters for your model. Inference parameters can be set on a per-request basis, while load parameters are set when loading the model.\n\n# Inference Parameters\n\nSet inference-time parameters such as `temperature`, `maxTokens`, `topP` and more.\n\n```lms_code_snippet\n  variants:\n    \".respond()\":\n      language: typescript\n      code: |\n        const prediction = model.respond(chat, {\n          temperature: 0.6,\n          maxTokens: 50,\n        });\n    \".complete()\":\n        language: typescript\n        code: |\n          const prediction = model.complete(prompt, {\n            temperature: 0.6,\n            maxTokens: 50,\n            stop: [\"\\n\\n\"],\n          });\n```\n\nSee [`LLMPredictionConfigInput`](./../api-reference/llm-prediction-config-input) for all configurable fields.\n\nAnother useful inference-time configuration parameter is [`structured`](\u003c(./structured-responses)\u003e), which allows you to rigorously enforce the structure of the output using a JSON or zod schema.\n\n# Load Parameters\n\nSet load-time parameters such as the context length, GPU offload ratio, and more.\n\n### Set Load Parameters with `.model()`\n\nThe `.model()` retrieves a handle to a model that has already been loaded, or loads a new one on demand (JIT loading).\n\n**Note**: if the model is already loaded, the configuration will be **ignored**.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\", {\n          config: {\n            contextLength: 8192,\n            gpu: {\n              ratio: 0.5,\n            },\n          },\n        });\n```\n\nSee [`LLMLoadModelConfig`](./../api-reference/llm-load-model-config) for all configurable fields.\n\n### Set Load Parameters with `.load()`\n\nThe `.load()` method creates a new model instance and loads it with the specified configuration.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        const model = await client.llm.load(\"qwen2.5-7b-instruct\", {\n          config: {\n            contextLength: 8192,\n            gpu: {\n              ratio: 0.5,\n            },\n          },\n        });\n```\n\nSee [`LLMLoadModelConfig`](./../api-reference/llm-load-model-config) for all configurable fields.\n"])</script><script>self.__next_f.push([1,"153:{\"metadata\":\"$154\",\"prettyName\":\"Configuration Parameters\",\"content\":\"$155\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/parameters.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}\n13d:{\"chat-completion\":\"$13e\",\"working-with-chats\":\"$141\",\"cancelling-predictions\":\"$144\",\"image-input\":\"$147\",\"structured-response\":\"$14a\",\"speculative-decoding\":\"$14d\",\"completion\":\"$150\",\"parameters\":\"$153\"}\n158:{\"title\":\"The `.act()` call\",\"description\":\"How to use the `.act()` call to turn LLMs into autonomous agents that can perform tasks on your local machine.\",\"index\":1}\n159:T16f8,"])</script><script>self.__next_f.push([1,"\n## Automatic tool calling\n\nWe introduce the concept of execution \"rounds\" to describe the combined process of running a tool, providing its output to the LLM, and then waiting for the LLM to decide what to do next.\n\n**Execution Round**\n\n```\n â€¢ run a tool -\u003e\n â†‘   â€¢ provide the result to the LLM -\u003e\n â”‚       â€¢ wait for the LLM to generate a response\n â”‚\n â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””âž” (return)\n```\n\nA model might choose to run tools multiple times before returning a final result. For example, if the LLM is writing code, it might choose to compile or run the program, fix errors, and then run it again, rinse and repeat until it gets the desired result.\n\nWith this in mind, we say that the `.act()` API is an automatic \"multi-round\" tool calling API.\n\n\n### Quick Example\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient, tool } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        const client = new LMStudioClient();\n\n        const multiplyTool = tool({\n          name: \"multiply\",\n          description: \"Given two numbers a and b. Returns the product of them.\",\n          parameters: { a: z.number(), b: z.number() },\n          implementation: ({ a, b }) =\u003e a * b,\n        });\n\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n        await model.act(\"What is the result of 12345 multiplied by 54321?\", [multiplyTool], {\n          onMessage: (message) =\u003e console.info(message.toString()),\n        });\n```\n\n\u003e **_NOTE:_**  at this time, this code expects zod v3\n\n### What does it mean for an LLM to \"use a tool\"?\n\nLLMs are largely text-in, text-out programs. So, you may ask \"how can an LLM use a tool?\". The answer is that some LLMs are trained to ask the human to call the tool for them, and expect the tool output to to be provided back in some format.\n\nImagine you're giving computer support to someone over the phone. You might say things like \"run this command for me ... OK what did it output? ... OK now click there and tell me what it says ...\". In this case you're the LLM! And you're \"calling tools\" vicariously through the person on the other side of the line.\n\n\n### Important: Model Selection\n\nThe model selected for tool use will greatly impact performance.\n\nSome general guidance when selecting a model:\n\n- Not all models are capable of intelligent tool use\n- Bigger is better (i.e., a 7B parameter model will generally perform better than a 3B parameter model)\n- We've observed [Qwen2.5-7B-Instruct](https://model.lmstudio.ai/download/lmstudio-community/Qwen2.5-7B-Instruct-GGUF) to perform well in a wide variety of cases\n- This guidance may change\n\n### Example: Multiple Tools\n\nThe following code demonstrates how to provide multiple tools in a single `.act()` call.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient, tool } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        const client = new LMStudioClient();\n\n        const additionTool = tool({\n          name: \"add\",\n          description: \"Given two numbers a and b. Returns the sum of them.\",\n          parameters: { a: z.number(), b: z.number() },\n          implementation: ({ a, b }) =\u003e a + b,\n        });\n\n        const isPrimeTool = tool({\n          name: \"isPrime\",\n          description: \"Given a number n. Returns true if n is a prime number.\",\n          parameters: { n: z.number() },\n          implementation: ({ n }) =\u003e {\n            if (n \u003c 2) return false;\n            const sqrt = Math.sqrt(n);\n            for (let i = 2; i \u003c= sqrt; i++) {\n              if (n % i === 0) return false;\n            }\n            return true;\n          },\n        });\n\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n        await model.act(\n          \"Is the result of 12345 + 45668 a prime? Think step by step.\",\n          [additionTool, isPrimeTool],\n          { onMessage: (message) =\u003e console.info(message.toString()) },\n        );\n```\n\n### Example: Chat Loop with Create File Tool\n\nThe following code creates a conversation loop with an LLM agent that can create files.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { Chat, LMStudioClient, tool } from \"@lmstudio/sdk\";\n        import { existsSync } from \"fs\";\n        import { writeFile } from \"fs/promises\";\n        import { createInterface } from \"readline/promises\";\n        import { z } from \"zod\";\n\n        const rl = createInterface({ input: process.stdin, output: process.stdout });\n        const client = new LMStudioClient();\n        const model = await client.llm.model();\n        const chat = Chat.empty();\n\n        const createFileTool = tool({\n          name: \"createFile\",\n          description: \"Create a file with the given name and content.\",\n          parameters: { name: z.string(), content: z.string() },\n          implementation: async ({ name, content }) =\u003e {\n            if (existsSync(name)) {\n              return \"Error: File already exists.\";\n            }\n            await writeFile(name, content, \"utf-8\");\n            return \"File created.\";\n          },\n        });\n\n        while (true) {\n          const input = await rl.question(\"You: \");\n          // Append the user input to the chat\n          chat.append(\"user\", input);\n\n          process.stdout.write(\"Bot: \");\n          await model.act(chat, [createFileTool], {\n            // When the model finish the entire message, push it to the chat\n            onMessage: (message) =\u003e chat.append(message),\n            onPredictionFragment: ({ content }) =\u003e {\n              process.stdout.write(content);\n            },\n          });\n          process.stdout.write(\"\\n\");\n        }\n```\n"])</script><script>self.__next_f.push([1,"157:{\"metadata\":\"$158\",\"prettyName\":\"The `.act()` call\",\"content\":\"$159\",\"pageRelUrl\":\"2_typescript/3_agent/act.md\",\"sectionKey\":\"agent\",\"sectionPrettyName\":\"Agentic Flows\"}\n15b:{\"title\":\"Tool Definition\",\"description\":\"Define tools with the `tool()` function and pass them to the model in the `act()` call.\",\"index\":2}\n15c:Tad2,"])</script><script>self.__next_f.push([1,"\nYou can define tools with the `tool()` function and pass them to the model in the `act()` call.\n\n## Anatomy of a Tool\n\nFollow this standard format to define functions as tools:\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        const exampleTool = tool({\n          // The name of the tool\n          name: \"add\",\n\n          // A description of the tool\n          description: \"Given two numbers a and b. Returns the sum of them.\",\n\n          // zod schema of the parameters\n          parameters: { a: z.number(), b: z.number() },\n\n          // The implementation of the tool. Just a regular function.\n          implementation: ({ a, b }) =\u003e a + b,\n        });\n```\n\n**Important**: The tool name, description, and the parameter definitions are all passed to the model!\n\nThis means that your wording will affect the quality of the generation. Make sure to always provide a clear description of the tool so the model knows how to use it.\n\n## Tools with External Effects (like Computer Use or API Calls)\n\nTools can also have external effects, such as creating files or calling programs and even APIs. By implementing tools with external effects, you\ncan essentially turn your LLMs into autonomous agents that can perform tasks on your local machine.\n\n## Example: `createFileTool`\n\n### Tool Definition\n\n```lms_code_snippet\n  title: \"createFileTool.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool } from \"@lmstudio/sdk\";\n        import { existsSync } from \"fs\";\n        import { writeFile } from \"fs/promises\";\n        import { z } from \"zod\";\n\n        const createFileTool = tool({\n          name: \"createFile\",\n          description: \"Create a file with the given name and content.\",\n          parameters: { name: z.string(), content: z.string() },\n          implementation: async ({ name, content }) =\u003e {\n            if (existsSync(name)) {\n              return \"Error: File already exists.\";\n            }\n            await writeFile(name, content, \"utf-8\");\n            return \"File created.\";\n          },\n        });\n```\n\n### Example code using the `createFile` tool:\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        import { createFileTool } from \"./createFileTool\";\n\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model(\"qwen2.5-7b-instruct\");\n        await model.act(\n          \"Please create a file named output.txt with your understanding of the meaning of life.\",\n          [createFileTool],\n        );\n```\n"])</script><script>self.__next_f.push([1,"15a:{\"metadata\":\"$15b\",\"prettyName\":\"Tool Definition\",\"content\":\"$15c\",\"pageRelUrl\":\"2_typescript/3_agent/tools.md\",\"sectionKey\":\"agent\",\"sectionPrettyName\":\"Agentic Flows\"}\n156:{\"act\":\"$157\",\"tools\":\"$15a\"}\n15f:{\"title\":\"Introduction to Plugins\",\"description\":\"A brief introduction to making plugins for LM Studio using TypeScript.\",\"index\":1}\n160:T71d,\nPlugins extend LM Studio's functionality by providing \"hook functions\" that execute at specific points during operation.\n\nPlugins are currently written in JavaScript/TypeScript and run on Node.js v20.18.0. Python support is in development.\n\n## Getting Started\n\nLM Studio includes Node.js, so no separate installation is required.\n\n### Create a new plugin\n\nTo create a new plugin, navigate to LM Studio... [TO BE CONTINUED]\n\n### Run a plugin in development mode\n\nOnce you've created a plugin, run this command in the plugin directory to start development mode:\n\n```bash\nlms dev\n```\n\nYour plugin will appear in LM Studio's plugin list. Development mode automatically rebuilds and reloads your plugin when you make code changes.\n\nYou only need `lms dev` during development. When the plugin is installed, LM Studio automatically runs them as needed. Learn more about distributing and installing plugins in the [Sharing Plugins](./plugins/sharing) section.\n\n## Next Steps\n\n- [Tools Providers](./plugins/tools-provider)\n\n  Give models extra capabilities by creating tools they can use during generation, like accessing external APIs or performing calculations.\n\n- [Prompt Preprocessors](./plugins/prompt-preprocessor)\n\n  Modify user input before it reaches the model - handle file uploads, inject context, or transform queries.\n\n- [Generators](./plugins/generator)\n\n  Create custom text generation sources that replace the local model, perfect for online model adapters.\n\n- [Custom Configurations](./plugins/custom-configuration)\n\n  Add configuration UIs so users can customize your plugin's behavior.\n\n- [Third-Party Dependencies](./plugins/dependencies)\n\n  Use npm packages to leverage existing li"])</script><script>self.__next_f.push([1,"braries in your plugins.\n\n- [Sharing Plugins](./plugins/publish-plugins)\n\n  Package and share your plugins with the community.\n15e:{\"metadata\":\"$15f\",\"prettyName\":\"Introduction to Plugins\",\"content\":\"$160\",\"pageRelUrl\":\"2_typescript/3_plugins/index.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n162:{\"title\":\"Using `npm` Dependencies\",\"description\":\"How to use npm packages in LM Studio plugins\",\"index\":6}\n161:{\"metadata\":\"$162\",\"prettyName\":\"Using `npm` Dependencies\",\"content\":\"\\n## Add dependencies to your plugin with `npm`\\n\\nLM Studio plugins supports `npm` packages. You can just install them using `npm install`.\\n\\nWhen the plugin is installed, LM Studio will automatically download all the required dependencies that are declared in `package.json` and `package-lock.json`. (The user does not need to have Node.js/npm installed.)\\n\\n### `postinstall` scripts\\n\\nFor safety reasons, we do **not** run `postinstall` scripts. Thus please make sure you are not using any npm packages that require postinstall scripts to work.\\n\\n## Using Other Package Managers\\n\\nSince we rely on `package-lock.json`, lock files produced by other package managers will not work. Thus we recommend only using `npm` when developing LM Studio plugins.\\n\",\"pageRelUrl\":\"2_typescript/3_plugins/dependencies.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n165:{\"title\":\"Introduction to Tools Provider\",\"description\":\"Writing tools providers for LM Studio plugins using TypeScript\",\"index\":1}\n164:{\"metadata\":\"$165\",\"prettyName\":\"Introduction to Tools Provider\",\"content\":\"\\nTools provider is a function that returns an array of tools that the model can use during generation.\\n\\n## Examples\\n\\nThe following are some plugins that make use of tools providers:\\n\\n- [lmstudio/wikipedia](https://lmstudio.ai/lmstudio/wikipedia)\\n\\n  Gives the LLM tools to search and read Wikipedia articles.\\n\\n- [lmstudio/js-code-sandbox](https://lmstudio.ai/lmstudio/js-code-sandbox)\\n\\n  Gives the LLM tools to run JavaScript/TypeScript code in"])</script><script>self.__next_f.push([1," a sandbox environment using [deno](https://deno.com/).\\n\\n- [lmstudio/dice](https://lmstudio.ai/lmstudio/dice)\\n\\n  Allows the LLM to generate random numbers using \\\"dice\\\".\\n\",\"pageRelUrl\":\"2_typescript/3_plugins/1_tools-provider/index.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n167:{\"title\":\"Single Tool\",\"description\":\"\",\"index\":3}\n168:Tb20,"])</script><script>self.__next_f.push([1,"\nTo setup a tools provider, first create the a file `toolsProvider.ts` in your plugin's `src` directory:\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool, Tool, ToolsProviderController } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n        import { existsSync } from \"fs\";\n        import { writeFile } from \"fs/promises\";\n        import { join } from \"path\";\n\n        export async function toolsProvider(ctl: ToolsProviderController) {\n          const tools: Tool[] = [];\n\n          const createFileTool = tool({\n            // Name of the tool, this will be passed to the model. Aim for concise, descriptive names\n            name: `create_file`,\n            // Your description here, more details will help the model to understand when to use the tool\n            description: \"Create a file with the given name and content.\",\n            parameters: { file_name: z.string(), content: z.string() },\n            implementation: async ({ file_name, content }) =\u003e {\n              const filePath = join(ctl.getWorkingDirectory(), file_name);\n              if (existsSync(filePath)) {\n                return \"Error: File already exists.\";\n              }\n              await writeFile(filePath, content, \"utf-8\");\n              return \"File created.\";\n            },\n          });\n          tools.push(createFileTool);\n\n          return tools;\n        }\n```\n\nThe above tools provider defines a single tool called `create_file` that allows the model to create a file with a specified name and content inside the working directory. You can learn more about defining tools in [Tool Definition](../agent/tools).\n\nThen register the tools provider in your plugin's `index.ts`:\n\n```lms_code_snippet\n  title: \"src/index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        // ... other imports ...\n        import { toolsProvider } from \"./toolsProvider\";\n\n        export async function main(context: PluginContext) {\n          // ... other plugin setup code ...\n\n          // Register the tools provider.\n          context.withToolsProvider(toolsProvider); // \u003c-- Register the tools provider\n\n          // ... other plugin setup code ...\n        }\n```\n\nNow, you can try to ask the LLM to create a file, and it should be able to do so using the tool you just created.\n\n## Tips\n\n- **Use Descriptive Names and Descriptions**: When defining tools, use descriptive names and detailed descriptions. This helps the model understand when and how to use each tool effectively.\n- **Return Errors as Strings**: Sometimes, the model may make a mistake when calling a tool. In such cases, you can return an error message as a string. In most cases, the model will try to correct itself and call the tool again with the correct parameters.\n"])</script><script>self.__next_f.push([1,"166:{\"metadata\":\"$167\",\"prettyName\":\"Single Tool\",\"content\":\"$168\",\"pageRelUrl\":\"2_typescript/3_plugins/1_tools-provider/single-tool.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n16a:{\"title\":\"Multiple Tools\",\"description\":\"\",\"index\":4}\n16b:T7ea,\nA tools provider can define multiple tools for the model to use. Simply create additional tool instances and add them to the tools array.\n\nIn the example below, we add a second tool to read the content of a file:\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool, Tool, ToolsProviderController } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n        import { existsSync } from \"fs\";\n        import { readFile, writeFile } from \"fs/promises\";\n        import { join } from \"path\";\n\n        export async function toolsProvider(ctl: ToolsProviderController) {\n          const tools: Tool[] = [];\n\n          const createFileTool = tool({\n            name: `create_file`,\n            description: \"Create a file with the given name and content.\",\n            parameters: { file_name: z.string(), content: z.string() },\n            implementation: async ({ file_name, content }) =\u003e {\n              const filePath = join(ctl.getWorkingDirectory(), file_name);\n              if (existsSync(filePath)) {\n                return \"Error: File already exists.\";\n              }\n              await writeFile(filePath, content, \"utf-8\");\n              return \"File created.\";\n            },\n          });\n          tools.push(createFileTool); // First tool\n\n          const readFileTool = tool({\n            name: `read_file`,\n            description: \"Read the content of a file with the given name.\",\n            parameters: { file_name: z.string() },\n            implementation: async ({ file_name }) =\u003e {\n              const filePath = join(ctl.getWorkingDirectory(), file_name);\n              if (!existsSync(filePath)) {\n                return \"Error: File does not exist.\";\n             "])</script><script>self.__next_f.push([1," }\n              const content = await readFile(filePath, \"utf-8\");\n              return content;\n            },\n          });\n          tools.push(readFileTool); // Second tool\n\n          return tools; // Return the tools array\n        }\n```\n169:{\"metadata\":\"$16a\",\"prettyName\":\"Multiple Tools\",\"content\":\"$16b\",\"pageRelUrl\":\"2_typescript/3_plugins/1_tools-provider/multiple-tools.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n16d:{\"title\":\"Custom Configuration\",\"description\":\"Add custom configuration options to your tools provider\",\"index\":5}\n16e:Tb4a,"])</script><script>self.__next_f.push([1,"\nYou can add custom configuration options to your tools provider, so the user of plugin can customize the behavior without modifying the code.\n\nIn the example below, we will ask the user to specify a folder name, and we will create files inside that folder within the working directory.\n\nFirst, add the config field to `config.ts`:\n\n```lms_code_snippet\n  title: \"src/config.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        export const configSchematics = createConfigSchematics()\n          .field(\n            \"folderName\", // Key of the configuration field\n            \"string\", // Type of the configuration field\n            {\n              displayName: \"Folder Name\",\n              subtitle: \"The name of the folder where files will be created.\",\n            },\n            \"default_folder\", // Default value\n          )\n          .build();\n```\n\n```lms_info\nIn this example, we added the field to `configSchematics`, which is the \"per-chat\" configuration. If you want to add a global configuration field that is shared across different chats, you should add it under the section `globalConfigSchematics` in the same file.\n\nLearn more about configurations in [Custom Configurations](../plugins/configurations).\n```\n\nThen, modify the tools provider to use the configuration value:\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool, Tool, ToolsProviderController } from \"@lmstudio/sdk\";\n        import { existsSync } from \"fs\";\n        import { mkdir, writeFile } from \"fs/promises\";\n        import { join } from \"path\";\n        import { z } from \"zod\";\n        import { configSchematics } from \"./config\";\n\n        export async function toolsProvider(ctl: ToolsProviderController) {\n          const tools: Tool[] = [];\n\n          const createFileTool = tool({\n            name: `create_file`,\n            description: \"Create a file with the given name and content.\",\n            parameters: { file_name: z.string(), content: z.string() },\n            implementation: async ({ file_name, content }) =\u003e {\n              // Read the config field\n              const folderName = ctl.getPluginConfig(configSchematics).get(\"folderName\");\n              const folderPath = join(ctl.getWorkingDirectory(), folderName);\n\n              // Ensure the folder exists\n              await mkdir(folderPath, { recursive: true });\n\n              // Create the file\n              const filePath = join(folderPath, file_name);\n              if (existsSync(filePath)) {\n                return \"Error: File already exists.\";\n              }\n              await writeFile(filePath, content, \"utf-8\");\n              return \"File created.\";\n            },\n          });\n          tools.push(createFileTool); // First tool\n\n          return tools; // Return the tools array\n        }\n```\n"])</script><script>self.__next_f.push([1,"16c:{\"metadata\":\"$16d\",\"prettyName\":\"Custom Configuration\",\"content\":\"$16e\",\"pageRelUrl\":\"2_typescript/3_plugins/1_tools-provider/custom-configuration.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n170:{\"title\":\"Status Reports \u0026 Warnings\",\"description\":\"\",\"index\":6}\n171:Td5b,"])</script><script>self.__next_f.push([1,"\nSometimes, a tool may take a long time to execute. In such cases, it will be helpful to provide status updates, so the user knows what is happening. In order times, you may want to warn the user about potential issues.\n\nYou can use `status` and `warn` methods on the second parameter of the tool's implementation function to send status updates and warnings.\n\nThe following example shows how to implement a tool that waits for a specified number of seconds, providing status updates and warnings if the wait time exceeds 10 seconds:\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool, Tool, ToolsProviderController } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        export async function toolsProvider(ctl: ToolsProviderController) {\n          const tools: Tool[] = [];\n\n          const waitTool = tool({\n            name: `wait`,\n            description: \"Wait for a specified number of seconds.\",\n            parameters: { seconds: z.number().min(1) },\n            implementation: async ({ seconds }, { status, warn }) =\u003e {\n              if (seconds \u003e 10) {\n                warn(\"The model asks to wait for more than 10 seconds.\");\n              }\n              for (let i = 0; i \u003c seconds; i++) {\n                status(`Waiting... ${i + 1}/${seconds} seconds`);\n                await new Promise((resolve) =\u003e setTimeout(resolve, 1000));\n              }\n            },\n          });\n          tools.push(waitTool);\n\n          return tools; // Return the tools array\n        }\n```\n\nNote status updates and warnings are only visible to the user. If you want the model to also see those messages, you should return them as part of the tool's return value.\n\n## Handling Aborts\n\nA prediction may be aborted by the user while your tool is still running. In such cases, you should handle the abort gracefully by handling the `AbortSignal` object passed as the second parameter to the tool's implementation function.\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool, Tool, ToolsProviderController } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        export async function toolsProvider(ctl: ToolsProviderController) {\n          const tools: Tool[] = [];\n\n          const fetchTool = tool({\n            name: `fetch`,\n            description: \"Fetch a URL using GET method.\",\n            parameters: { url: z.string() },\n            implementation: async ({ url }, { signal }) =\u003e {\n              const response = await fetch(url, {\n                method: \"GET\",\n                signal, // \u003c-- Here, we pass the signal to fetch to allow cancellation\n              });\n              if (!response.ok) {\n                return `Error: Failed to fetch ${url}: ${response.statusText}`;\n              }\n              const data = await response.text();\n              return {\n                status: response.status,\n                headers: Object.fromEntries(response.headers.entries()),\n                data: data.substring(0, 1000), // Limit to 1000 characters\n              };\n            },\n          });\n          tools.push(fetchTool);\n\n          return tools;\n        }\n```\n\nYou can learn more about `AbortSignal` in the [MDN documentation](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal).\n"])</script><script>self.__next_f.push([1,"16f:{\"metadata\":\"$170\",\"prettyName\":\"Status Reports \u0026 Warnings\",\"content\":\"$171\",\"pageRelUrl\":\"2_typescript/3_plugins/1_tools-provider/status-reports-and-warnings.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n173:{\"title\":\"Handling Aborts\",\"description\":\"Gracefully handle user-aborted tool executions in your tools provider\",\"index\":7}\n174:T662,\nA prediction may be aborted by the user while your tool is still running. In such cases, you should handle the abort gracefully by handling the `AbortSignal` object passed as the second parameter to the tool's implementation function.\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { tool, Tool, ToolsProviderController } from \"@lmstudio/sdk\";\n        import { z } from \"zod\";\n\n        export async function toolsProvider(ctl: ToolsProviderController) {\n          const tools: Tool[] = [];\n\n          const fetchTool = tool({\n            name: `fetch`,\n            description: \"Fetch a URL using GET method.\",\n            parameters: { url: z.string() },\n            implementation: async ({ url }, { signal }) =\u003e {\n              const response = await fetch(url, {\n                method: \"GET\",\n                signal, // \u003c-- Here, we pass the signal to fetch to allow cancellation\n              });\n              if (!response.ok) {\n                return `Error: Failed to fetch ${url}: ${response.statusText}`;\n              }\n              const data = await response.text();\n              return {\n                status: response.status,\n                headers: Object.fromEntries(response.headers.entries()),\n                data: data.substring(0, 1000), // Limit to 1000 characters\n              };\n            },\n          });\n          tools.push(fetchTool);\n\n          return tools;\n        }\n```\n\nYou can learn more about `AbortSignal` in the [MDN documentation](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal).\n172:{\"metadata\":\"$173\",\"prettyName\":\"Handling Abo"])</script><script>self.__next_f.push([1,"rts\",\"content\":\"$174\",\"pageRelUrl\":\"2_typescript/3_plugins/1_tools-provider/handling-aborts.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n163:{\"\":\"$164\",\"single-tool\":\"$166\",\"multiple-tools\":\"$169\",\"custom-configuration\":\"$16c\",\"status-reports-and-warnings\":\"$16f\",\"handling-aborts\":\"$172\"}\n177:{\"title\":\"Introduction\",\"description\":\"Writing prompt preprocessors for LM Studio plugins using TypeScript\",\"index\":1}\n178:T456,\nPrompt Preprocessor is a function that is called upon the user hitting the \"Send\" button. It receives the user input and can modify it before it reaches the model. If multiple prompt preprocessors are registered, they will be chained together, with each one receiving the output of the previous one.\n\nThe modified result will be saved in the chat history, meaning that even if your plugin is disabled afterwards, the modified input will still be used.\n\nPrompt preprocessors will only be triggered for the current user input. It will not be triggered for previous messages in the chat history even if they were not preprocessed.\n\nPrompt preprocessors takes in a `ctl` object for controlling the preprocessing and a `userMessage` it needs to preprocess. It returns either a string or a message object which will replace the user message.\n\n### Examples\n\nThe following are some plugins that make use of prompt preprocessors:\n\n- [lmstudio/rag-v1](https://lmstudio.ai/lmstudio/rag-v1)\n\n  Retrieval Augmented Generation (RAG) for LM Studio. This is the plugin that gives document handling capabilities to LM Studio.\n176:{\"metadata\":\"$177\",\"prettyName\":\"Introduction\",\"content\":\"$178\",\"pageRelUrl\":\"2_typescript/3_plugins/2_prompt-preprocessor/index.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n17a:{\"title\":\"Examples\",\"description\":\"\",\"index\":2}\n17b:T5ce,\n### Example: Inject Current Time\n\nThe following is an example preprocessor that injects the current time before each user message.\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typ"])</script><script>self.__next_f.push([1,"escript\n      code: |\n        import { type PromptPreprocessorController, type ChatMessage } from \"@lmstudio/sdk\";\n        export async function preprocess(ctl: PromptPreprocessorController, userMessage: ChatMessage) {\n          const textContent = userMessage.getText();\n          const transformed = `Current time: ${new Date().toString()}\\n\\n${textContent}`;\n          return transformed;\n        }\n```\n\n### Example: Replace Trigger Words\n\nAnother example you can do it with simple text only processing is by replacing certain trigger words. For example, you can replace a `@init` trigger with a special initialization message.\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { type PromptPreprocessorController, type ChatMessage, text } from \"@lmstudio/sdk\";\n\n        const mySpecialInstructions = text`\n          Here are some special instructions...\n        `;\n\n        export async function preprocess(ctl: PromptPreprocessorController, userMessage: ChatMessage) {\n          const textContent = userMessage.getText();\n          const transformed = textContent.replaceAll(\"@init\", mySpecialInstructions);\n          return transformed;\n        }\n```\n179:{\"metadata\":\"$17a\",\"prettyName\":\"Examples\",\"content\":\"$17b\",\"pageRelUrl\":\"2_typescript/3_plugins/2_prompt-preprocessor/examples.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n17d:{\"title\":\"Custom Configuration\",\"description\":\"\",\"index\":3}\n17e:T93b,"])</script><script>self.__next_f.push([1,"\nYou can access custom configurations via `ctl.getPluginConfig` and `ctl.getGlobalPluginConfig`. See [Custom Configurations](./configurations) for more details.\n\nThe following is an example of how you can make the `specialInstructions` and `triggerWord` configurable:\n\nFirst, add the config field to `config.ts`:\n\n```lms_code_snippet\n  title: \"src/config.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { createConfigSchematics } from \"@lmstudio/sdk\";\n        export const configSchematics = createConfigSchematics()\n          .field(\n            \"specialInstructions\",\n            \"string\",\n            {\n              displayName: \"Special Instructions\",\n              subtitle: \"Special instructions to be injected when the trigger word is found.\",\n            },\n            \"Here is some default special instructions.\",\n          )\n          .field(\n            \"triggerWord\",\n            \"string\",\n            {\n              displayName: \"Trigger Word\",\n              subtitle: \"The word that will trigger the special instructions.\",\n            },\n            \"@init\",\n          )\n          .build();\n```\n\n```lms_info\nIn this example, we added the field to `configSchematics`, which is the \"per-chat\" configuration. If you want to add a global configuration field that is shared across different chats, you should add it under the section `globalConfigSchematics` in the same file.\n\nLearn more about configurations in [Custom Configurations](../plugins/configurations).\n```\n\nThen, modify the prompt preprocessor to use the configuration:\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { type PromptPreprocessorController, type ChatMessage } from \"@lmstudio/sdk\";\n        import { configSchematics } from \"./config\";\n\n        export async function preprocess(ctl: PromptPreprocessorController, userMessage: ChatMessage) {\n          const textContent = userMessage.getText();\n          const pluginConfig = ctl.getPluginConfig(configSchematics);\n\n          const triggerWord = pluginConfig.get(\"triggerWord\");\n          const specialInstructions = pluginConfig.get(\"specialInstructions\");\n\n          const transformed = textContent.replaceAll(triggerWord, specialInstructions);\n          return transformed;\n        }\n```\n"])</script><script>self.__next_f.push([1,"17c:{\"metadata\":\"$17d\",\"prettyName\":\"Custom Configuration\",\"content\":\"$17e\",\"pageRelUrl\":\"2_typescript/3_plugins/2_prompt-preprocessor/custom-configuration.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n180:{\"title\":\"Custom Status Report\",\"description\":\"\",\"index\":4}\n181:T459,\nDepending on the task, the prompt preprocessor may take some time to complete, for example, it may need to fetch some data from the internet or perform some heavy computation. In such cases, you can report the status of the preprocessing using `ctl.setStatus`.\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        const status = ctl.createStatus({\n          status: \"loading\",\n          text: \"Preprocessing.\",\n        });\n```\n\nYou can update the status at any time by calling `status.setState`.\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        status.setState({\n          status: \"done\",\n          text: \"Preprocessing done.\",\n        })\n```\n\nYou can even add sub status to the status:\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        const subStatus = status.addSubStatus({\n          status: \"loading\",\n          text: \"I am a sub status.\"\n        });\n```\n17f:{\"metadata\":\"$180\",\"prettyName\":\"Custom Status Report\",\"content\":\"$181\",\"pageRelUrl\":\"2_typescript/3_plugins/2_prompt-preprocessor/custom-status-report.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n183:{\"title\":\"Handling Aborts\",\"description\":\"\",\"index\":5}\n182:{\"metadata\":\"$183\",\"prettyName\":\"Handling Aborts\",\"content\":\"\\nA prediction may be aborted by the user while your generator is still running. In such cases, you should handle the abort gracefully by handling the `ctl.abortSignal`.\\n\\nYou can learn more about `AbortSignal` in the [MDN documentation](https://developer.mozilla.org/en-US/docs/Web/API/AbortSig"])</script><script>self.__next_f.push([1,"nal).\\n\",\"pageRelUrl\":\"2_typescript/3_plugins/2_prompt-preprocessor/handling-aborts.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n175:{\"\":\"$176\",\"examples\":\"$179\",\"custom-configuration\":\"$17c\",\"custom-status-report\":\"$17f\",\"handling-aborts\":\"$182\"}\n186:{\"title\":\"Introduction\",\"description\":\"Writing generators for LM Studio plugins using TypeScript\",\"index\":1}\n187:T408,\nGenerators are replacement for local LLMs. They act like a token source. When a plugin with a generator is used, LM Studio will no longer use the local model to generate text. The generator will be used instead.\n\nGenerators are useful for implementing adapters for external models, such as using a remote LM Studio instance or other online models.\n\nIf a plugin contains a generator, it will no longer show up in the plugins list. Instead, it will show up in the model dropdown and act as a model. If your plugins contains [Tools Provider](./tools-providers.md) or [Prompt Preprocessor](./prompt-preprocessors.md), they will be used when your generator is being selected.\n\n## Examples\n\nThe following are some plugins that make use of generators:\n\n- [lmstudio/remote-lmstudio](https://lmstudio.ai/lmstudio/remote-lmstudio)\n\n  Basic support for using a remote LM Studio instance to generate text.\n\n- [lmstudio/openai-compat-endpoint](https://lmstudio.ai/lmstudio/openai-compat-endpoint)\n\n  Use any OpenAI-compatible API in LM Studio.\n185:{\"metadata\":\"$186\",\"prettyName\":\"Introduction\",\"content\":\"$187\",\"pageRelUrl\":\"2_typescript/3_plugins/3_generator/index.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n189:{\"title\":\"Text-only Generators\",\"description\":\"\",\"index\":2}\n18a:T63a,\nGenerators take in the the generator controller and the current conversation state, start the generation, and then report the generated text using the `ctl.fragmentGenerated` method.\n\nThe following is an example of a simple generator that echos back the last user message with 200 ms delay between each word:\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  "])</script><script>self.__next_f.push([1,"variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { Chat, GeneratorController } from \"@lmstudio/sdk\";\n\n        export async function generate(ctl: GeneratorController, chat: Chat) {\n          // Just echo back the last message\n          const lastMessage = chat.at(-1).getText();\n          // Split the last message into words\n          const words = lastMessage.split(/(?= )/);\n          for (const word of words) {\n            ctl.fragmentGenerated(word); // Send each word as a fragment\n            ctl.abortSignal.throwIfAborted(); // Allow for cancellation\n            await new Promise((resolve) =\u003e setTimeout(resolve, 200)); // Simulate some processing time\n          }\n        }\n```\n\n## Custom Configurations\n\nYou can access custom configurations via `ctl.getPluginConfig` and `ctl.getGlobalPluginConfig`. See [Custom Configurations](./configurations) for more details.\n\n## Handling Aborts\n\nA prediction may be aborted by the user while your generator is still running. In such cases, you should handle the abort gracefully by handling the `ctl.abortSignal`.\n\nYou can learn more about `AbortSignal` in the [MDN documentation](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal).\n188:{\"metadata\":\"$189\",\"prettyName\":\"Text-only Generators\",\"content\":\"$18a\",\"pageRelUrl\":\"2_typescript/3_plugins/3_generator/text-only-generators.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n18c:{\"title\":\"Tool calling generators\",\"description\":\"\",\"index\":3}\n18d:Tba3,"])</script><script>self.__next_f.push([1,"\nTo enable tool use, it is slightly more involved. To see a comprehensive example that adapts OpenAI API, see the [openai-compat-endpoint plugin](https://lmstudio.ai/lmstudio/openai-compat-endpoint).\n\nYou can read the definition of tools available using `ctl.getToolDefinitions()`. For example, if you are making an online model adapter, you need to pass the tool definition to the model.\n\nOnce the model starts to make tool calls, you need to tell LM Studio about those calls.\n\nUse `ctl.toolCallGenerationStarted` to report the start of a tool call generation (i.e. the model starts to generate a tool call).\n\nUse `ctl.toolCallGenerationEnded` to report a successful generation of a tool call or use `ctl.toolCallGenerationFailed` to report a failed generation of a tool call.\n\nOptionally, you can also `ctl.toolCallGenerationNameReceived` to eagerly report the name of the tool being called once that is available. You can also use `ctl.toolCallGenerationArgumentFragmentGenerated` to report fragments of the tool call arguments as they are generated. These two methods are useful for providing better user experience, but are not strictly necessary.\n\nOverall, your generator must call these ctl methods in the following order:\n\n1. 0 - N calls to `ctl.fragmentGenerated` to report the generated non-tool-call text fragments.\n2. For each tool call:\n   1. Call `ctl.toolCallGenerationStarted` to indicate the start of a tool call generation.\n   2. (Optionally) Call `ctl.toolCallGenerationNameReceived` to report the name of the tool being called.\n   3. (Optionally) Call any times of `ctl.toolCallGenerationArgumentFragmentGenerated` to report the generated fragments of the tool call arguments.\n   4. Call either `ctl.toolCallGenerationEnded` to report a successful generation of the tool call or `ctl.toolCallGenerationFailed` to report a failed generation of the tool call.\n   5. If the model generates more text between/after the tool call, 0 - N calls to `ctl.fragmentGenerated` to report the generated non-tool-call text fragments. (This should not happen normally, but it is technically possible for some smaller models to do this. **Critically: this is not the same as model receiving the tool results and continuing the conversation. This is just model refusing to stop talking after made a tool request - the tool result is not available to the model yet.** When multi-round prediction happens, i.e. the model actually receives the tool call, your generator function will be called again with the updated conversation state.)\n\nSome API formats may report the tool name together with the beginning of the tool call generation, in which case you can call `ctl.toolCallGenerationNameReceived` immediately after `ctl.toolCallGenerationStarted`.\n\nSome API formats may not have incremental tool call updates (i.e. the entire tool call request is given at once), in which case you can just call `ctl.toolCallGenerationStarted` then immediately `ctl.toolCallGenerationEnded`.\n"])</script><script>self.__next_f.push([1,"18b:{\"metadata\":\"$18c\",\"prettyName\":\"Tool calling generators\",\"content\":\"$18d\",\"pageRelUrl\":\"2_typescript/3_plugins/3_generator/tool-calling-generators.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n184:{\"\":\"$185\",\"text-only-generators\":\"$188\",\"tool-calling-generators\":\"$18b\"}\n190:{\"title\":\"Introduction\",\"description\":\"Add custom configurations to LM Studio plugins using TypeScript\",\"index\":1}\n191:T641,\nLM Studio plugins support custom configurations. That is, you can define a configuration schema and LM Studio will present a UI to the user so they can configure your plugin without having to edit any code.\n\nThere are two types of configurations:\n\n- **Per-chat configuration**: tied to a specific chat. Different chats can have different configurations. Most configurations that affects the behavior of the plugin should be of this type.\n- **Global configuration**: apply to _all_ chats and are shared across the application. This is useful for global settings such as API keys.\n\n## Types of Configurations\n\nYou can define configurations in TypeScript using the `createConfigSchematics` function from the `@lmstudio/sdk` package. This function allows you to define fields with various types and options.\n\nSupported types include:\n\n- `string`: A text input field.\n- `numeric`: A number input field with optional validation and slider UI.\n- `boolean`: A checkbox or toggle input field.\n- `stringArray`: An array of string values with configurable constraints.\n- `select`: A dropdown selection field with predefined options.\n\nSee the [Defining New Fields](./custom-configuration/defining-new-fields) section for more details on how to define these fields.\n\n## Examples\n\nThe following are some plugins that make use of custom configurations\n\n- [lmstudio/wikipedia](https://lmstudio.ai/lmstudio/wikipedia)\n\n  Gives the LLM tools to search and read Wikipedia articles.\n\n- [lmstudio/openai-compat-endpoint](https://lmstudio.ai/lmstudio/openai-compat-endpoint)\n\n  Use any OpenAI-compatible API in LM Studio.\n18f:{\"metadata\":\"$190\","])</script><script>self.__next_f.push([1,"\"prettyName\":\"Introduction\",\"content\":\"$191\",\"pageRelUrl\":\"2_typescript/3_plugins/4_custom-configuration/index.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n193:{\"title\":\"`config.ts` File\",\"description\":\"\",\"index\":2}\n194:Ta12,"])</script><script>self.__next_f.push([1,"\nBy default, the plugin scaffold will create a `config.ts` file in the `src/` directory which will contain the schematics of the configurations. If the files does not exist, you can create it manually:\n\n```lms_code_snippet\n  title: \"src/toolsProvider.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { createConfigSchematics } from \"@lmstudio/sdk\";\n\n        export const configSchematics = createConfigSchematics()\n          .field(\n            \"myCustomField\", // The key of the field.\n            \"numeric\", // Type of the field.\n            // Options for the field. Different field types will have different options.\n            {\n              displayName: \"My Custom Field\",\n              hint: \"This is my custom field. Doesn't do anything special.\",\n              slider: { min: 0, max: 100, step: 1 }, // Add a slider to the field.\n            },\n            80, // Default Value\n          )\n          // You can add more fields by chaining the field method.\n          // For example:\n          //   .field(\"anotherField\", ...)\n          .build();\n\n        export const globalConfigSchematics = createConfigSchematics()\n          .field(\n            \"myGlobalCustomField\", // The key of the field.\n            \"string\",\n            {\n              displayName: \"My Global Custom Field\",\n              hint: \"This is my global custom field. Doesn't do anything special.\",\n            },\n            \"default value\", // Default Value\n          )\n          // You can add more fields by chaining the field method.\n          // For example:\n          //  .field(\"anotherGlobalField\", ...)\n          .build();\n```\n\nIf you've added your config schematics manual, you will also need to register the configurations in your plugin's `index.ts` file.\n\nThis is done by calling `context.withConfigSchematics(configSchematics)` and `context.withGlobalConfigSchematics(globalConfigSchematics)` in the `main` function of your plugin.\n\n```lms_code_snippet\n  title: \"src/index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        // ... other imports ...\n        import { toolsProvider } from \"./toolsProvider\";\n\n        export async function main(context: PluginContext) {\n          // ... other plugin setup code ...\n\n          // Register the configuration schematics.\n          context.withConfigSchematics(configSchematics);\n          // Register the global configuration schematics.\n          context.withGlobalConfigSchematics(globalConfigSchematics);\n\n          // ... other plugin setup code ...\n        }\n```\n"])</script><script>self.__next_f.push([1,"192:{\"metadata\":\"$193\",\"prettyName\":\"`config.ts` File\",\"content\":\"$194\",\"pageRelUrl\":\"2_typescript/3_plugins/4_custom-configuration/config-ts.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n196:{\"title\":\"Accessing Configuration\",\"description\":\"\",\"index\":3}\n197:T447,\nYou can access the configuration using the method `ctl.getPluginConfig(configSchematics)` and `ctl.getGlobalConfig(globalConfigSchematics)` respectively.\n\nFor example, here is how to access the config within the promptPreprocessor:\n\n```lms_code_snippet\n  title: \"src/promptPreprocessor.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { type PreprocessorController, type ChatMessage } from \"@lmstudio/sdk\";\n        import { configSchematics } from \"./config\";\n\n        export async function preprocess(ctl: PreprocessorController, userMessage: ChatMessage) {\n          const pluginConfig = ctl.getPluginConfig(configSchematics);\n          const myCustomField = pluginConfig.get(\"myCustomField\");\n\n          const globalPluginConfig = ctl.getGlobalPluginConfig(configSchematics);\n          const globalMyCustomField = globalPluginConfig.get(\"myCustomField\");\n\n          return (\n            `${userMessage.getText()},` +\n            `myCustomField: ${myCustomField}, ` +\n            `globalMyCustomField: ${globalMyCustomField}`\n          );\n        }\n```\n195:{\"metadata\":\"$196\",\"prettyName\":\"Accessing Configuration\",\"content\":\"$197\",\"pageRelUrl\":\"2_typescript/3_plugins/4_custom-configuration/accessing-config.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n199:{\"title\":\"Defining New Fields\",\"description\":\"\",\"index\":4}\n19a:T1109,"])</script><script>self.__next_f.push([1,"\nWe support the following field types:\n\n- `string`: A text input field.\n\n  ```lms_code_snippet\n    variants:\n      TypeScript:\n        language: typescript\n        code: |\n          // ... other fields ...\n          .field(\n            \"stringField\", // The key of the field.\n            \"string\", // Type of the field.\n            {\n              displayName: \"A string field\",\n              subtitle: \"Subtitle\", // Optional subtitle for the field. (Show below the field)\n              hint: \"Hint\", // Optional hint for the field. (Show on hover)\n              isParagraph: false, // Whether to show a large text input area for this field.\n              isProtected: false, // Whether the value should be obscured in the UI (e.g., for passwords).\n              placeholder: \"Placeholder text\", // Optional placeholder text for the field.\n            },\n            \"default value\", // Default Value\n          )\n          // ... other fields ...\n  ```\n\n- `numeric`: A number input field with optional validation and slider UI.\n\n  ```lms_code_snippet\n    variants:\n      TypeScript:\n        language: typescript\n        code: |\n          // ... other fields ...\n          .field(\n            \"numberField\", // The key of the field.\n            \"numeric\", // Type of the field.\n            {\n              displayName: \"A number field\",\n              subtitle: \"Subtitle for\", // Optional subtitle for the field. (Show below the field)\n              hint: \"Hint for number field\", // Optional hint for the field. (Show on hover)\n              int: false, // Whether the field should accept only integer values.\n              min: 0, // Minimum value for the field.\n              max: 100, // Maximum value for the field.\n              slider: {\n                // If present, configurations for the slider UI\n                min: 0, // Minimum value for the slider.\n                max: 100, // Maximum value for the slider.\n                step: 1, // Step value for the slider.\n              },\n            },\n            42, // Default Value\n          )\n          // ... other fields ...\n  ```\n\n- `boolean`: A checkbox or toggle input field.\n\n  ```lms_code_snippet\n    variants:\n      TypeScript:\n        language: typescript\n        code: |\n          // ... other fields ...\n          .field(\n            \"booleanField\", // The key of the field.\n            \"boolean\", // Type of the field.\n            {\n              displayName: \"A boolean field\",\n              subtitle: \"Subtitle\", // Optional subtitle for the field. (Show below the field)\n              hint: \"Hint\", // Optional hint for the field. (Show on hover)\n            },\n            true, // Default Value\n          )\n          // ... other fields ...\n  ```\n\n- `stringArray`: An array of string values with configurable constraints.\n\n  ```lms_code_snippet\n    variants:\n      TypeScript:\n        language: typescript\n        code: |\n          // ... other fields ...\n          .field(\n            \"stringArrayField\",\n            \"stringArray\",\n            {\n              displayName: \"A string array field\",\n              subtitle: \"Subtitle\", // Optional subtitle for the field. (Show below the field)\n              hint: \"Hint\", // Optional hint for the field. (Show on hover)\n              allowEmptyStrings: true, // Whether to allow empty strings in the array.\n              maxNumItems: 5, // Maximum number of items in the array.\n            },\n            [\"default\", \"values\"], // Default Value\n          )\n          // ... other fields ...\n  ```\n\n- `select`: A dropdown selection field with predefined options.\n\n  ```lms_code_snippet\n    variants:\n      TypeScript:\n        language: typescript\n        code: |\n          // ... other fields ...\n          .field(\n            \"selectField\",\n            \"select\",\n            {\n              displayName: \"A select field\",\n              options: [\n                { value: \"option1\", displayName: \"Option 1\" },\n                { value: \"option2\", displayName: \"Option 2\" },\n                { value: \"option3\", displayName: \"Option 3\" },\n              ],\n              subtitle: \"Subtitle\", // Optional subtitle for the field. (Show below the field)\n              hint: \"Hint\", // Optional hint for the field. (Show on hover)\n            },\n            \"option1\", // Default Value\n          )\n          // ... other fields ...\n  ```\n"])</script><script>self.__next_f.push([1,"198:{\"metadata\":\"$199\",\"prettyName\":\"Defining New Fields\",\"content\":\"$19a\",\"pageRelUrl\":\"2_typescript/3_plugins/4_custom-configuration/defining-new-fields.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n18e:{\"\":\"$18f\",\"config-ts\":\"$192\",\"accessing-config\":\"$195\",\"defining-new-fields\":\"$198\"}\n19d:{\"title\":\"Sharing Plugins\",\"description\":\"How to publish your LM Studio plugins so they can be used by others\",\"index\":7}\n19e:T544,\nTo share publish your LM Studio plugin, open the plugin directory in a terminal and run:\n\n```bash\nlms push\n```\n\nThis command will package your plugin and upload it to the LM Studio Hub. You can use this command to create new plugins or update existing ones.\n\n### Changing Plugin Names\n\nIf you wish to change the name of the plugin, you can do so by editing the `manifest.json` file in the root of your plugin directory. Look for the `name` field and update it to your desired plugin name. Note the `name` must be kebab-case.\n\nWhen you `lms push` the plugin, it will be treated as a new plugin if the name has changed. You can delete the old plugin from the LM Studio Hub if you no longer need it.\n\n### Publishing Plugins to an Organization\n\nIf you are in an organization and wish to publish the plugin to the organization, you can do so by editing the `manifest.json` file in the root of your plugin directory. Look for the `owner` field and set it to the name of your organization. When you run `lms push`, the plugin will be published to the organization instead of your personal account.\n\n### Private Plugins\n\nIf your account supports private plugins, you can publish your plugins privately by using the `--private` flag when running `lms push`:\n\n```bash\nlms push --private\n```\n\nPrivate artifact is in test. Get in touch if you are interested.\n19c:{\"metadata\":\"$19d\",\"prettyName\":\"Sharing Plugins\",\"content\":\"$19e\",\"pageRelUrl\":\"2_typescript/3_plugins/5_publish-plugins/index.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}\n19b:{\"\":\"$19c\"}\n15d:{\"\":\"$15e\",\"dependencies\":\"$161\",\"tool"])</script><script>self.__next_f.push([1,"s-provider\":\"$163\",\"prompt-preprocessor\":\"$175\",\"generator\":\"$184\",\"custom-configuration\":\"$18e\",\"publish-plugins\":\"$19b\"}\n1a1:{\"title\":\"Embedding\",\"description\":\"Generate text embeddings from input text\",\"index\":1}\n1a0:{\"metadata\":\"$1a1\",\"prettyName\":\"Generating embedding vectors\",\"content\":\"\\nGenerate embeddings for input text. Embeddings are vector representations of text that capture semantic meaning. Embeddings are a building block for RAG (Retrieval-Augmented Generation) and other similarity-based tasks.\\n\\n### Prerequisite: Get an Embedding Model\\n\\nIf you don't yet have an embedding model, you can download a model like `nomic-ai/nomic-embed-text-v1.5` using the following command:\\n\\n```bash\\nlms get nomic-ai/nomic-embed-text-v1.5\\n```\\n\\n## Create Embeddings\\n\\nTo convert a string to a vector representation, pass it to the `embed` method on the corresponding embedding model handle.\\n\\n```lms_code_snippet\\n  title: \\\"index.ts\\\"\\n  variants:\\n    TypeScript:\\n      language: typescript\\n      code: |\\n        import { LMStudioClient } from \\\"@lmstudio/sdk\\\";\\n        const client = new LMStudioClient();\\n\\n        const model = await client.embedding.model(\\\"nomic-embed-text-v1.5\\\");\\n\\n        const { embedding } = await model.embed(\\\"Hello, world!\\\");\\n```\\n\",\"pageRelUrl\":\"2_typescript/4_embedding/index.md\",\"sectionKey\":\"embedding\",\"sectionPrettyName\":\"Text Embedding\"}\n19f:{\"\":\"$1a0\"}\n1a4:{\"title\":\"Tokenization\",\"description\":\"Tokenize text using a model's tokenizer\",\"index\":1}\n1a5:Td2e,"])</script><script>self.__next_f.push([1,"\nModels use a tokenizer to internally convert text into \"tokens\" they can deal with more easily. LM Studio exposes this tokenizer for utility.\n\n## Tokenize\n\nYou can tokenize a string with a loaded LLM or embedding model using the SDK. In the below examples, `llm` can be replaced with an embedding model `emb`.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const model = await client.llm.model();\n\n        const tokens = await model.tokenize(\"Hello, world!\");\n\n        console.info(tokens); // Array of token IDs.\n```\n\n## Count tokens\n\nIf you only care about the number of tokens, you can use the `.countTokens` method instead.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        const tokenCount = await model.countTokens(\"Hello, world!\");\n        console.info(\"Token count:\", tokenCount);\n```\n\n### Example: Count Context\n\nYou can determine if a given conversation fits into a model's context by doing the following:\n\n1. Convert the conversation to a string using the prompt template.\n2. Count the number of tokens in the string.\n3. Compare the token count to the model's context length.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { Chat, type LLM, LMStudioClient } from \"@lmstudio/sdk\";\n\n        async function doesChatFitInContext(model: LLM, chat: Chat) {\n          // Convert the conversation to a string using the prompt template.\n          const formatted = await model.applyPromptTemplate(chat);\n          // Count the number of tokens in the string.\n          const tokenCount = await model.countTokens(formatted);\n          // Get the current loaded context length of the model\n          const contextLength = await model.getContextLength();\n          return tokenCount \u003c contextLength;\n        }\n\n        const client = new LMStudioClient();\n        const model = await client.llm.model();\n\n        const chat = Chat.from([\n          { role: \"user\", content: \"What is the meaning of life.\" },\n          { role: \"assistant\", content: \"The meaning of life is...\" },\n          // ... More messages\n        ]);\n\n        console.info(\"Fits in context:\", await doesChatFitInContext(model, chat));\n```\n\n\u003c!-- ### Context length comparisons\n\nThe below examples check whether a conversation is over a LLM's context length\n(replace `llm` with `emb` to check for an embedding model).\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient, Chat } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n        const llm = await client.llm.model();\n\n        // To check for a string, simply tokenize\n        var tokens = await llm.tokenize(\"Hello, world!\");\n\n        // To check for a Chat, apply the prompt template first\n        const chat = Chat.createEmpty().withAppended(\"user\", \"Hello, world!\");\n        const templatedChat = await llm.applyPromptTemplate(chat);\n        tokens = await llm.tokenize(templatedChat);\n\n        // If the prompt's length in tokens is less than the context length, you're good!\n        const contextLength = await llm.getContextLength()\n        const isOkay = (tokens.length \u003c contextLength)\n``` --\u003e\n"])</script><script>self.__next_f.push([1,"1a3:{\"metadata\":\"$1a4\",\"prettyName\":\"Tokenizing text\",\"content\":\"$1a5\",\"pageRelUrl\":\"2_typescript/5_tokenization/index.md\",\"sectionKey\":\"tokenization\",\"sectionPrettyName\":\"Tokenization\"}\n1a2:{\"\":\"$1a3\"}\n1a8:{\"title\":\"List Local Models\",\"description\":\"APIs to list the available models in a given local environment\",\"index\":null}\n1a9:T5a6,\nYou can iterate through locally available models using the `listLocalModels` method.\n\n## Available Model on the Local Machine\n\n`listLocalModels` lives under the `system` namespace of the `LMStudioClient` object.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        console.info(await client.system.listDownloadedModels());\n```\n\nThis will give you results equivalent to using [`lms ls`](../../cli/ls) in the CLI.\n\n### Example output:\n\n```json\n[\n  {\n    \"type\": \"llm\",\n    \"modelKey\": \"qwen2.5-7b-instruct\",\n    \"format\": \"gguf\",\n    \"displayName\": \"Qwen2.5 7B Instruct\",\n    \"path\": \"lmstudio-community/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q4_K_M.gguf\",\n    \"sizeBytes\": 4683073952,\n    \"paramsString\": \"7B\",\n    \"architecture\": \"qwen2\",\n    \"vision\": false,\n    \"trainedForToolUse\": true,\n    \"maxContextLength\": 32768\n  },\n  {\n    \"type\": \"embedding\",\n    \"modelKey\": \"text-embedding-nomic-embed-text-v1.5@q4_k_m\",\n    \"format\": \"gguf\",\n    \"displayName\": \"Nomic Embed Text v1.5\",\n    \"path\": \"nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q4_K_M.gguf\",\n    \"sizeBytes\": 84106624,\n    \"architecture\": \"nomic-bert\",\n    \"maxContextLength\": 2048\n  }\n]\n```\n\n\u003c!-- Learn more about the `client.system` namespace in the [System API Reference](../api-reference/system-namespace). --\u003e\n1a7:{\"metadata\":\"$1a8\",\"prettyName\":\"List Local Models\",\"content\":\"$1a9\",\"pageRelUrl\":\"2_typescript/6_manage-models/list-downloaded.md\",\"sectionKey\":\"manage-models\",\"sectionPrettyName\":\"Manage Models\"}\n1ab:{\"title\":\"List Loaded Models\",\"description\":\"Query which m"])</script><script>self.__next_f.push([1,"odels are currently loaded\",\"index\":null}\n1aa:{\"metadata\":\"$1ab\",\"prettyName\":\"List Loaded Models\",\"content\":\"\\nYou can iterate through models loaded into memory using the `listLoaded` method. This method lives under the `llm` and `embedding` namespaces of the `LMStudioClient` object.\\n\\n## List Models Currently Loaded in Memory\\n\\nThis will give you results equivalent to using [`lms ps`](../../cli/ps) in the CLI.\\n\\n```lms_code_snippet\\n  variants:\\n    TypeScript:\\n      language: typescript\\n      code: |\\n        import { LMStudioClient } from \\\"@lmstudio/sdk\\\";\\n\\n        const client = new LMStudioClient();\\n\\n        const llmOnly = await client.llm.listLoaded();\\n        const embeddingOnly = await client.embedding.listLoaded();\\n```\\n\\n\u003c!-- Learn more about `client.llm` namespace in the [API Reference](../api-reference/llm-namespace). --\u003e\\n\",\"pageRelUrl\":\"2_typescript/6_manage-models/list-loaded.md\",\"sectionKey\":\"manage-models\",\"sectionPrettyName\":\"Manage Models\"}\n1ad:{\"title\":\"Manage Models in Memory\",\"description\":\"APIs to load, access, and unload models from memory\",\"index\":null}\n1ae:T10ad,"])</script><script>self.__next_f.push([1,"\nAI models are huge. It can take a while to load them into memory. LM Studio's SDK allows you to precisely control this process.\n\n**Most commonly:**\n\n- Use `.model()` to get any currently loaded model\n- Use `.model(\"model-key\")` to use a specific model\n\n**Advanced (manual model management):**\n\n- Use `.load(\"model-key\")` to load a new instance of a model\n- Use `model.unload()` to unload a model from memory\n\n## Get the Current Model with `.model()`\n\nIf you already have a model loaded in LM Studio (either via the GUI or `lms load`), you can use it by calling `.model()` without any arguments.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model();\n```\n\n## Get a Specific Model with `.model(\"model-key\")`\n\nIf you want to use a specific model, you can provide the model key as an argument to `.model()`.\n\n#### Get if Loaded, or Load if not\n\nCalling `.model(\"model-key\")` will load the model if it's not already loaded, or return the existing instance if it is.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model(\"qwen/qwen3-4b-2507\");\n```\n\n\u003c!-- Learn more about the `.model()` method and the parameters it accepts in the [API Reference](../api-reference/model). --\u003e\n\n## Load a New Instance of a Model with `.load()`\n\nUse `load()` to load a new instance of a model, even if one already exists. This allows you to have multiple instances of the same or different models loaded at the same time.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n        const client = new LMStudioClient();\n\n        const llama = await client.llm.load(\"qwen/qwen3-4b-2507\");\n        const another_llama = await client.llm.load(\"qwen/qwen3-4b-2507\", {\n          identifier: \"second-llama\"\n        });\n```\n\n\u003c!-- Learn more about the `.load()` method and the parameters it accepts in the [API Reference](../api-reference/load). --\u003e\n\n### Note about Instance Identifiers\n\nIf you provide an instance identifier that already exists, the server will throw an error.\nSo if you don't really care, it's safer to not provide an identifier, in which case\nthe server will generate one for you. You can always check in the server tab in LM Studio, too!\n\n## Unload a Model from Memory with `.unload()`\n\nOnce you no longer need a model, you can unload it by simply calling `unload()` on its handle.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model();\n        await model.unload();\n```\n\n## Set Custom Load Config Parameters\n\nYou can also specify the same load-time configuration options when loading a model, such as Context Length and GPU offload.\n\nSee [load-time configuration](../llm-prediction/parameters) for more.\n\n## Set an Auto Unload Timer (TTL)\n\nYou can specify a _time to live_ for a model you load, which is the idle time (in seconds)\nafter the last request until the model unloads. See [Idle TTL](/docs/api/ttl-and-auto-evict) for more on this.\n\n```lms_code_snippet\n  variants:\n    \"Using .load\":\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n\n        const model = await client.llm.load(\"qwen/qwen3-4b-2507\", {\n          ttl: 300, // 300 seconds\n        });\n    \"Using .model\":\n      language: typescript\n      code: |\n        import { LMStudioClient } from \"@lmstudio/sdk\";\n\n        const client = new LMStudioClient();\n\n        const model = await client.llm.model(\"qwen/qwen3-4b-2507\", {\n          // Note: specifying ttl in `.model` will only set the TTL for the model if the model is\n          // loaded from this call. If the model was already loaded, the TTL will not be updated.\n          ttl: 300, // 300 seconds\n        });\n```\n"])</script><script>self.__next_f.push([1,"1ac:{\"metadata\":\"$1ad\",\"prettyName\":\"Load and Access Models\",\"content\":\"$1ae\",\"pageRelUrl\":\"2_typescript/6_manage-models/loading.md\",\"sectionKey\":\"manage-models\",\"sectionPrettyName\":\"Manage Models\"}\n1a6:{\"list-downloaded\":\"$1a7\",\"list-loaded\":\"$1aa\",\"loading\":\"$1ac\"}\n1b1:{\"title\":\"`LLMLoadModelConfig`\",\"description\":\"API Reference for `LLMLoadModelConfig`\",\"index\":null}\n1b2:T1667,"])</script><script>self.__next_f.push([1,"\n### Parameters\n\n```lms_params\n- name: gpu\n  description: |\n    How to distribute the work to your GPUs. See {@link GPUSetting} for more information.\n  public: true\n  type: GPUSetting\n  optional: true\n\n- name: contextLength\n  description: |\n    The size of the context length in number of tokens. This will include both the prompts and the\n    responses. Once the context length is exceeded, the value set in\n    {@link LLMPredictionConfigBase#contextOverflowPolicy} is used to determine the behavior.\n\n    See {@link LLMContextOverflowPolicy} for more information.\n  type: number\n  optional: true\n\n- name: ropeFrequencyBase\n  description: |\n    Custom base frequency for rotary positional embeddings (RoPE).\n\n    This advanced parameter adjusts how positional information is embedded in the model's\n    representations. Increasing this value may enable better performance at high context lengths by\n    modifying how the model processes position-dependent information.\n  type: number\n  optional: true\n\n- name: ropeFrequencyScale\n  description: |\n    Scaling factor for RoPE (Rotary Positional Encoding) frequency.\n\n    This factor scales the effective context window by modifying how positional information is\n    encoded. Higher values allow the model to handle longer contexts by making positional encoding\n    more granular, which can be particularly useful for extending a model beyond its original\n    training context length.\n  type: number\n  optional: true\n\n- name: evalBatchSize\n  description: |\n    Number of input tokens to process together in a single batch during evaluation.\n\n    Increasing this value typically improves processing speed and throughput by leveraging\n    parallelization, but requires more memory. Finding the optimal batch size often involves\n    balancing between performance gains and available hardware resources.\n  type: number\n  optional: true\n\n- name: flashAttention\n  description: |\n    Enables Flash Attention for optimized attention computation.\n\n    Flash Attention is an efficient implementation that reduces memory usage and speeds up\n    generation by optimizing how attention mechanisms are computed. This can significantly\n    improve performance on compatible hardware, especially for longer sequences.\n  type: boolean\n  optional: true\n\n- name: keepModelInMemory\n  description: |\n    When enabled, prevents the model from being swapped out of system memory.\n\n    This option reserves system memory for the model even when portions are offloaded to GPU,\n    ensuring faster access times when the model needs to be used. Improves performance\n    particularly for interactive applications, but increases overall RAM requirements.\n  type: boolean\n  optional: true\n\n- name: seed\n  description: |\n    Random seed value for model initialization to ensure reproducible outputs.\n\n    Setting a specific seed ensures that random operations within the model (like sampling)\n    produce the same results across different runs, which is important for reproducibility\n    in testing and development scenarios.\n  type: number\n  optional: true\n\n- name: useFp16ForKVCache\n  description: |\n    When enabled, stores the key-value cache in half-precision (FP16) format.\n\n    This option significantly reduces memory usage during inference by using 16-bit floating\n    point numbers instead of 32-bit for the attention cache. While this may slightly reduce\n    numerical precision, the impact on output quality is generally minimal for most applications.\n  type: boolean\n  optional: true\n\n- name: tryMmap\n  description: |\n    Attempts to use memory-mapped (mmap) file access when loading the model.\n\n    Memory mapping can improve initial load times by mapping model files directly from disk to\n    memory, allowing the operating system to handle paging. This is particularly beneficial for\n    quick startup, but may reduce performance if the model is larger than available system RAM,\n    causing frequent disk access.\n  type: boolean\n  optional: true\n\n- name: numExperts\n  description: |\n    Specifies the number of experts to use for models with Mixture of Experts (MoE) architecture.\n\n    MoE models contain multiple \"expert\" networks that specialize in different aspects of the task.\n    This parameter controls how many of these experts are active during inference, affecting both\n    performance and quality of outputs. Only applicable for models designed with the MoE architecture.\n  type: number\n  optional: true\n\n- name: llamaKCacheQuantizationType\n  description: |\n    Quantization type for the Llama model's key cache.\n\n    This option determines the precision level used to store the key component of the attention\n    mechanism's cache. Lower precision values (e.g., 4-bit or 8-bit quantization) significantly\n    reduce memory usage during inference but may slightly impact output quality. The effect varies\n    between different models, with some being more robust to quantization than others.\n\n    Set to false to disable quantization and use full precision.\n  type: LLMLlamaCacheQuantizationType | false\n  optional: true\n\n- name: llamaVCacheQuantizationType\n  description: |\n    Quantization type for the Llama model's value cache.\n\n    Similar to the key cache quantization, this option controls the precision used for the value\n    component of the attention mechanism's cache. Reducing precision saves memory but may affect\n    generation quality. This option requires Flash Attention to be enabled to function properly.\n\n    Different models respond differently to value cache quantization, so experimentation may be\n    needed to find the optimal setting for a specific use case. Set to false to disable quantization.\n  type: LLMLlamaCacheQuantizationType | false\n  optional: true\n```\n"])</script><script>self.__next_f.push([1,"1b0:{\"metadata\":\"$1b1\",\"prettyName\":\"`LLMLoadModelConfig`\",\"content\":\"$1b2\",\"pageRelUrl\":\"2_typescript/7_api-reference/llm-load-model-config.md\",\"sectionKey\":\"api-reference\",\"sectionPrettyName\":\"API Reference\"}\n1b4:{\"title\":\"`LLMPredictionConfigInput`\",\"description\":\"\",\"index\":null}\n1b5:T184c,"])</script><script>self.__next_f.push([1,"\n### Fields\n\n```lms_params\n- name: \"maxTokens\"\n  type: \"number | false\"\n  optional: true\n  description: \"Number of tokens to predict at most. If set to false, the model will predict as many tokens as it wants.\\n\\nWhen the prediction is stopped because of this limit, the `stopReason` in the prediction stats will be set to `maxPredictedTokensReached`.\"\n\n- name: \"temperature\"\n  type: \"number\"\n  optional: true\n  description: \"The temperature parameter for the prediction model. A higher value makes the predictions more random, while a lower value makes the predictions more deterministic. The value should be between 0 and 1.\"\n\n- name: \"stopStrings\"\n  type: \"Array\u003cstring\u003e\"\n  optional: true\n  description: \"An array of strings. If the model generates one of these strings, the prediction will stop.\\n\\nWhen the prediction is stopped because of this limit, the `stopReason` in the prediction stats will be set to `stopStringFound`.\"\n\n- name: \"toolCallStopStrings\"\n  type: \"Array\u003cstring\u003e\"\n  optional: true\n  description: \"An array of strings. If the model generates one of these strings, the prediction will stop with the `stopReason` `toolCalls`.\"\n\n- name: \"contextOverflowPolicy\"\n  type: \"LLMContextOverflowPolicy\"\n  optional: true\n  description: \"The behavior for when the generated tokens length exceeds the context window size. The allowed values are:\\n\\n- `stopAtLimit`: Stop the prediction when the generated tokens length exceeds the context window size. If the generation is stopped because of this limit, the `stopReason` in the prediction stats will be set to `contextLengthReached`\\n- `truncateMiddle`: Keep the system prompt and the first user message, truncate middle.\\n- `rollingWindow`: Maintain a rolling window and truncate past messages.\"\n\n- name: \"structured\"\n  type: \"ZodType\u003cTStructuredOutputType\u003e | LLMStructuredPredictionSetting\"\n  optional: true\n  description: \"Configures the model to output structured JSON data that follows a specific schema defined using Zod.\\n\\nWhen you provide a Zod schema, the model will be instructed to generate JSON that conforms to that schema rather than free-form text.\\n\\nThis is particularly useful for extracting specific data points from model responses or when you need the output in a format that can be directly used by your application.\"\n\n- name: \"topKSampling\"\n  type: \"number\"\n  optional: true\n  description: \"Controls token sampling diversity by limiting consideration to the K most likely next tokens.\\n\\nFor example, if set to 40, only the 40 tokens with the highest probabilities will be considered for the next token selection. A lower value (e.g., 20) will make the output more focused and conservative, while a higher value (e.g., 100) allows for more creative and diverse outputs.\\n\\nTypical values range from 20 to 100.\"\n\n- name: \"repeatPenalty\"\n  type: \"number | false\"\n  optional: true\n  description: \"Applies a penalty to repeated tokens to prevent the model from getting stuck in repetitive patterns.\\n\\nA value of 1.0 means no penalty. Values greater than 1.0 increase the penalty. For example, 1.2 would reduce the probability of previously used tokens by 20%. This is particularly useful for preventing the model from repeating phrases or getting stuck in loops.\\n\\nSet to false to disable the penalty completely.\"\n\n- name: \"minPSampling\"\n  type: \"number | false\"\n  optional: true\n  description: \"Sets a minimum probability threshold that a token must meet to be considered for generation.\\n\\nFor example, if set to 0.05, any token with less than 5% probability will be excluded from consideration. This helps filter out unlikely or irrelevant tokens, potentially improving output quality.\\n\\nValue should be between 0 and 1. Set to false to disable this filter.\"\n\n- name: \"topPSampling\"\n  type: \"number | false\"\n  optional: true\n  description: \"Implements nucleus sampling by only considering tokens whose cumulative probabilities reach a specified threshold.\\n\\nFor example, if set to 0.9, the model will consider only the most likely tokens that together add up to 90% of the probability mass. This helps balance between diversity and quality by dynamically adjusting the number of tokens considered based on their probability distribution.\\n\\nValue should be between 0 and 1. Set to false to disable nucleus sampling.\"\n\n- name: \"xtcProbability\"\n  type: \"number | false\"\n  optional: true\n  description: \"Controls how often the XTC (Exclude Top Choices) sampling technique is applied during generation.\\n\\nXTC sampling can boost creativity and reduce clichÃ©s by occasionally filtering out common tokens. For example, if set to 0.3, there's a 30% chance that XTC sampling will be applied when generating each token.\\n\\nValue should be between 0 and 1. Set to false to disable XTC completely.\"\n\n- name: \"xtcThreshold\"\n  type: \"number | false\"\n  optional: true\n  description: \"Defines the lower probability threshold for the XTC (Exclude Top Choices) sampling technique.\\n\\nWhen XTC sampling is activated (based on xtcProbability), the algorithm identifies tokens with probabilities between this threshold and 0.5, then removes all such tokens except the least probable one. This helps introduce more diverse and unexpected tokens into the generation.\\n\\nOnly takes effect when xtcProbability is enabled.\"\n\n- name: \"cpuThreads\"\n  type: \"number\"\n  optional: true\n  description: \"Specifies the number of CPU threads to allocate for model inference.\\n\\nHigher values can improve performance on multi-core systems but may compete with other processes. For example, on an 8-core system, a value of 4-6 might provide good performance while leaving resources for other tasks.\\n\\nIf not specified, the system will use a default value based on available hardware.\"\n\n- name: \"draftModel\"\n  type: \"string\"\n  optional: true\n  description: \"The draft model to use for speculative decoding. Speculative decoding is a technique that can drastically increase the generation speed (up to 3x for larger models) by paring a main model with a smaller draft model.\\n\\nSee here for more information: https://lmstudio.ai/docs/advanced/speculative-decoding\\n\\nYou do not need to load the draft model yourself. Simply specifying its model key here is enough.\"\n```\n"])</script><script>self.__next_f.push([1,"1b3:{\"metadata\":\"$1b4\",\"prettyName\":\"`LLMPredictionConfigInput`\",\"content\":\"$1b5\",\"pageRelUrl\":\"2_typescript/7_api-reference/llm-prediction-config-input.md\",\"sectionKey\":\"api-reference\",\"sectionPrettyName\":\"API Reference\"}\n1af:{\"llm-load-model-config\":\"$1b0\",\"llm-prediction-config-input\":\"$1b3\"}\n1b8:{\"title\":\"Get Context Length\",\"description\":\"API to get the maximum context length of a model.\",\"index\":null}\n1b9:T917,"])</script><script>self.__next_f.push([1,"\nLLMs and embedding models, due to their fundamental architecture, have a property called `context length`, and more specifically a **maximum** context length. Loosely speaking, this is how many tokens the models can \"keep in memory\" when generating text or embeddings. Exceeding this limit will result in the model behaving erratically.\n\n## Use the `getContextLength()` Function on the Model Object\n\nIt's useful to be able to check the context length of a model, especially as an extra check before providing potentially long input to the model.\n\n```lms_code_snippet\n  title: \"index.ts\"\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        const contextLength = await model.getContextLength();\n```\n\nThe `model` in the above code snippet is an instance of a loaded model you get from the `llm.model` method. See [Manage Models in Memory](../manage-models/loading) for more information.\n\n### Example: Check if the input will fit in the model's context window\n\nYou can determine if a given conversation fits into a model's context by doing the following:\n\n1. Convert the conversation to a string using the prompt template.\n2. Count the number of tokens in the string.\n3. Compare the token count to the model's context length.\n\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        import { Chat, type LLM, LMStudioClient } from \"@lmstudio/sdk\";\n\n        async function doesChatFitInContext(model: LLM, chat: Chat) {\n          // Convert the conversation to a string using the prompt template.\n          const formatted = await model.applyPromptTemplate(chat);\n          // Count the number of tokens in the string.\n          const tokenCount = await model.countTokens(formatted);\n          // Get the current loaded context length of the model\n          const contextLength = await model.getContextLength();\n          return tokenCount \u003c contextLength;\n        }\n\n        const client = new LMStudioClient();\n        const model = await client.llm.model();\n\n        const chat = Chat.from([\n          { role: \"user\", content: \"What is the meaning of life.\" },\n          { role: \"assistant\", content: \"The meaning of life is...\" },\n          // ... More messages\n        ]);\n\n        console.info(\"Fits in context:\", await doesChatFitInContext(model, chat));\n```\n"])</script><script>self.__next_f.push([1,"1b7:{\"metadata\":\"$1b8\",\"prettyName\":\"Get Context Length\",\"content\":\"$1b9\",\"pageRelUrl\":\"2_typescript/8_model-info/get-context-length.md\",\"sectionKey\":\"model-info\",\"sectionPrettyName\":\"Model Info\"}\n1bb:{\"title\":\"Get Model Info\",\"description\":\"Get information about the model\",\"index\":null}\n1ba:{\"metadata\":\"$1bb\",\"prettyName\":\"Get Model Info\",\"content\":\"\\nYou can access information about a loaded model using the `getInfo` method.\\n\\n```lms_code_snippet\\n  variants:\\n    LLM:\\n      language: typescript\\n      code: |\\n        import { LMStudioClient } from \\\"@lmstudio/sdk\\\";\\n\\n        const client = new LMStudioClient();\\n        const model = await client.llm.model();\\n\\n        const modelInfo = await model.getInfo();\\n\\n        console.info(\\\"Model Key\\\", modelInfo.modelKey);\\n        console.info(\\\"Current Context Length\\\", model.contextLength);\\n        console.info(\\\"Model Trained for Tool Use\\\", modelInfo.trainedForToolUse);\\n        // etc.\\n    Embedding Model:\\n      language: typescript\\n      code: |\\n        import { LMStudioClient } from \\\"@lmstudio/sdk\\\";\\n\\n        const client = new LMStudioClient();\\n        const model = await client.embedding.model();\\n\\n        const modelInfo = await model.getInfo();\\n\\n        console.info(\\\"Model Key\\\", modelInfo.modelKey);\\n        console.info(\\\"Current Context Length\\\", modelInfo.contextLength);\\n        // etc.\\n```\\n\",\"pageRelUrl\":\"2_typescript/8_model-info/get-model-info.md\",\"sectionKey\":\"model-info\",\"sectionPrettyName\":\"Model Info\"}\n1b6:{\"get-context-length\":\"$1b7\",\"get-model-info\":\"$1ba\"}\n137:{\"\":\"$138\",\"project-setup\":\"$13b\",\"llm-prediction\":\"$13d\",\"agent\":\"$156\",\"plugins\":\"$15d\",\"embedding\":\"$19f\",\"tokenization\":\"$1a2\",\"manage-models\":\"$1a6\",\"api-reference\":\"$1af\",\"model-info\":\"$1b6\"}\n1be:{\"title\":\"`lms` â€” LM Studio's CLI\",\"description\":\"Get starting with the `lms` command line utility.\",\"index\":1}\n1bf:Tc78,"])</script><script>self.__next_f.push([1,"\n## Install `lms`\n\n`lms` ships with LM Studio, so you don't need to do any additional installation steps if you have LM Studio installed.\n\nJust open a terminal window and run `lms`:\n\n```shell\nlms --help\n```\n\n## Open source\n\n`lms` is **MIT Licensed** and is developed in this repository on GitHub: https://github.com/lmstudio-ai/lms\n\n## Command quick links\n\n| Command | Syntax | Docs |\n| --- | --- | --- |\n| Chat in the terminal | `lms chat` | [Guide](/docs/cli/local-models/chat) |\n| Download models | `lms get` | [Guide](/docs/cli/local-models/get) |\n| List your models | `lms ls` | [Guide](/docs/cli/local-models/ls) |\n| See models loaded into memory | `lms ps` | [Guide](/docs/cli/local-models/ps) |\n| Control the server | `lms server start` | [Guide](/docs/cli/serve/server-start) |\n| Manage the inference runtime | `lms runtime` | [Guide](/docs/cli/runtime) |\n\n\n### Verify the installation\n\n```lms_info\nðŸ‘‰ You need to run LM Studio _at least once_ before you can use `lms`.\n```\n\nOpen a terminal window and run `lms`.\n\n```lms_terminal\n$ lms\n\nlms is LM Studio's CLI utility for your models, server, and inference runtime. (v0.0.47)\n\nUsage: lms [options] [command]\n\nLocal models\n   chat               Start an interactive chat with a model\n   get                Search and download models\n   load               Load a model\n   unload             Unload a model\n   ls                 List the models available on disk\n   ps                 List the models currently loaded in memory\n   import             Import a model file into LM Studio\n\nServe\n   server             Commands for managing the local server\n   log                Log incoming and outgoing messages\n\nRuntime\n   runtime            Manage and update the inference runtime\n\nDevelop \u0026 Publish (Beta)\n   clone              Clone an artifact from LM Studio Hub to a local folder\n   push               Uploads the artifact in the current folder to LM Studio Hub\n   dev                Starts a plugin dev server in the current folder\n   login              Authenticate with LM Studio\n\nLearn more:           https://lmstudio.ai/docs/developer\nJoin our Discord:     https://discord.gg/lmstudio\n```\n\n## Use `lms` to automate and debug your workflows\n\n### Start and stop the local server\n\n```bash\nlms server start\nlms server stop\n```\n\nLearn more about [`lms server`](/docs/cli/serve/server-start).\n\n### List the local models on the machine\n\n```bash\nlms ls\n```\n\nLearn more about [`lms ls`](/docs/cli/local-models/ls).\n\nThis will reflect the current LM Studio models directory, which you set in **ðŸ“‚ My Models** tab in the app.\n\n### List the currently loaded models\n\n```bash\nlms ps\n```\n\nLearn more about [`lms ps`](/docs/cli/local-models/ps).\n\n### Load a model (with options)\n\n```bash\nlms load [--gpu=max|auto|0.0-1.0] [--context-length=1-N]\n```\n\n`--gpu=1.0` means 'attempt to offload 100% of the computation to the GPU'.\n\n- Optionally, assign an identifier to your local LLM:\n\n```bash\nlms load openai/gpt-oss-20b --identifier=\"my-model-name\"\n```\n\nThis is useful if you want to keep the model identifier consistent.\n\n### Unload a model\n```\nlms unload [--all]\n```\n\nLearn more about [`lms load and unload`](/docs/cli/local-models/load).\n"])</script><script>self.__next_f.push([1,"1bd:{\"metadata\":\"$1be\",\"prettyName\":\"Introduction\",\"content\":\"$1bf\",\"pageRelUrl\":\"3_cli/index.md\"}\n1c1:{\"title\":\"Contributing\",\"description\":\"Learn where to file issues and how to contribute to the `lms` CLI.\",\"index\":2}\n1c0:{\"metadata\":\"$1c1\",\"prettyName\":\"Contributing\",\"content\":\"\\nThe `lms` CLI is open source on GitHub: https://github.com/lmstudio-ai/lms\\n\\nIf you spot a bug, want to request a feature, or plan to contribute:\\n\\n- File issues or feature requests in the GitHub repository.\\n- Open pull requests against the `main` branch with a concise summary and testing notes.\\n- Review the repository README for setup instructions and coding standards.\\n\",\"pageRelUrl\":\"3_cli/contributing.md\"}\n1c4:{\"title\":\"`lms chat`\",\"description\":\"Start a chat session with a local model from the command line.\",\"index\":1}\n1c5:T614,\nUse `lms chat` to talk to a local model directly in the terminal. This is handy for quick experiments or scripting.\n\n### Flags \n```lms_params\n- name: \"[model]\"\n  type: \"string\"\n  optional: true\n  description: \"Identifier of the model to use. If omitted, you will be prompted to pick one.\"\n- name: \"-p, --prompt\"\n  type: \"string\"\n  optional: true\n  description: \"Send a one-off prompt and print the response to stdout before exiting\"\n- name: \"-s, --system-prompt\"\n  type: \"string\"\n  optional: true\n  description: \"Custom system prompt for the chat\"\n- name: \"--stats\"\n  type: \"flag\"\n  optional: true\n  description: \"Show detailed prediction statistics after each response\"\n- name: \"--ttl\"\n  type: \"number\"\n  optional: true\n  description: \"Seconds to keep the model loaded after the chat ends (default: 3600)\"\n```\n\n### Start an interactive chat\n\n```shell\nlms chat\n```\n\nYou will be prompted to pick a model if one is not provided.\n\n### Chat with a specific model\n\n```shell\nlms chat my-model\n```\n\n### Send a single prompt and exit\n\nUse `-p` to print the response to stdout and exit instead of staying interactive:\n\n```shell\nlms chat my-model -p \"Summarize this release note\"\n```\n\n### Set a system prompt\n\n```shell\nlms chat m"])</script><script>self.__next_f.push([1,"y-model -s \"You are a terse assistant. Reply in two sentences.\"\n```\n\n### Keep the model loaded after chatting\n\n```shell\nlms chat my-model --ttl 600\n```\n\n### Pipe input from another command\n\n`lms chat` reads from stdin, so you can pipe content directly into a prompt:\n\n```shell\ncat my_file.txt | lms chat -p \"Summarize this, please\"\n```\n1c3:{\"metadata\":\"$1c4\",\"prettyName\":\"`lms chat`\",\"content\":\"$1c5\",\"pageRelUrl\":\"3_cli/0_local-models/chat.md\",\"sectionKey\":\"local-models\",\"sectionPrettyName\":\"Local Models\"}\n1c7:{\"title\":\"`lms get`\",\"description\":\"Search and download models from the command line.\",\"index\":2}\n1c8:T75e,\nThe `lms get` command allows you to search and download models from online repositories. If no model is specified, it shows staff-picked recommendations.\n\nModels you download via `lms get` will be stored in your LM Studio model directory. \n\n### Flags \n```lms_params\n- name: \"[modelName]\"\n  type: \"string\"\n  optional: true\n  description: \"The model to download. If omitted, staff picks are shown. For models with multiple quantizations, append '@' (e.g., 'llama-3.1-8b@q4_k_m').\"\n- name: \"--mlx\"\n  type: \"flag\"\n  optional: true\n  description: \"Include only MLX models in search results. If either '--mlx' or '--gguf' is set, only matching formats are shown; otherwise results match installed runtimes.\"\n- name: \"--gguf\"\n  type: \"flag\"\n  optional: true\n  description: \"Include only GGUF models in search results. If either '--mlx' or '--gguf' is set, only matching formats are shown; otherwise results match installed runtimes.\"\n- name: \"-n, --limit\"\n  type: \"number\"\n  optional: true\n  description: \"Limit the number of model options shown.\"\n- name: \"--always-show-all-results\"\n  type: \"flag\"\n  optional: true\n  description: \"Always prompt you to choose from search results, even when there's an exact match.\"\n- name: \"-a, --always-show-download-options\"\n  type: \"flag\"\n  optional: true\n  description: \"Always prompt you to choose a quantization, even when an exact match is auto-selected.\"\n```\n\n## Download a model\n\nDownload a"])</script><script>self.__next_f.push([1," model by name:\n\n```shell\nlms get llama-3.1-8b\n```\n\n### Specify quantization\n\nDownload a specific model quantization:\n\n```shell\nlms get llama-3.1-8b@q4_k_m\n```\n\n### Filter by format\n\nShow only MLX or GGUF models:\n\n```shell\nlms get --mlx\nlms get --gguf\n```\n\n### Control search results\n\nLimit the number of results:\n\n```shell\nlms get --limit 5\n```\n\nAlways show all options:\n\n```shell\nlms get --always-show-all-results\nlms get --always-show-download-options\n```\n1c6:{\"metadata\":\"$1c7\",\"prettyName\":\"`lms get`\",\"content\":\"$1c8\",\"pageRelUrl\":\"3_cli/0_local-models/get.md\",\"sectionKey\":\"local-models\",\"sectionPrettyName\":\"Local Models\"}\n1ca:{\"title\":\"`lms load`\",\"description\":\"Load or unload models, set context length, GPU offload, TTL, or estimate memory usage without loading.\",\"index\":3}\n1cb:T101c,"])</script><script>self.__next_f.push([1,"\nThe `lms load` command loads a model into memory. You can optionally set parameters such as context length, GPU offload, and TTL. This guide also covers unloading models with `lms unload`.\n\n### Flags \n```lms_params\n- name: \"[path]\"\n  type: \"string\"\n  optional: true\n  description: \"The path of the model to load. If not provided, you will be prompted to select one\"\n- name: \"--ttl\"\n  type: \"number\"\n  optional: true\n  description: \"If provided, when the model is not used for this number of seconds, it will be unloaded\"\n- name: \"--gpu\"\n  type: \"string\"\n  optional: true\n  description: \"How much to offload to the GPU. Values: 0-1, off, max\"\n- name: \"--context-length\"\n  type: \"number\"\n  optional: true\n  description: \"The number of tokens to consider as context when generating text\"\n- name: \"--identifier\"\n  type: \"string\"\n  optional: true\n  description: \"The identifier to assign to the loaded model for API reference\"\n- name: \"--estimate-only\"\n  type: \"boolean\"\n  optional: true\n  description: \"Print a resource (memory) estimate and exit without loading the model\"\n```\n\n## Load a model\n\nLoad a model into memory by running the following command:\n\n```shell\nlms load \u003cmodel_key\u003e\n```\n\nYou can find the `model_key` by first running [`lms ls`](/docs/cli/local-models/ls) to list your locally downloaded models.\n\n### Set a custom identifier\n\nOptionally, you can assign a custom identifier to the loaded model for API reference:\n\n```shell\nlms load \u003cmodel_key\u003e --identifier \"my-custom-identifier\"\n```\n\nYou will then be able to refer to this model by the identifier `my_model` in subsequent commands and API calls (`model` parameter).\n\n### Set context length\n\nYou can set the context length when loading a model using the `--context-length` flag:\n\n```shell\nlms load \u003cmodel_key\u003e --context-length 4096\n```\n\nThis determines how many tokens the model will consider as context when generating text.\n\n### Set GPU offload\n\nControl GPU memory usage with the `--gpu` flag:\n\n```shell\nlms load \u003cmodel_key\u003e --gpu 0.5    # Offload 50% of layers to GPU\nlms load \u003cmodel_key\u003e --gpu max    # Offload all layers to GPU\nlms load \u003cmodel_key\u003e --gpu off    # Disable GPU offloading\n```\n\nIf not specified, LM Studio will automatically determine optimal GPU usage.\n\n### Set TTL\n\nSet an auto-unload timer with the `--ttl` flag (in seconds):\n\n```shell\nlms load \u003cmodel_key\u003e --ttl 3600   # Unload after 1 hour of inactivity\n```\n\n### Estimate resources without loading\n\nPreview memory requirements before loading a model using `--estimate-only`:\n\n```shell\nlms load --estimate-only \u003cmodel_key\u003e\n```\n\nOptional flags such as `--context-length` and `--gpu` are honored and reflected in the estimate. The estimator accounts for factors like context length, flash attention, and whether the model is visionâ€‘enabled.\n\nExample:\n\n```bash\n$ lms load --estimate-only gpt-oss-120b\nModel: openai/gpt-oss-120b\nEstimated GPU Memory:   65.68 GB\nEstimated Total Memory: 65.68 GB\n\nEstimate: This model may be loaded based on your resource guardrails settings.\n```\n\n## Unload models\n\nUse `lms unload` to remove models from memory.\n\n### Flags \n```lms_params\n- name: \"[model_key]\"\n  type: \"string\"\n  optional: true\n  description: \"The key of the model to unload. If not provided, you will be prompted to select one\"\n- name: \"--all\"\n  type: \"flag\"\n  optional: true\n  description: \"Unload all currently loaded models\"\n- name: \"--host\"\n  type: \"string\"\n  optional: true\n  description: \"The host address of a remote LM Studio instance to connect to\"\n```\n\n### Unload a specific model\n\n```shell\nlms unload \u003cmodel_key\u003e\n```\n\nIf no model key is provided, you will be prompted to select from currently loaded models.\n\n### Unload all models\n\n```shell\nlms unload --all\n```\n\n### Unload from a remote LM Studio instance\n\n```shell\nlms unload \u003cmodel_key\u003e --host \u003chost\u003e\n```\n\n## Operate on a remote LM Studio instance\n\n`lms load` supports the `--host` flag to connect to a remote LM Studio instance. \n\n```shell\nlms load \u003cmodel_key\u003e --host \u003chost\u003e\n```\n\nFor this to work, the remote LM Studio instance must be running and accessible from your local machine, e.g. be accessible on the same subnet.\n"])</script><script>self.__next_f.push([1,"1c9:{\"metadata\":\"$1ca\",\"prettyName\":\"`lms load`\",\"content\":\"$1cb\",\"pageRelUrl\":\"3_cli/0_local-models/load.md\",\"sectionKey\":\"local-models\",\"sectionPrettyName\":\"Local Models\"}\n1cd:{\"title\":\"`lms ls`\",\"description\":\"List all downloaded models in your LM Studio installation.\",\"index\":4}\n1ce:T836,"])</script><script>self.__next_f.push([1,"\nThe `lms ls` command displays a list of all models downloaded to your machine, including their size, architecture, and parameters.\n\n### Flags \n\n```lms_params\n- name: \"--llm\"\n  type: \"flag\"\n  optional: true\n  description: \"Show only LLMs. When not set, all models are shown\"\n- name: \"--embedding\"\n  type: \"flag\"\n  optional: true\n  description: \"Show only embedding models\"\n- name: \"--json\"\n  type: \"flag\"\n  optional: true\n  description: \"Output the list in JSON format\"\n- name: \"--detailed\"\n  type: \"flag\"\n  optional: true\n  description: \"Show detailed information about each model\"\n```\n\n## List all models\n\nShow all downloaded models:\n\n```shell\nlms ls\n```\n\nExample output:\n\n```\nYou have 47 models, taking up 160.78 GB of disk space.\n\nLLMs (Large Language Models)                       PARAMS      ARCHITECTURE           SIZE\nlmstudio-community/meta-llama-3.1-8b-instruct          8B         Llama            4.92 GB\nhugging-quants/llama-3.2-1b-instruct                   1B         Llama            1.32 GB\nmistral-7b-instruct-v0.3                                         Mistral           4.08 GB\nzeta                                                   7B         Qwen2            4.09 GB\n\n... (abbreviated in this example) ...\n\nEmbedding Models                                   PARAMS      ARCHITECTURE           SIZE\ntext-embedding-nomic-embed-text-v1.5@q4_k_m                     Nomic BERT        84.11 MB\ntext-embedding-bge-small-en-v1.5                     33M           BERT           24.81 MB\n```\n\n### Filter by model type\n\nList only LLM models:\n\n```shell\nlms ls --llm\n```\n\nList only embedding models:\n\n```shell\nlms ls --embedding\n```\n\n### Additional output formats\n\nGet detailed information about models:\n\n```shell\nlms ls --detailed\n```\n\nOutput in JSON format:\n\n```shell\nlms ls --json\n```\n\n## Operate on a remote LM Studio instance\n\n`lms ls` supports the `--host` flag to connect to a remote LM Studio instance:\n\n```shell\nlms ls --host \u003chost\u003e\n```\n\nFor this to work, the remote LM Studio instance must be running and accessible from your local machine, e.g. be accessible on the same subnet.\n"])</script><script>self.__next_f.push([1,"1cc:{\"metadata\":\"$1cd\",\"prettyName\":\"`lms ls`\",\"content\":\"$1ce\",\"pageRelUrl\":\"3_cli/0_local-models/ls.md\",\"sectionKey\":\"local-models\",\"sectionPrettyName\":\"Local Models\"}\n1d0:{\"title\":\"`lms ps`\",\"description\":\"Show information about currently loaded models from the command line.\",\"index\":5}\n1cf:{\"metadata\":\"$1d0\",\"prettyName\":\"`lms ps`\",\"content\":\"\\nThe `lms ps` command displays information about all models currently loaded in memory.\\n\\n## List loaded models\\n\\nShow all currently loaded models:\\n\\n```shell\\nlms ps\\n```\\n\\nExample output:\\n```\\n   LOADED MODELS\\n\\nIdentifier: unsloth/deepseek-r1-distill-qwen-1.5b\\n  â€¢ Type:  LLM\\n  â€¢ Path: unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf\\n  â€¢ Size: 1.12 GB\\n  â€¢ Architecture: Qwen2\\n```\\n\\n### JSON output\\n\\nGet the list in machine-readable format:\\n```shell\\nlms ps --json\\n```\\n\\n## Operate on a remote LM Studio instance\\n\\n`lms ps` supports the `--host` flag to connect to a remote LM Studio instance:\\n\\n```shell\\nlms ps --host \u003chost\u003e\\n```\\n\\nFor this to work, the remote LM Studio instance must be running and accessible from your local machine, e.g. be accessible on the same subnet.\\n\",\"pageRelUrl\":\"3_cli/0_local-models/ps.md\",\"sectionKey\":\"local-models\",\"sectionPrettyName\":\"Local Models\"}\n1d2:{\"title\":\"`lms import`\",\"description\":\"Import a local model file into your LM Studio models directory.\",\"index\":6}\n1d3:T644,\nUse `lms import` to bring an existing model file into LM Studio without downloading it.\n\n### Flags \n```lms_params\n- name: \"\u003cfile-path\u003e\"\n  type: \"string\"\n  optional: false\n  description: \"Path to the model file to import\"\n- name: \"--user-repo\"\n  type: \"string\"\n  optional: true\n  description: \"Set the target folder as \u003cuser\u003e/\u003crepo\u003e. Skips the categorization prompts.\"\n- name: \"-y, --yes\"\n  type: \"flag\"\n  optional: true\n  description: \"Skip confirmations and try to infer the model location from the file name\"\n- name: \"-c, --copy\"\n  type: \"flag\"\n  optional: true\n  description: \"Copy the file instead of moving it\"\n"])</script><script>self.__next_f.push([1,"- name: \"-L, --hard-link\"\n  type: \"flag\"\n  optional: true\n  description: \"Create a hard link instead of moving or copying the file\"\n- name: \"-l, --symbolic-link\"\n  type: \"flag\"\n  optional: true\n  description: \"Create a symbolic link instead of moving or copying the file\"\n- name: \"--dry-run\"\n  type: \"flag\"\n  optional: true\n  description: \"Do not perform the import, just show what would be done\"\n```\n\nOnly one of `--copy`, `--hard-link`, or `--symbolic-link` can be used at a time. If none is provided, `lms import` moves the file by default.\n\n### Import a model file\n\n```shell\nlms import ~/Downloads/model.gguf\n```\n\n### Keep the original file\n\n```shell\nlms import ~/Downloads/model.gguf --copy\n```\n\n### Pick the target folder yourself\n\nUse `--user-repo` to skip prompts and place the model in the chosen namespace:\n\n```shell\nlms import ~/Downloads/model.gguf --user-repo my-user/custom-models\n```\n\n### Dry run before importing\n\n```shell\nlms import ~/Downloads/model.gguf --dry-run\n```\n1d1:{\"metadata\":\"$1d2\",\"prettyName\":\"`lms import`\",\"content\":\"$1d3\",\"pageRelUrl\":\"3_cli/0_local-models/import.md\",\"sectionKey\":\"local-models\",\"sectionPrettyName\":\"Local Models\"}\n1c2:{\"chat\":\"$1c3\",\"get\":\"$1c6\",\"load\":\"$1c9\",\"ls\":\"$1cc\",\"ps\":\"$1cf\",\"import\":\"$1d1\"}\n1d6:{\"title\":\"`lms server start`\",\"description\":\"Start the LM Studio local server with customizable port and logging options.\",\"index\":1}\n1d7:T41d,\nThe `lms server start` command launches the LM Studio local server, allowing you to interact with loaded models via HTTP API calls.\n\n### Flags \n```lms_params\n- name: \"--port\"\n  type: \"number\"\n  optional: true\n  description: \"Port to run the server on. If not provided, uses the last used port\"\n- name: \"--cors\"\n  type: \"flag\"\n  optional: true\n  description: \"Enable CORS support for web application development. When not set, CORS is disabled\"\n```\n\n## Start the server\n\nStart the server with default settings:\n\n```shell\nlms server start\n```\n\n### Specify a custom port\n\nRun the server on a specific port:\n\n```shell\nlms server start --port 3000\n```\n\n"])</script><script>self.__next_f.push([1,"### Enable CORS support\n\nFor usage with web applications or some VS Code extensions, you may need to enable CORS support:\n\n```shell\nlms server start --cors\n```\n\nNote that enabling CORS may expose your server to security risks, so use it only when necessary.\n\n### Check the server status\n\nSee [`lms server status`](/docs/cli/serve/server-status) for more information on checking the status of the server.\n1d5:{\"metadata\":\"$1d6\",\"prettyName\":\"`lms server start`\",\"content\":\"$1d7\",\"pageRelUrl\":\"3_cli/1_serve/server-start.md\",\"sectionKey\":\"serve\",\"sectionPrettyName\":\"serve\"}\n1d9:{\"title\":\"`lms server status`\",\"description\":\"Check the status of your running LM Studio server instance.\",\"index\":2}\n1da:T56b,\nThe `lms server status` command displays the current status of the LM Studio local server, including whether it's running and its configuration.\n\n### Flags \n```lms_params\n- name: \"--json\"\n  type: \"flag\"\n  optional: true\n  description: \"Output the status in JSON format\"\n- name: \"--verbose\"\n  type: \"flag\"\n  optional: true\n  description: \"Enable detailed logging output\"\n- name: \"--quiet\"\n  type: \"flag\"\n  optional: true\n  description: \"Suppress all logging output\"\n- name: \"--log-level\"\n  type: \"string\"\n  optional: true\n  description: \"The level of logging to use. Defaults to 'info'\"\n```\n\n## Check server status\n\nGet the basic status of the server:\n\n```shell\nlms server status\n```\n\nExample output:\n```\nThe server is running on port 1234.\n```\n\n### Example usage\n\n```console\nâžœ  ~ lms server start\nStarting server...\nWaking up LM Studio service...\nSuccess! Server is now running on port 1234\n\nâžœ  ~ lms server status\nThe server is running on port 1234.\n```\n\n### JSON output\n\nGet the status in machine-readable JSON format:\n\n```shell\nlms server status --json --quiet\n```\n\nExample output:\n```json\n{\"running\":true,\"port\":1234}\n```\n\n### Control logging output\n\nAdjust logging verbosity:\n\n```shell\nlms server status --verbose\nlms server status --quiet\nlms server status --log-level debug\n```\n\nYou can only use one logging control flag at a time ("])</script><script>self.__next_f.push([1,"`--verbose`, `--quiet`, or `--log-level`).\n1d8:{\"metadata\":\"$1d9\",\"prettyName\":\"`lms server status`\",\"content\":\"$1da\",\"pageRelUrl\":\"3_cli/1_serve/server-status.md\",\"sectionKey\":\"serve\",\"sectionPrettyName\":\"serve\"}\n1dc:{\"title\":\"`lms server stop`\",\"description\":\"Stop the running LM Studio server instance.\",\"index\":3}\n1db:{\"metadata\":\"$1dc\",\"prettyName\":\"`lms server stop`\",\"content\":\"\\nThe `lms server stop` command gracefully stops the running LM Studio server.\\n\\n```shell\\nlms server stop\\n```\\n\\nExample output:\\n```\\nStopped the server on port 1234.\\n```\\n\\nAny active request will be terminated when the server is stopped. You can restart the server using [`lms server start`](/docs/cli/serve/server-start).\\n\",\"pageRelUrl\":\"3_cli/1_serve/server-stop.md\",\"sectionKey\":\"serve\",\"sectionPrettyName\":\"serve\"}\n1de:{\"title\":\"`lms log stream`\",\"description\":\"Stream logs from LM Studio. Useful for debugging prompts sent to the model.\",\"index\":4}\n1df:T547,\n`lms log stream` lets you inspect the exact strings LM Studio sends to and receives from models, and (new in 0.3.26) stream server logs. This is useful for debugging prompt templates, model IO, and server operations.\n\n### Flags\n\n```lms_params\n- name: \"-s, --source\"\n  type: \"string\"\n  optional: true\n  description: \"Source of logs: model or server (default: model)\"\n- name: \"--stats\"\n  type: \"flag\"\n  optional: true\n  description: \"Print prediction stats when available\"\n- name: \"--filter\"\n  type: \"string\"\n  optional: true\n  description: \"Filter for model source: input, output, or both\"\n- name: \"--json\"\n  type: \"flag\"\n  optional: true\n  description: \"Output logs as JSON (newline separated)\"\n```\n\n### Quick start\n\nStream model IO (default):\n\n```shell\nlms log stream\n```\n\nStream server logs:\n\n```shell\nlms log stream --source server\n```\n\n### Filter model logs\n\n```bash\n# Only the formatted user input\nlms log stream --source model --filter input\n\n# Only the model output (emitted once the message completes)\nlms log stream --source model --filter output\n\n# Both directions\nlms log stream -"])</script><script>self.__next_f.push([1,"-source model --filter input,output\n```\n\n### JSON output and stats\n\nEmit JSON:\n\n  ```shell\n  lms log stream --source model --filter input,output --json\n  ```\n\nInclude prediction stats:\n\n  ```shell\n  lms log stream --source model --filter output --stats\n  ```\n1dd:{\"metadata\":\"$1de\",\"prettyName\":\"`lms log stream`\",\"content\":\"$1df\",\"pageRelUrl\":\"3_cli/1_serve/log-stream.md\",\"sectionKey\":\"serve\",\"sectionPrettyName\":\"serve\"}\n1d4:{\"server-start\":\"$1d5\",\"server-status\":\"$1d8\",\"server-stop\":\"$1db\",\"log-stream\":\"$1dd\"}\n1e2:{\"title\":\"`lms runtime`\",\"description\":\"Manage LM Studio inference runtimes from the CLI.\",\"index\":1}\n1e1:{\"metadata\":\"$1e2\",\"prettyName\":\"`lms runtime`\",\"content\":\"\\nUse `lms runtime` to list, download, switch, or remove inference runtimes without opening the app.\\n\\n### Commands\\n\\n- `lms runtime ls` â€” list installed runtimes.\\n- `lms runtime get` â€” download a runtime.\\n- `lms runtime select` â€” set the active runtime.\\n- `lms runtime remove` â€” uninstall a runtime.\\n- `lms runtime update` â€” update an installed runtime.\\n\\n### List installed runtimes\\n\\n```shell\\nlms runtime ls\\n```\\n\\n### Download a runtime\\n\\n```shell\\nlms runtime get\\n```\\n\\n### Switch to a runtime\\n\\n```shell\\nlms runtime select\\n```\\n\\nFollow the interactive prompts to choose the version you want.\\n\",\"pageRelUrl\":\"3_cli/2_runtime/runtime.md\",\"sectionKey\":\"runtime\",\"sectionPrettyName\":\"runtime\"}\n1e0:{\"runtime\":\"$1e1\"}\n1e5:{\"title\":\"`lms clone`\",\"description\":\"Clone an artifact from LM Studio Hub to a local folder (beta).\",\"index\":1}\n1e4:{\"metadata\":\"$1e5\",\"prettyName\":\"`lms clone`\",\"content\":\"\\nUse `lms clone` to copy an artifact from LM Studio Hub onto your machine.\\n\\n### Flags\\n```lms_params\\n- name: \\\"\u003cartifact\u003e\\\"\\n  type: \\\"string\\\"\\n  optional: false\\n  description: \\\"Artifact identifier in the form owner/name\\\"\\n- name: \\\"[path]\\\"\\n  type: \\\"string\\\"\\n  optional: true\\n  description: \\\"Destination folder. Defaults to a new folder named after the artifact.\\\"\\n```\\n\\nIf no path is provided, `lms clone owner/name` crea"])</script><script>self.__next_f.push([1,"tes a folder called `name` in the current directory. The command exits if the target path already exists.\\n\\n### Clone the latest revision\\n\\n```shell\\nlms clone alice/sample-plugin\\n```\\n\\n### Clone into a specific directory\\n\\n```shell\\nlms clone alice/sample-plugin ./my-folder\\n```\\n\",\"pageRelUrl\":\"3_cli/3_develop-and-publish/clone.md\",\"sectionKey\":\"develop-and-publish\",\"sectionPrettyName\":\"Develop and Publish (Beta)\"}\n1e7:{\"title\":\"`lms push` (Beta)\",\"description\":\"Upload the current folder's artifact to LM Studio Hub (beta).\",\"index\":2}\n1e8:T4fa,\nRun `lms push` from inside a [plugin](/docs/typescript/plugins), [preset](/docs/app/presets), or [`model.yaml`](/docs/app/modelyaml) project to publish a new revision. If a `model.yaml` exists, the CLI will generate a `manifest.json` for you before pushing. \n\nFor plugins, the CLI will ask for confirmation unless you pass `-y`.\n\n### Publish the current folder\n\n```shell\nlms push\n```\n\n### Flags\n```lms_params\n- name: \"--description\"\n  type: \"string\"\n  optional: true\n  description: \"Override the artifact description for this push\"\n- name: \"--overrides\"\n  type: \"string\"\n  optional: true\n  description: \"JSON string to override manifest fields (parsed with JSON.parse)\"\n- name: \"-y, --yes\"\n  type: \"flag\"\n  optional: true\n  description: \"Suppress confirmations and warnings\"\n- name: \"--private\"\n  type: \"flag\"\n  optional: true\n  description: \"Mark the artifact as private when first published\"\n- name: \"--write-revision\"\n  type: \"flag\"\n  optional: true\n  description: \"Write the returned revision number to manifest.json\"\n```\n\n\n### Advanced \n\n#### Publish quietly and keep the revision in manifest.json\n\n```shell\nlms push -y --write-revision\n```\n\n#### Override metadata for this upload\n\n```shell\nlms push --description \"New beta build\" --overrides '{\"tags\": [\"beta\"]}'\n```\n1e6:{\"metadata\":\"$1e7\",\"prettyName\":\"`lms push`\",\"content\":\"$1e8\",\"pageRelUrl\":\"3_cli/3_develop-and-publish/push.md\",\"sectionKey\":\"develop-and-publish\",\"sectionPrettyName\":\"Develop and Publish (Beta)\"}\n1ea:{\"title\":\"`"])</script><script>self.__next_f.push([1,"lms dev` (Beta)\",\"description\":\"Start a plugin dev server or install a local plugin (beta).\",\"index\":3}\n1e9:{\"metadata\":\"$1ea\",\"prettyName\":\"`lms dev`\",\"content\":\"\\nUse `lms dev` inside a plugin project to run a local dev server that rebuilds and reloads on file changes.\\n\\nThis feature is a part of LM Studio [Plugins](/docs/typescript/plugins), currently in private beta.\\n\\n### Run the dev plugin server\\n\\n```shell\\nlms dev\\n```\\n\\nThis verifies `manifest.json`, installs dependencies if needed, and starts a watcher that rebuilds the plugin on changes. Supported runners: Node/ECMAScript and Deno.\\n\\n### Install the plugin instead of running dev\\n\\n```shell\\nlms dev --install\\n```\\n\\n### Flags\\n```lms_params\\n- name: \\\"-i, --install\\\"\\n  type: \\\"flag\\\"\\n  optional: true\\n  description: \\\"Install the plugin into LM Studio instead of running the dev server\\\"\\n- name: \\\"--no-notify\\\"\\n  type: \\\"flag\\\"\\n  optional: true\\n  description: \\\"Do not show the \\\\\\\"Plugin started\\\\\\\" notification in LM Studio\\\"\\n```\",\"pageRelUrl\":\"3_cli/3_develop-and-publish/dev.md\",\"sectionKey\":\"develop-and-publish\",\"sectionPrettyName\":\"Develop and Publish (Beta)\"}\n1ec:{\"title\":\"`lms login`\",\"description\":\"Authenticate with LM Studio Hub (beta).\",\"index\":4}\n1ed:T410,\nUse `lms login` to authenticate the CLI with LM Studio Hub.\n\n### Sign in with the browser\n\n```shell\nlms login\n```\n\nThe CLI opens a browser window for authentication. If a browser cannot be opened automatically, copy the printed URL into your browser.\n\n### \"CI style\" login with pre-authenticated keys\n\n```bash\nlms login --with-pre-authenticated-keys \\\n  --key-id \u003cKEY_ID\u003e \\\n  --public-key \u003cPUBLIC_KEY\u003e \\\n  --private-key \u003cPRIVATE_KEY\u003e \n```\n\n### Advanced Flags\n```lms_params\n- name: \"--with-pre-authenticated-keys\"\n  type: \"flag\"\n  optional: true\n  description: \"Authenticate using pre-generated keys (CI/CD). Requires --key-id, --public-key, and --private-key.\"\n- name: \"--key-id\"\n  type: \"string\"\n  optional: true\n  description: \"Key ID to use with --with-pre-authenticated-keys\"\n- name: \""])</script><script>self.__next_f.push([1,"--public-key\"\n  type: \"string\"\n  optional: true\n  description: \"Public key to use with --with-pre-authenticated-keys\"\n- name: \"--private-key\"\n  type: \"string\"\n  optional: true\n  description: \"Private key to use with --with-pre-authenticated-keys\"\n```1eb:{\"metadata\":\"$1ec\",\"prettyName\":\"`lms login`\",\"content\":\"$1ed\",\"pageRelUrl\":\"3_cli/3_develop-and-publish/login.md\",\"sectionKey\":\"develop-and-publish\",\"sectionPrettyName\":\"Develop and Publish (Beta)\"}\n1e3:{\"clone\":\"$1e4\",\"push\":\"$1e6\",\"dev\":\"$1e9\",\"login\":\"$1eb\"}\n1bc:{\"\":\"$1bd\",\"contributing\":\"$1c0\",\"local-models\":\"$1c2\",\"serve\":\"$1d4\",\"runtime\":\"$1e0\",\"develop-and-publish\":\"$1e3\"}\n71:{\"app\":\"$72\",\"developer\":\"$bc\",\"python\":\"$f1\",\"typescript\":\"$137\",\"cli\":\"$1bc\"}\n1f9:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L6\",null,{\"buildId\":\"9KZfEg6k4D5s0t0I3H7AV\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"docs\",\"app\"],\"initialTree\":[\"\",{\"children\":[\"(static)\",{\"children\":[\"docs\",{\"children\":[[\"slug\",\"app\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"app\\\"]}\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"(static)\",{\"children\":[\"docs\",{\"children\":[[\"slug\",\"app\",\"c\"],{\"children\":[\"__PAGE__\",{},[[\"$L7\",[\"$\",\"div\",null,{\"className\":\"z-10 flex h-full w-full items-start overflow-y-hidden p-0 px-5 md:pl-[20px] justify-center\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex h-full w-full flex-col justify-start pl-0 z-10 max-w-[950px] flex-none gap-0 pb-[150px] pt-[20px] xl:px-[30px]\",\"children\":[false,[\"$\",\"div\",null,{\"className\":\"flex w-full flex-col gap-1.5 py-5 md:py-5 md:pb-10\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex w-full flex-col gap-1.5\",\"children\":[false,[\"$\",\"div\",null,{\"className\":\"flex w-full flex-row items-center justify-between gap-3\",\"children\":[[\"$\",\"div\",null,{\"className\":\"min-w-0 flex-1\",\"children\":\"$L8\"}],[\"$\",\"div\",null,{\"className\":\"mb-3\",\"children\":[\"$\",\"$L9\",null,{\"markdown\":\"$a\"}]}]]}]]}],\"$Lb\"]}],\"$Lc\",[\"$\",\"div\",null,{\"className\":\"mt-12 flex w-full flex-col items-center justify-center gap-5\",\"children\":[[\"$\",\"div\",null,{\"className\":\"h-7 w-full\"}],[\"$\",\"div\",null,{\"className\":\"w-full rounded border border-border/10 p-2 text-center\",\"children\":\"$Ld\"}]]}]]}]}],null],null],null]},[[null,\"$Le\"],null],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5d898acc654ad377.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/77904bbb8e5c2b67.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"div\",null,{\"className\":\"__variable_2f8338 __variable_06d220 docs-typography\",\"children\":[[\"$\",\"$Lf\",null,{\"documentationArticles\":{\"app\":{\"\":{\"\":{\"metadata\":{\"title\":\"Welcome to LM Studio Docs!\",\"description\":\"Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM Studio.\",\"index\":1},\"prettyName\":\"Welcome\",\"content\":\"$10\",\"pageRelUrl\":\"0_app/0_root/index.md\"},\"system-requirements\":{\"metadata\":{\"title\":\"System Requirements\",\"description\":\"Supported CPU, GPU types for LM Studio on Mac (M1/M2/M3/M4), Windows (x64/ARM), and Linux (x64/ARM64)\",\"index\":3},\"prettyName\":\"System Requirements\",\"content\":\"$11\",\"pageRelUrl\":\"0_app/0_root/system-requirements.md\"},\"offline\":{\"metadata\":{\"title\":\"Offline Operation\",\"description\":\"LM Studio can operate entirely offline, just make sure to get some model files first.\",\"index\":4},\"prettyName\":\"Offline Operation\",\"content\":\"$12\",\"pageRelUrl\":\"0_app/0_root/offline.md\"}},\"basics\":{\"\":{\"metadata\":{\"title\":\"Get started with LM Studio\",\"description\":\"Download and run Large Language Models like Qwen, Mistral, Gemma, or gpt-oss in LM Studio.\",\"index\":1},\"prettyName\":\"Overview\",\"content\":\"$13\",\"pageRelUrl\":\"0_app/1_basics/index.md\",\"sectionKey\":\"basics\",\"sectionPrettyName\":\"Getting Started\"},\"chat\":{\"metadata\":{\"title\":\"Manage chats\",\"description\":\"Manage conversation threads with LLMs\",\"index\":2},\"prettyName\":\"Manage chats\",\"content\":\"$14\",\"pageRelUrl\":\"0_app/1_basics/chat.md\",\"sectionKey\":\"basics\",\"sectionPrettyName\":\"Getting Started\"},\"download-model\":{\"metadata\":{\"title\":\"Download an LLM\",\"description\":\"Discover and download supported LLMs in LM Studio\",\"index\":3},\"prettyName\":\"Download an LLM\",\"content\":\"$15\",\"pageRelUrl\":\"0_app/1_basics/download-model.md\",\"sectionKey\":\"basics\",\"sectionPrettyName\":\"Getting Started\"},\"rag\":{\"metadata\":{\"title\":\"Chat with Documents\",\"description\":\"How to provide local documents to an LLM as additional context\",\"index\":4},\"prettyName\":\"Chat with Documents\",\"content\":\"$16\",\"pageRelUrl\":\"0_app/1_basics/rag.md\",\"sectionKey\":\"basics\",\"sectionPrettyName\":\"Getting Started\"}},\"mcp\":{\"\":{\"metadata\":{\"title\":\"Use MCP Servers\",\"description\":\"Connect MCP servers to LM Studio\",\"index\":1},\"prettyName\":\"Use MCP Servers\",\"content\":\"$17\",\"pageRelUrl\":\"0_app/2_mcp/index.md\",\"sectionKey\":\"mcp\",\"sectionPrettyName\":\"Model Context Protocol (MCP)\"},\"deeplink\":{\"metadata\":{\"title\":\"`Add to LM Studio` Button\",\"description\":\"Add MCP servers to LM Studio using a deeplink\",\"index\":2},\"prettyName\":\"`Add to LM Studio` Button\",\"content\":\"$18\",\"pageRelUrl\":\"0_app/2_mcp/deeplink.md\",\"sectionKey\":\"mcp\",\"sectionPrettyName\":\"Model Context Protocol (MCP)\"}},\"modelyaml\":{\"\":{\"metadata\":{\"title\":\"Introduction to `model.yaml`\",\"description\":\"Describe models with the cross-platform `model.yaml` specification.\",\"index\":5,\"socialCard\":{\"url\":\"https://files.lmstudio.ai/modelyaml-card.jpg\",\"alt\":\"model.yaml logo\"}},\"prettyName\":\"Introduction to `model.yaml`\",\"content\":\"$19\",\"pageRelUrl\":\"0_app/3_modelyaml/index.md\",\"sectionKey\":\"modelyaml\",\"sectionPrettyName\":\"Models (model.yaml)\"},\"publish\":{\"metadata\":{\"title\":\"Publish a `model.yaml`\",\"description\":\"Upload your model definition to the LM Studio Hub.\",\"index\":7},\"prettyName\":\"Publish a `model.yaml`\",\"content\":\"$1a\",\"pageRelUrl\":\"0_app/3_modelyaml/publish.md\",\"sectionKey\":\"modelyaml\",\"sectionPrettyName\":\"Models (model.yaml)\"}},\"presets\":{\"\":{\"metadata\":{\"title\":\"Config Presets\",\"description\":\"Save your system prompts and other parameters as Presets for easy reuse across chats.\",\"index\":1},\"prettyName\":\"Overview\",\"content\":\"$1b\",\"pageRelUrl\":\"0_app/3_presets/index.md\",\"sectionKey\":\"presets\",\"sectionPrettyName\":\"Presets\"},\"import\":{\"metadata\":{\"title\":\"Importing and Sharing\",\"description\":\"You can import preset files directly from disk, or pull presets made by others via URL.\",\"index\":2},\"prettyName\":\"Importing and Sharing\",\"content\":\"$1c\",\"pageRelUrl\":\"0_app/3_presets/import.md\",\"sectionKey\":\"presets\",\"sectionPrettyName\":\"Presets\"},\"publish\":{\"metadata\":{\"title\":\"Publish Your Presets\",\"description\":\"Publish your Presets to the LM Studio Hub. Share your Presets with the community or with your colleagues.\",\"index\":3},\"prettyName\":\"Publish a Preset\",\"content\":\"$1d\",\"pageRelUrl\":\"0_app/3_presets/publish.md\",\"sectionKey\":\"presets\",\"sectionPrettyName\":\"Presets\"},\"pull\":{\"metadata\":{\"title\":\"Pull Updates\",\"description\":\"How to pull the latest revisions of your Presets, or presets you have imported from others.\",\"index\":4},\"prettyName\":\"Pull Updates\",\"content\":\"`Feature In Preview`\\n\\nYou can pull the latest revisions of your Presets, or presets you have imported from others. This is useful for keeping your Presets up to date with the latest changes.\\n\\n\u003chr\u003e\\n\\n## How to Pull Updates\\nClick the `â€¢â€¢â€¢` button in the Preset dropdown and select \\\"Pull\\\" from the menu.\\n\\n\u003cimg src=\\\"/assets/docs/preset-pull-latest.png\\\" data-caption=\\\"Pull the latest revisions of your or imported Presets.\\\" /\u003e\\n\\n## Your Presets vs Others'\\n\\nBoth your published Presets and other downloaded Presets can be pulled and updated the same way.\",\"pageRelUrl\":\"0_app/3_presets/pull.md\",\"sectionKey\":\"presets\",\"sectionPrettyName\":\"Presets\"},\"push\":{\"metadata\":{\"title\":\"Push New Revisions\",\"description\":\"Publish new revisions of your Presets to the LM Studio Hub.\",\"index\":5},\"prettyName\":\"Push New Revisions\",\"content\":\"\\n`Feature In Preview`\\n\\nStarting LM Studio 0.3.15, you can publish your Presets to the LM Studio community. This allows you to share your Presets with others and import Presets from other users.\\n\\nThis feature is early and we would love to hear your feedback. Please report bugs and feedback to bugs@lmstudio.ai.\\n\\n---\\n\\n## Published Presets\\n\\nPresets you share on the LM Studio Hub can be updated.\\n\\n\u003cimg src=\\\"/assets/docs/preset-cloud-indicator.png\\\" data-caption=\\\"Your shared Presets are marked with a cloud icon.\\\" /\u003e\\n\\n## Step 1: Make Changes and Commit\\n\\nMake any changes to your Preset, both in parameters that are already included in the Preset, or by adding new parameters.\\n\\n## Step 2: Click the Push Button\\nOnce changes are committed, you will see a `Push` button. Click it to push your changes to the Hub. \\n\\nPushing changes will result in a new revision of your Preset on the Hub.\\n\\n\u003cimg src=\\\"/assets/docs/preset-push-button.png\\\" data-caption=\\\"Click the Push button to push your changes to the Hub.\\\" /\u003e\\n\",\"pageRelUrl\":\"0_app/3_presets/push.md\",\"sectionKey\":\"presets\",\"sectionPrettyName\":\"Presets\"}},\"advanced\":{\"speculative-decoding\":{\"metadata\":{\"title\":\"Speculative Decoding\",\"description\":\"Speed up generation with a draft model\",\"index\":1},\"prettyName\":\"Speculative Decoding\",\"content\":\"$1e\",\"pageRelUrl\":\"0_app/5_advanced/speculative-decoding.md\",\"sectionKey\":\"advanced\",\"sectionPrettyName\":\"Advanced\"},\"import-model\":{\"metadata\":{\"title\":\"Import Models\",\"description\":\"Use model files you've downloaded outside of LM Studio\",\"index\":6},\"prettyName\":\"Import Models\",\"content\":\"$1f\",\"pageRelUrl\":\"0_app/5_advanced/import-model.md\",\"sectionKey\":\"advanced\",\"sectionPrettyName\":\"Advanced\"},\"per-model\":{\"metadata\":{\"title\":\"Per-model Defaults\",\"description\":\"You can set default settings for each model in LM Studio\",\"index\":null},\"prettyName\":\"Per-model Defaults\",\"content\":\"$20\",\"pageRelUrl\":\"0_app/5_advanced/per-model.md\",\"sectionKey\":\"advanced\",\"sectionPrettyName\":\"Advanced\"},\"prompt-template\":{\"metadata\":{\"title\":\"Prompt Template\",\"description\":\"Optionally set or modify the model's prompt template\",\"index\":null},\"prettyName\":\"Prompt Template\",\"content\":\"$21\",\"pageRelUrl\":\"0_app/5_advanced/prompt-template.md\",\"sectionKey\":\"advanced\",\"sectionPrettyName\":\"Advanced\"}},\"user-interface\":{\"languages\":{\"metadata\":{\"title\":\"LM Studio in your language\",\"description\":\"LM Studio is available in English, Chinese, Spanish, French, German, Korean, Russian, and 26+ more languages.\",\"index\":null},\"prettyName\":\"Languages\",\"content\":\"$22\",\"pageRelUrl\":\"0_app/6_user-interface/languages.md\",\"sectionKey\":\"user-interface\",\"sectionPrettyName\":\"User Interface\"},\"modes\":{\"metadata\":{\"title\":\"User, Power User, or Developer\",\"description\":\"Hide or reveal advanced features\",\"index\":null},\"prettyName\":\"UI Modes\",\"content\":\"\\n### Selecting a UI Mode\\n\\nYou can configure LM Studio to run in increasing levels of configurability.\\n\\nSelect between User, Power User, and Developer.\\n\\n\u003cimg src=\\\"/assets/docs/modes.png\\\" style=\\\"width: 500px; margin-top:30px\\\" data-caption=\\\"Choose a mode at the bottom of the app\\\" /\u003e\\n\\n### Which mode should I choose?\\n\\n#### `User`\\n\\nShow only the chat interface, and auto-configure everything. This is the best choice for beginners or anyone who's happy with the default settings.\\n\\n#### `Power User`\\n\\nUse LM Studio in this mode if you want access to configurable [load](/docs/configuration/load) and [inference](/docs/configuration/inference) parameters as well as advanced chat features such as [insert, edit, \u0026amp; continue](/docs/advanced/context) (for either role, user or assistant).\\n\\n#### `Developer`\\n\\nFull access to all aspects in LM Studio. This includes keyboard shortcuts and development features. Check out the Developer section under Settings for more.\\n\",\"pageRelUrl\":\"0_app/6_user-interface/modes.md\",\"sectionKey\":\"user-interface\",\"sectionPrettyName\":\"User Interface\"},\"themes\":{\"metadata\":{\"title\":\"Color Themes\",\"description\":\"Customize LM Studio's color theme\",\"index\":null},\"prettyName\":\"Color Themes\",\"content\":\"\\n### Selecting a Theme\\n\\nPress `Cmd` + `K` then `T` (macOS) or `Ctrl` + `K` then `T` (Windows/Linux) to open the theme selector.\\n\\nYou can also choose a theme in the Settings tab (`Cmd` + `,` on macOS or `Ctrl` + `,` on Windows/Linux).\\n\\nChoosing the \\\"Auto\\\" option will automatically switch between Light and Dark themes based on your system settings.\\n\",\"pageRelUrl\":\"0_app/6_user-interface/themes.md\",\"sectionKey\":\"user-interface\",\"sectionPrettyName\":\"User Interface\"}}},\"developer\":{\"\":{\"metadata\":{\"title\":\"LM Studio Developer Docs\",\"description\":\"Build with LM Studio's local APIs and SDKs â€” TypeScript, Python, REST, and OpenAIâ€‘compatible endpoints.\",\"index\":1},\"prettyName\":\"Introduction\",\"content\":\"$23\",\"pageRelUrl\":\"1_developer/index.md\"},\"api-changelog\":{\"metadata\":{\"title\":\"API Changelog\",\"description\":\"Updates and changes to the LM Studio API.\",\"index\":2},\"prettyName\":\"API Changelog\",\"content\":\"$24\",\"pageRelUrl\":\"1_developer/api-changelog.md\"},\"core\":{\"ttl-and-auto-evict\":{\"metadata\":{\"title\":\"Idle TTL and Auto-Evict\",\"description\":\"Optionally auto-unload idle models after a certain amount of time (TTL)\",\"index\":1},\"prettyName\":\"Idle TTL and Auto-Evict\",\"content\":\"$25\",\"pageRelUrl\":\"1_developer/0_core/ttl-and-auto-evict.md\",\"sectionKey\":\"core\",\"sectionPrettyName\":\"Core\"},\"headless\":{\"metadata\":{\"title\":\"Run LM Studio as a service (headless)\",\"description\":\"GUI-less operation of LM Studio: run in the background, start on machine login, and load models on demand\",\"index\":2},\"prettyName\":\"Headless Mode\",\"content\":\"$26\",\"pageRelUrl\":\"1_developer/0_core/headless.md\",\"sectionKey\":\"core\",\"sectionPrettyName\":\"Core\"},\"server\":{\"\":{\"metadata\":{\"title\":\"LM Studio as a Local LLM API Server\",\"description\":\"Run an LLM API server on `localhost` with LM Studio\",\"index\":1,\"fullPage\":false},\"prettyName\":\"Running the Server\",\"content\":\"$27\",\"pageRelUrl\":\"1_developer/0_core/0_server/index.md\",\"sectionKey\":\"core\",\"sectionPrettyName\":\"Core\"},\"settings\":{\"metadata\":{\"title\":\"Server Settings\",\"description\":\"Configure server settings for LM Studio API Server\",\"index\":2,\"fullPage\":false},\"prettyName\":\"Server Settings\",\"content\":\"$28\",\"pageRelUrl\":\"1_developer/0_core/0_server/settings.md\",\"sectionKey\":\"core\",\"sectionPrettyName\":\"Core\"},\"serve-on-network\":{\"metadata\":{\"title\":\"Serve on Local Network\",\"description\":\"Allow other devices in your network use this LM Studio API server\",\"index\":3,\"fullPage\":false},\"prettyName\":\"Serve on Local Network\",\"content\":\"\\n\\nEnabling the \\\"Serve on Local Network\\\" option allows the LM Studio API server running on your machine to be accessible by other devices connected to the same local network.\\n\\nThis is useful for scenarios where you want to:\\n- Use a local LLM on your other less powerful devices by connecting them to a more powerful machine running LM Studio.\\n- Let multiple people use a single LM Studio instance on the network.\\n- Use the API from IoT devices, edge computing units, or other services in your local setup.\\n\\nOnce enabled, the server will bind to your local network IP address instead of localhost. The API access URL will be updated accordingly which you can use in your applications.\\n\\n\u003cimg src=\\\"/assets/docs/serve-local-network.png\\\" style=\\\"\\\" data-caption=\\\"Serve LM Studio API Server on Local Network\\\" /\u003e\\n\",\"pageRelUrl\":\"1_developer/0_core/0_server/serve-on-network.md\",\"sectionKey\":\"core\",\"sectionPrettyName\":\"Core\"}}},\"rest\":{\"endpoints\":{\"metadata\":{\"title\":\"REST API v0\",\"description\":\"The REST API includes enhanced stats such as Token / Second and Time To First Token (TTFT), as well as rich information about models such as loaded vs unloaded, max context, quantization, and more.\",\"index\":null},\"prettyName\":\"REST API v0\",\"content\":\"$29\",\"pageRelUrl\":\"1_developer/2_rest/endpoints.md\",\"sectionKey\":\"rest\",\"sectionPrettyName\":\"LM Studio REST API\"}},\"openai-compat\":{\"\":{\"metadata\":{\"title\":\"OpenAI Compatibility Endpoints\",\"description\":\"Send requests to Responses, Chat Completions (text and images), Completions, and Embeddings endpoints.\",\"index\":1},\"prettyName\":\"Overview\",\"content\":\"$2a\",\"pageRelUrl\":\"1_developer/3_openai-compat/index.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"},\"structured-output\":{\"metadata\":{\"title\":\"Structured Output\",\"description\":\"Enforce LLM response formats using JSON schemas.\",\"index\":2},\"prettyName\":\"Structured Output\",\"content\":\"$2b\",\"pageRelUrl\":\"1_developer/3_openai-compat/structured-output.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"},\"tools\":{\"metadata\":{\"title\":\"Tool Use\",\"description\":\"Enable LLMs to interact with external functions and APIs.\",\"index\":2},\"prettyName\":\"Tools and Function Calling\",\"content\":\"$2c\",\"pageRelUrl\":\"1_developer/3_openai-compat/tools.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"},\"models\":{\"metadata\":{\"title\":\"List Models\",\"description\":\"List available models via the OpenAI-compatible endpoint.\",\"index\":3,\"apiInfo\":{\"method\":\"GET\"}},\"prettyName\":\"List Models\",\"content\":\"\\n- Method: `GET`\\n- Returns the models visible to the server. The list may include all downloaded models when Justâ€‘Inâ€‘Time loading is enabled.\\n\\n##### cURL\\n\\n```bash\\ncurl http://localhost:1234/v1/models\\n```\\n\",\"pageRelUrl\":\"1_developer/3_openai-compat/models.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"},\"responses\":{\"metadata\":{\"title\":\"Responses\",\"description\":\"Create responses with support for streaming, reasoning, prior response state, and optional Remote MCP tools.\",\"index\":3,\"apiInfo\":{\"method\":\"POST\"}},\"prettyName\":\"Responses\",\"content\":\"$2d\",\"pageRelUrl\":\"1_developer/3_openai-compat/responses.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"},\"chat-completions\":{\"metadata\":{\"title\":\"Chat Completions\",\"description\":\"Send a chat history and get the assistant's response.\",\"index\":4,\"apiInfo\":{\"method\":\"POST\"}},\"prettyName\":\"Chat Completions\",\"content\":\"\\n- Method: `POST`\\n- Prompt template is applied automatically for chatâ€‘tuned models\\n- Provide inference parameters (temperature, top_p, etc.) in the payload\\n- See OpenAI docs: https://platform.openai.com/docs/api-reference/chat\\n- Tip: keep a terminal open with [`lms log stream`](/docs/cli/serve/log-stream) to inspect model input\\n\\n##### Python example\\n\\n```python\\nfrom openai import OpenAI\\nclient = OpenAI(base_url=\\\"http://localhost:1234/v1\\\", api_key=\\\"lm-studio\\\")\\n\\ncompletion = client.chat.completions.create(\\n  model=\\\"model-identifier\\\",\\n  messages=[\\n    {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"Always answer in rhymes.\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Introduce yourself.\\\"}\\n  ],\\n  temperature=0.7,\\n)\\n\\nprint(completion.choices[0].message)\\n```\\n\\n### Supported payload parameters\\n\\nSee https://platform.openai.com/docs/api-reference/chat/create for parameter semantics.\\n\\n```py\\nmodel\\ntop_p\\ntop_k\\nmessages\\ntemperature\\nmax_tokens\\nstream\\nstop\\npresence_penalty\\nfrequency_penalty\\nlogit_bias\\nrepeat_penalty\\nseed\\n```\\n\",\"pageRelUrl\":\"1_developer/3_openai-compat/chat-completions.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"},\"embeddings\":{\"metadata\":{\"title\":\"Embeddings\",\"description\":\"Generate embedding vectors from input text.\",\"index\":5,\"apiInfo\":{\"method\":\"POST\"}},\"prettyName\":\"Embeddings\",\"content\":\"\\n- Method: `POST`\\n- See OpenAI docs: https://platform.openai.com/docs/api-reference/embeddings\\n\\n##### Python example\\n\\n```python\\nfrom openai import OpenAI\\nclient = OpenAI(base_url=\\\"http://localhost:1234/v1\\\", api_key=\\\"lm-studio\\\")\\n\\ndef get_embedding(text, model=\\\"model-identifier\\\"):\\n   text = text.replace(\\\"\\\\n\\\", \\\" \\\")\\n   return client.embeddings.create(input=[text], model=model).data[0].embedding\\n\\nprint(get_embedding(\\\"Once upon a time, there was a cat.\\\"))\\n```\\n\",\"pageRelUrl\":\"1_developer/3_openai-compat/embeddings.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"},\"completions\":{\"metadata\":{\"title\":\"Completions (Legacy)\",\"description\":\"Text completion for base models (legacy OpenAI endpoint).\",\"index\":6,\"apiInfo\":{\"method\":\"POST\"}},\"prettyName\":\"Completions (Legacy)\",\"content\":\"\\n```lms_warning\\nThis endpoint is no longer supported by OpenAI. LM Studio continues to support it.\\n\\nUsing this endpoint with chatâ€‘tuned models may produce unexpected tokens. Prefer base models.\\n```\\n\\n- Method: `POST`\\n- Prompt template is not applied\\n- See OpenAI docs: https://platform.openai.com/docs/api-reference/completions\\n\",\"pageRelUrl\":\"1_developer/3_openai-compat/completions.md\",\"sectionKey\":\"openai-compat\",\"sectionPrettyName\":\"OpenAI Compatible Endpoints\"}}},\"python\":{\"\":{\"metadata\":{\"title\":\"`lmstudio-python` (Python SDK)\",\"description\":\"Getting started with LM Studio's Python SDK\",\"index\":1},\"prettyName\":\"Introduction\",\"content\":\"$2e\",\"pageRelUrl\":\"1_python/index.md\"},\"getting-started\":{\"project-setup\":{\"metadata\":{\"title\":\"Project Setup\",\"description\":\"Set up your `lmstudio-python` app or script.\",\"index\":2},\"prettyName\":\"Project Setup\",\"content\":\"$2f\",\"pageRelUrl\":\"1_python/1_getting-started/project-setup.md\",\"sectionKey\":\"getting-started\",\"sectionPrettyName\":\"Getting Started\"},\"repl\":{\"metadata\":{\"title\":\"Using `lmstudio-python` in REPL\",\"description\":\"You can use `lmstudio-python` in REPL (Read-Eval-Print Loop) to interact with LLMs, manage models, and more.\",\"index\":2},\"prettyName\":\"REPL Usage\",\"content\":\"$30\",\"pageRelUrl\":\"1_python/1_getting-started/repl.md\",\"sectionKey\":\"getting-started\",\"sectionPrettyName\":\"Getting Started\"}},\"llm-prediction\":{\"chat-completion\":{\"metadata\":{\"title\":\"Chat Completions\",\"description\":\"APIs for a multi-turn chat conversations with an LLM\",\"index\":2},\"prettyName\":\"Chat\",\"content\":\"$31\",\"pageRelUrl\":\"1_python/1_llm-prediction/chat-completion.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"image-input\":{\"metadata\":{\"title\":\"Image Input\",\"description\":\"API for passing images as input to the model\",\"index\":2},\"prettyName\":\"Image Input\",\"content\":\"$32\",\"pageRelUrl\":\"1_python/1_llm-prediction/image-input.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"cancelling-predictions\":{\"metadata\":{\"title\":\"Cancelling Predictions\",\"description\":\"Stop an ongoing prediction in `lmstudio-python`\",\"index\":4},\"prettyName\":\"Cancelling Predictions\",\"content\":\"$33\",\"pageRelUrl\":\"1_python/1_llm-prediction/cancelling-predictions.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"structured-response\":{\"metadata\":{\"title\":\"Structured Response\",\"description\":\"Enforce a structured response from the model using Pydantic models or JSON Schema\",\"index\":4},\"prettyName\":\"Structured Response\",\"content\":\"$34\",\"pageRelUrl\":\"1_python/1_llm-prediction/structured-response.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"speculative-decoding\":{\"metadata\":{\"title\":\"Speculative Decoding\",\"description\":\"API to use a draft model in speculative decoding in `lmstudio-python`\",\"index\":5},\"prettyName\":\"Speculative Decoding\",\"content\":\"$35\",\"pageRelUrl\":\"1_python/1_llm-prediction/speculative-decoding.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"completion\":{\"metadata\":{\"title\":\"Text Completions\",\"description\":\"Provide a string input for the model to complete\",\"index\":null},\"prettyName\":\"Text Completions\",\"content\":\"$36\",\"pageRelUrl\":\"1_python/1_llm-prediction/completion.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"parameters\":{\"metadata\":{\"title\":\"Configuring the Model\",\"description\":\"APIs for setting inference-time and load-time parameters for your model\",\"index\":null},\"prettyName\":\"Configuration Parameters\",\"content\":\"$37\",\"pageRelUrl\":\"1_python/1_llm-prediction/parameters.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"working-with-chats\":{\"metadata\":{\"title\":\"Working with Chats\",\"description\":\"APIs for representing a chat conversation with an LLM\",\"index\":null},\"prettyName\":\"Working with Chats\",\"content\":\"$38\",\"pageRelUrl\":\"1_python/1_llm-prediction/working-with-chats.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}},\"agent\":{\"act\":{\"metadata\":{\"title\":\"The `.act()` call\",\"description\":\"How to use the `.act()` call to turn LLMs into autonomous agents that can perform tasks on your local machine.\",\"index\":1},\"prettyName\":\"The `.act()` call\",\"content\":\"$39\",\"pageRelUrl\":\"1_python/2_agent/act.md\",\"sectionKey\":\"agent\",\"sectionPrettyName\":\"Agentic Flows\"},\"tools\":{\"metadata\":{\"title\":\"Tool Definition\",\"description\":\"Define tools to be called by the LLM, and pass them to the model in the `act()` call.\",\"index\":2},\"prettyName\":\"Tool Definition\",\"content\":\"$3a\",\"pageRelUrl\":\"1_python/2_agent/tools.md\",\"sectionKey\":\"agent\",\"sectionPrettyName\":\"Agentic Flows\"}},\"embedding\":{\"\":{\"metadata\":{\"title\":\"Embedding\",\"description\":\"Generate text embeddings from input text\",\"index\":1},\"prettyName\":\"Generating embedding vectors\",\"content\":\"\\nGenerate embeddings for input text. Embeddings are vector representations of text that capture semantic meaning. Embeddings are a building block for RAG (Retrieval-Augmented Generation) and other similarity-based tasks.\\n\\n### Prerequisite: Get an Embedding Model\\n\\nIf you don't yet have an embedding model, you can download a model like `nomic-ai/nomic-embed-text-v1.5` using the following command:\\n\\n```bash\\nlms get nomic-ai/nomic-embed-text-v1.5\\n```\\n\\n## Create Embeddings\\n\\nTo convert a string to a vector representation, pass it to the `embed` method on the corresponding embedding model handle.\\n\\n```lms_code_snippet\\n  title: \\\"example.py\\\"\\n  variants:\\n    \\\"Python (convenience API)\\\":\\n      language: python\\n      code: |\\n        import lmstudio as lms\\n\\n        model = lms.embedding_model(\\\"nomic-embed-text-v1.5\\\")\\n\\n        embedding = model.embed(\\\"Hello, world!\\\")\\n\\n```\\n\",\"pageRelUrl\":\"1_python/3_embedding/index.md\",\"sectionKey\":\"embedding\",\"sectionPrettyName\":\"Text Embedding\"}},\"tokenization\":{\"\":{\"metadata\":{\"title\":\"Tokenization\",\"description\":\"Tokenize text using a model's tokenizer\",\"index\":1},\"prettyName\":\"Tokenizing text\",\"content\":\"$3b\",\"pageRelUrl\":\"1_python/4_tokenization/index.md\",\"sectionKey\":\"tokenization\",\"sectionPrettyName\":\"Tokenization\"}},\"manage-models\":{\"list-downloaded\":{\"metadata\":{\"title\":\"List Downloaded Models\",\"description\":\"APIs to list the available models in a given local environment\",\"index\":null},\"prettyName\":\"List Downloaded Models\",\"content\":\"$3c\",\"pageRelUrl\":\"1_python/5_manage-models/list-downloaded.md\",\"sectionKey\":\"manage-models\",\"sectionPrettyName\":\"Manage Models\"},\"list-loaded\":{\"metadata\":{\"title\":\"List Loaded Models\",\"description\":\"Query which models are currently loaded\",\"index\":null},\"prettyName\":\"List Loaded Models\",\"content\":\"$3d\",\"pageRelUrl\":\"1_python/5_manage-models/list-loaded.md\",\"sectionKey\":\"manage-models\",\"sectionPrettyName\":\"Manage Models\"},\"loading\":{\"metadata\":{\"title\":\"Manage Models in Memory\",\"description\":\"APIs to load, access, and unload models from memory\",\"index\":null},\"prettyName\":\"Load and Access Models\",\"content\":\"$3e\",\"pageRelUrl\":\"1_python/5_manage-models/loading.md\",\"sectionKey\":\"manage-models\",\"sectionPrettyName\":\"Manage Models\"}},\"model-info\":{\"get-context-length\":{\"metadata\":{\"title\":\"Get Context Length\",\"description\":\"API to get the maximum context length of a model.\",\"index\":null},\"prettyName\":\"Get Context Length\",\"content\":\"$3f\",\"pageRelUrl\":\"1_python/6_model-info/get-context-length.md\",\"sectionKey\":\"model-info\",\"sectionPrettyName\":\"Model Info\"},\"get-load-config\":{\"metadata\":{\"title\":\"Get Load Config\",\"description\":\"Get the load configuration of the model\",\"index\":null},\"prettyName\":\"Get Load Config\",\"content\":\"$40\",\"pageRelUrl\":\"1_python/6_model-info/get-load-config.md\",\"sectionKey\":\"model-info\",\"sectionPrettyName\":\"Model Info\"},\"get-model-info\":{\"metadata\":{\"title\":\"Get Model Info\",\"description\":\"Get information about the model\",\"index\":null},\"prettyName\":\"Get Model Info\",\"content\":\"$41\",\"pageRelUrl\":\"1_python/6_model-info/get-model-info.md\",\"sectionKey\":\"model-info\",\"sectionPrettyName\":\"Model Info\"}}},\"typescript\":{\"\":{\"metadata\":{\"title\":\"`lmstudio-js` (TypeScript SDK)\",\"description\":\"Getting started with LM Studio's Typescript / JavaScript SDK\",\"index\":1},\"prettyName\":\"Introduction\",\"content\":\"$42\",\"pageRelUrl\":\"2_typescript/index.md\"},\"project-setup\":{\"metadata\":{\"title\":\"Project Setup\",\"description\":\"Set up your `lmstudio-js` app or script.\",\"index\":2},\"prettyName\":\"Project Setup\",\"content\":\"\\n`@lmstudio/sdk` is a library published on npm that allows you to use `lmstudio-js` in your own projects. It is open source and it's developed on GitHub. You can find the source code [here](https://github.com/lmstudio-ai/lmstudio-js).\\n\\n## Creating a New `node` Project\\n\\nUse the following command to start an interactive project setup:\\n\\n```lms_code_snippet\\n  variants:\\n    TypeScript (Recommended):\\n      language: bash\\n      code: |\\n        lms create node-typescript\\n    Javascript:\\n      language: bash\\n      code: |\\n        lms create node-javascript\\n```\\n\\n## Add `lmstudio-js` to an Exiting Project\\n\\nIf you have already created a project and would like to use `lmstudio-js` in it, you can install it using npm, yarn, or pnpm.\\n\\n```lms_code_snippet\\n  variants:\\n    npm:\\n      language: bash\\n      code: |\\n        npm install @lmstudio/sdk --save\\n    yarn:\\n      language: bash\\n      code: |\\n        yarn add @lmstudio/sdk\\n    pnpm:\\n      language: bash\\n      code: |\\n        pnpm add @lmstudio/sdk\\n```\\n\",\"pageRelUrl\":\"2_typescript/project-setup.md\"},\"llm-prediction\":{\"chat-completion\":{\"metadata\":{\"title\":\"Chat Completions\",\"description\":\"APIs for a multi-turn chat conversations with an LLM\",\"index\":2},\"prettyName\":\"Chat\",\"content\":\"$43\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/chat-completion.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"working-with-chats\":{\"metadata\":{\"title\":\"Working with Chats\",\"description\":\"APIs for representing a chat conversation with an LLM\",\"index\":3},\"prettyName\":\"Working with Chats\",\"content\":\"$44\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/working-with-chats.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"cancelling-predictions\":{\"metadata\":{\"title\":\"Cancelling Predictions\",\"description\":\"Stop an ongoing prediction in `lmstudio-js`\",\"index\":4},\"prettyName\":\"Cancelling Predictions\",\"content\":\"$45\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/cancelling-predictions.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"image-input\":{\"metadata\":{\"title\":\"Image Input\",\"description\":\"API for passing images as input to the model\",\"index\":4},\"prettyName\":\"Image Input\",\"content\":\"$46\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/image-input.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"structured-response\":{\"metadata\":{\"title\":\"Structured Response\",\"description\":\"Enforce a structured response from the model using Pydantic (Python), Zod (TypeScript), or JSON Schema\",\"index\":4},\"prettyName\":\"Structured Response\",\"content\":\"$47\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/structured-response.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"speculative-decoding\":{\"metadata\":{\"title\":\"Speculative Decoding\",\"description\":\"API to use a draft model in speculative decoding in `lmstudio-js`\",\"index\":5},\"prettyName\":\"Speculative Decoding\",\"content\":\"$48\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/speculative-decoding.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"completion\":{\"metadata\":{\"title\":\"Generate Completions\",\"description\":\"Provide a string input for the model to complete\",\"index\":6},\"prettyName\":\"Generate Completions\",\"content\":\"$49\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/completion.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"},\"parameters\":{\"metadata\":{\"title\":\"Configuring the Model\",\"description\":\"APIs for setting inference-time and load-time parameters for your model\",\"index\":null},\"prettyName\":\"Configuration Parameters\",\"content\":\"$4a\",\"pageRelUrl\":\"2_typescript/2_llm-prediction/parameters.md\",\"sectionKey\":\"llm-prediction\",\"sectionPrettyName\":\"Basics\"}},\"agent\":{\"act\":{\"metadata\":{\"title\":\"The `.act()` call\",\"description\":\"How to use the `.act()` call to turn LLMs into autonomous agents that can perform tasks on your local machine.\",\"index\":1},\"prettyName\":\"The `.act()` call\",\"content\":\"$4b\",\"pageRelUrl\":\"2_typescript/3_agent/act.md\",\"sectionKey\":\"agent\",\"sectionPrettyName\":\"Agentic Flows\"},\"tools\":{\"metadata\":{\"title\":\"Tool Definition\",\"description\":\"Define tools with the `tool()` function and pass them to the model in the `act()` call.\",\"index\":2},\"prettyName\":\"Tool Definition\",\"content\":\"$4c\",\"pageRelUrl\":\"2_typescript/3_agent/tools.md\",\"sectionKey\":\"agent\",\"sectionPrettyName\":\"Agentic Flows\"}},\"plugins\":{\"\":{\"metadata\":{\"title\":\"Introduction to Plugins\",\"description\":\"A brief introduction to making plugins for LM Studio using TypeScript.\",\"index\":1},\"prettyName\":\"Introduction to Plugins\",\"content\":\"$4d\",\"pageRelUrl\":\"2_typescript/3_plugins/index.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"dependencies\":{\"metadata\":{\"title\":\"Using `npm` Dependencies\",\"description\":\"How to use npm packages in LM Studio plugins\",\"index\":6},\"prettyName\":\"Using `npm` Dependencies\",\"content\":\"\\n## Add dependencies to your plugin with `npm`\\n\\nLM Studio plugins supports `npm` packages. You can just install them using `npm install`.\\n\\nWhen the plugin is installed, LM Studio will automatically download all the required dependencies that are declared in `package.json` and `package-lock.json`. (The user does not need to have Node.js/npm installed.)\\n\\n### `postinstall` scripts\\n\\nFor safety reasons, we do **not** run `postinstall` scripts. Thus please make sure you are not using any npm packages that require postinstall scripts to work.\\n\\n## Using Other Package Managers\\n\\nSince we rely on `package-lock.json`, lock files produced by other package managers will not work. Thus we recommend only using `npm` when developing LM Studio plugins.\\n\",\"pageRelUrl\":\"2_typescript/3_plugins/dependencies.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"tools-provider\":{\"\":{\"metadata\":{\"title\":\"Introduction to Tools Provider\",\"description\":\"Writing tools providers for LM Studio plugins using TypeScript\",\"index\":1},\"prettyName\":\"Introduction to Tools Provider\",\"content\":\"\\nTools provider is a function that returns an array of tools that the model can use during generation.\\n\\n## Examples\\n\\nThe following are some plugins that make use of tools providers:\\n\\n- [lmstudio/wikipedia](https://lmstudio.ai/lmstudio/wikipedia)\\n\\n  Gives the LLM tools to search and read Wikipedia articles.\\n\\n- [lmstudio/js-code-sandbox](https://lmstudio.ai/lmstudio/js-code-sandbox)\\n\\n  Gives the LLM tools to run JavaScript/TypeScript code in a sandbox environment using [deno](https://deno.com/).\\n\\n- [lmstudio/dice](https://lmstudio.ai/lmstudio/dice)\\n\\n  Allows the LLM to generate random numbers using \\\"dice\\\".\\n\",\"pageRelUrl\":\"2_typescript/3_plugins/1_tools-provider/index.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"single-tool\":{\"metadata\":{\"title\":\"Single Tool\",\"description\":\"\",\"index\":3},\"prettyName\":\"Single Tool\",\"content\":\"$4e\",\"pageRelUrl\":\"2_typescript/3_plugins/1_tools-provider/single-tool.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"multiple-tools\":{\"metadata\":{\"title\":\"Multiple Tools\",\"description\":\"\",\"index\":4},\"prettyName\":\"Multiple Tools\",\"content\":\"$4f\",\"pageRelUrl\":\"2_typescript/3_plugins/1_tools-provider/multiple-tools.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"custom-configuration\":{\"metadata\":{\"title\":\"Custom Configuration\",\"description\":\"Add custom configuration options to your tools provider\",\"index\":5},\"prettyName\":\"Custom Configuration\",\"content\":\"$50\",\"pageRelUrl\":\"2_typescript/3_plugins/1_tools-provider/custom-configuration.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"status-reports-and-warnings\":{\"metadata\":{\"title\":\"Status Reports \u0026 Warnings\",\"description\":\"\",\"index\":6},\"prettyName\":\"Status Reports \u0026 Warnings\",\"content\":\"$51\",\"pageRelUrl\":\"2_typescript/3_plugins/1_tools-provider/status-reports-and-warnings.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"handling-aborts\":{\"metadata\":{\"title\":\"Handling Aborts\",\"description\":\"Gracefully handle user-aborted tool executions in your tools provider\",\"index\":7},\"prettyName\":\"Handling Aborts\",\"content\":\"$52\",\"pageRelUrl\":\"2_typescript/3_plugins/1_tools-provider/handling-aborts.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}},\"prompt-preprocessor\":{\"\":{\"metadata\":{\"title\":\"Introduction\",\"description\":\"Writing prompt preprocessors for LM Studio plugins using TypeScript\",\"index\":1},\"prettyName\":\"Introduction\",\"content\":\"$53\",\"pageRelUrl\":\"2_typescript/3_plugins/2_prompt-preprocessor/index.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"examples\":{\"metadata\":{\"title\":\"Examples\",\"description\":\"\",\"index\":2},\"prettyName\":\"Examples\",\"content\":\"$54\",\"pageRelUrl\":\"2_typescript/3_plugins/2_prompt-preprocessor/examples.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"custom-configuration\":{\"metadata\":{\"title\":\"Custom Configuration\",\"description\":\"\",\"index\":3},\"prettyName\":\"Custom Configuration\",\"content\":\"$55\",\"pageRelUrl\":\"2_typescript/3_plugins/2_prompt-preprocessor/custom-configuration.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"custom-status-report\":{\"metadata\":{\"title\":\"Custom Status Report\",\"description\":\"\",\"index\":4},\"prettyName\":\"Custom Status Report\",\"content\":\"$56\",\"pageRelUrl\":\"2_typescript/3_plugins/2_prompt-preprocessor/custom-status-report.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"handling-aborts\":{\"metadata\":{\"title\":\"Handling Aborts\",\"description\":\"\",\"index\":5},\"prettyName\":\"Handling Aborts\",\"content\":\"\\nA prediction may be aborted by the user while your generator is still running. In such cases, you should handle the abort gracefully by handling the `ctl.abortSignal`.\\n\\nYou can learn more about `AbortSignal` in the [MDN documentation](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal).\\n\",\"pageRelUrl\":\"2_typescript/3_plugins/2_prompt-preprocessor/handling-aborts.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}},\"generator\":{\"\":{\"metadata\":{\"title\":\"Introduction\",\"description\":\"Writing generators for LM Studio plugins using TypeScript\",\"index\":1},\"prettyName\":\"Introduction\",\"content\":\"$57\",\"pageRelUrl\":\"2_typescript/3_plugins/3_generator/index.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"text-only-generators\":{\"metadata\":{\"title\":\"Text-only Generators\",\"description\":\"\",\"index\":2},\"prettyName\":\"Text-only Generators\",\"content\":\"$58\",\"pageRelUrl\":\"2_typescript/3_plugins/3_generator/text-only-generators.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"tool-calling-generators\":{\"metadata\":{\"title\":\"Tool calling generators\",\"description\":\"\",\"index\":3},\"prettyName\":\"Tool calling generators\",\"content\":\"$59\",\"pageRelUrl\":\"2_typescript/3_plugins/3_generator/tool-calling-generators.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}},\"custom-configuration\":{\"\":{\"metadata\":{\"title\":\"Introduction\",\"description\":\"Add custom configurations to LM Studio plugins using TypeScript\",\"index\":1},\"prettyName\":\"Introduction\",\"content\":\"$5a\",\"pageRelUrl\":\"2_typescript/3_plugins/4_custom-configuration/index.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"config-ts\":{\"metadata\":{\"title\":\"`config.ts` File\",\"description\":\"\",\"index\":2},\"prettyName\":\"`config.ts` File\",\"content\":\"$5b\",\"pageRelUrl\":\"2_typescript/3_plugins/4_custom-configuration/config-ts.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"accessing-config\":{\"metadata\":{\"title\":\"Accessing Configuration\",\"description\":\"\",\"index\":3},\"prettyName\":\"Accessing Configuration\",\"content\":\"$5c\",\"pageRelUrl\":\"2_typescript/3_plugins/4_custom-configuration/accessing-config.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"},\"defining-new-fields\":{\"metadata\":{\"title\":\"Defining New Fields\",\"description\":\"\",\"index\":4},\"prettyName\":\"Defining New Fields\",\"content\":\"$5d\",\"pageRelUrl\":\"2_typescript/3_plugins/4_custom-configuration/defining-new-fields.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}},\"publish-plugins\":{\"\":{\"metadata\":{\"title\":\"Sharing Plugins\",\"description\":\"How to publish your LM Studio plugins so they can be used by others\",\"index\":7},\"prettyName\":\"Sharing Plugins\",\"content\":\"$5e\",\"pageRelUrl\":\"2_typescript/3_plugins/5_publish-plugins/index.md\",\"sectionKey\":\"plugins\",\"sectionPrettyName\":\"Plugins (Beta)\"}}},\"embedding\":{\"\":{\"metadata\":{\"title\":\"Embedding\",\"description\":\"Generate text embeddings from input text\",\"index\":1},\"prettyName\":\"Generating embedding vectors\",\"content\":\"\\nGenerate embeddings for input text. Embeddings are vector representations of text that capture semantic meaning. Embeddings are a building block for RAG (Retrieval-Augmented Generation) and other similarity-based tasks.\\n\\n### Prerequisite: Get an Embedding Model\\n\\nIf you don't yet have an embedding model, you can download a model like `nomic-ai/nomic-embed-text-v1.5` using the following command:\\n\\n```bash\\nlms get nomic-ai/nomic-embed-text-v1.5\\n```\\n\\n## Create Embeddings\\n\\nTo convert a string to a vector representation, pass it to the `embed` method on the corresponding embedding model handle.\\n\\n```lms_code_snippet\\n  title: \\\"index.ts\\\"\\n  variants:\\n    TypeScript:\\n      language: typescript\\n      code: |\\n        import { LMStudioClient } from \\\"@lmstudio/sdk\\\";\\n        const client = new LMStudioClient();\\n\\n        const model = await client.embedding.model(\\\"nomic-embed-text-v1.5\\\");\\n\\n        const { embedding } = await model.embed(\\\"Hello, world!\\\");\\n```\\n\",\"pageRelUrl\":\"2_typescript/4_embedding/index.md\",\"sectionKey\":\"embedding\",\"sectionPrettyName\":\"Text Embedding\"}},\"tokenization\":{\"\":{\"metadata\":{\"title\":\"Tokenization\",\"description\":\"Tokenize text using a model's tokenizer\",\"index\":1},\"prettyName\":\"Tokenizing text\",\"content\":\"$5f\",\"pageRelUrl\":\"2_typescript/5_tokenization/index.md\",\"sectionKey\":\"tokenization\",\"sectionPrettyName\":\"Tokenization\"}},\"manage-models\":{\"list-downloaded\":{\"metadata\":{\"title\":\"List Local Models\",\"description\":\"APIs to list the available models in a given local environment\",\"index\":null},\"prettyName\":\"List Local Models\",\"content\":\"$60\",\"pageRelUrl\":\"2_typescript/6_manage-models/list-downloaded.md\",\"sectionKey\":\"manage-models\",\"sectionPrettyName\":\"Manage Models\"},\"list-loaded\":{\"metadata\":{\"title\":\"List Loaded Models\",\"description\":\"Query which models are currently loaded\",\"index\":null},\"prettyName\":\"List Loaded Models\",\"content\":\"\\nYou can iterate through models loaded into memory using the `listLoaded` method. This method lives under the `llm` and `embedding` namespaces of the `LMStudioClient` object.\\n\\n## List Models Currently Loaded in Memory\\n\\nThis will give you results equivalent to using [`lms ps`](../../cli/ps) in the CLI.\\n\\n```lms_code_snippet\\n  variants:\\n    TypeScript:\\n      language: typescript\\n      code: |\\n        import { LMStudioClient } from \\\"@lmstudio/sdk\\\";\\n\\n        const client = new LMStudioClient();\\n\\n        const llmOnly = await client.llm.listLoaded();\\n        const embeddingOnly = await client.embedding.listLoaded();\\n```\\n\\n\u003c!-- Learn more about `client.llm` namespace in the [API Reference](../api-reference/llm-namespace). --\u003e\\n\",\"pageRelUrl\":\"2_typescript/6_manage-models/list-loaded.md\",\"sectionKey\":\"manage-models\",\"sectionPrettyName\":\"Manage Models\"},\"loading\":{\"metadata\":{\"title\":\"Manage Models in Memory\",\"description\":\"APIs to load, access, and unload models from memory\",\"index\":null},\"prettyName\":\"Load and Access Models\",\"content\":\"$61\",\"pageRelUrl\":\"2_typescript/6_manage-models/loading.md\",\"sectionKey\":\"manage-models\",\"sectionPrettyName\":\"Manage Models\"}},\"api-reference\":{\"llm-load-model-config\":{\"metadata\":{\"title\":\"`LLMLoadModelConfig`\",\"description\":\"API Reference for `LLMLoadModelConfig`\",\"index\":null},\"prettyName\":\"`LLMLoadModelConfig`\",\"content\":\"$62\",\"pageRelUrl\":\"2_typescript/7_api-reference/llm-load-model-config.md\",\"sectionKey\":\"api-reference\",\"sectionPrettyName\":\"API Reference\"},\"llm-prediction-config-input\":{\"metadata\":{\"title\":\"`LLMPredictionConfigInput`\",\"description\":\"\",\"index\":null},\"prettyName\":\"`LLMPredictionConfigInput`\",\"content\":\"$63\",\"pageRelUrl\":\"2_typescript/7_api-reference/llm-prediction-config-input.md\",\"sectionKey\":\"api-reference\",\"sectionPrettyName\":\"API Reference\"}},\"model-info\":{\"get-context-length\":{\"metadata\":{\"title\":\"Get Context Length\",\"description\":\"API to get the maximum context length of a model.\",\"index\":null},\"prettyName\":\"Get Context Length\",\"content\":\"$64\",\"pageRelUrl\":\"2_typescript/8_model-info/get-context-length.md\",\"sectionKey\":\"model-info\",\"sectionPrettyName\":\"Model Info\"},\"get-model-info\":{\"metadata\":{\"title\":\"Get Model Info\",\"description\":\"Get information about the model\",\"index\":null},\"prettyName\":\"Get Model Info\",\"content\":\"\\nYou can access information about a loaded model using the `getInfo` method.\\n\\n```lms_code_snippet\\n  variants:\\n    LLM:\\n      language: typescript\\n      code: |\\n        import { LMStudioClient } from \\\"@lmstudio/sdk\\\";\\n\\n        const client = new LMStudioClient();\\n        const model = await client.llm.model();\\n\\n        const modelInfo = await model.getInfo();\\n\\n        console.info(\\\"Model Key\\\", modelInfo.modelKey);\\n        console.info(\\\"Current Context Length\\\", model.contextLength);\\n        console.info(\\\"Model Trained for Tool Use\\\", modelInfo.trainedForToolUse);\\n        // etc.\\n    Embedding Model:\\n      language: typescript\\n      code: |\\n        import { LMStudioClient } from \\\"@lmstudio/sdk\\\";\\n\\n        const client = new LMStudioClient();\\n        const model = await client.embedding.model();\\n\\n        const modelInfo = await model.getInfo();\\n\\n        console.info(\\\"Model Key\\\", modelInfo.modelKey);\\n        console.info(\\\"Current Context Length\\\", modelInfo.contextLength);\\n        // etc.\\n```\\n\",\"pageRelUrl\":\"2_typescript/8_model-info/get-model-info.md\",\"sectionKey\":\"model-info\",\"sectionPrettyName\":\"Model Info\"}}},\"cli\":{\"\":{\"metadata\":{\"title\":\"`lms` â€” LM Studio's CLI\",\"description\":\"Get starting with the `lms` command line utility.\",\"index\":1},\"prettyName\":\"Introduction\",\"content\":\"$65\",\"pageRelUrl\":\"3_cli/index.md\"},\"contributing\":{\"metadata\":{\"title\":\"Contributing\",\"description\":\"Learn where to file issues and how to contribute to the `lms` CLI.\",\"index\":2},\"prettyName\":\"Contributing\",\"content\":\"\\nThe `lms` CLI is open source on GitHub: https://github.com/lmstudio-ai/lms\\n\\nIf you spot a bug, want to request a feature, or plan to contribute:\\n\\n- File issues or feature requests in the GitHub repository.\\n- Open pull requests against the `main` branch with a concise summary and testing notes.\\n- Review the repository README for setup instructions and coding standards.\\n\",\"pageRelUrl\":\"3_cli/contributing.md\"},\"local-models\":{\"chat\":{\"metadata\":{\"title\":\"`lms chat`\",\"description\":\"Start a chat session with a local model from the command line.\",\"index\":1},\"prettyName\":\"`lms chat`\",\"content\":\"$66\",\"pageRelUrl\":\"3_cli/0_local-models/chat.md\",\"sectionKey\":\"local-models\",\"sectionPrettyName\":\"Local Models\"},\"get\":{\"metadata\":{\"title\":\"`lms get`\",\"description\":\"Search and download models from the command line.\",\"index\":2},\"prettyName\":\"`lms get`\",\"content\":\"$67\",\"pageRelUrl\":\"3_cli/0_local-models/get.md\",\"sectionKey\":\"local-models\",\"sectionPrettyName\":\"Local Models\"},\"load\":{\"metadata\":{\"title\":\"`lms load`\",\"description\":\"Load or unload models, set context length, GPU offload, TTL, or estimate memory usage without loading.\",\"index\":3},\"prettyName\":\"`lms load`\",\"content\":\"$68\",\"pageRelUrl\":\"3_cli/0_local-models/load.md\",\"sectionKey\":\"local-models\",\"sectionPrettyName\":\"Local Models\"},\"ls\":{\"metadata\":{\"title\":\"`lms ls`\",\"description\":\"List all downloaded models in your LM Studio installation.\",\"index\":4},\"prettyName\":\"`lms ls`\",\"content\":\"$69\",\"pageRelUrl\":\"3_cli/0_local-models/ls.md\",\"sectionKey\":\"local-models\",\"sectionPrettyName\":\"Local Models\"},\"ps\":{\"metadata\":{\"title\":\"`lms ps`\",\"description\":\"Show information about currently loaded models from the command line.\",\"index\":5},\"prettyName\":\"`lms ps`\",\"content\":\"\\nThe `lms ps` command displays information about all models currently loaded in memory.\\n\\n## List loaded models\\n\\nShow all currently loaded models:\\n\\n```shell\\nlms ps\\n```\\n\\nExample output:\\n```\\n   LOADED MODELS\\n\\nIdentifier: unsloth/deepseek-r1-distill-qwen-1.5b\\n  â€¢ Type:  LLM\\n  â€¢ Path: unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf\\n  â€¢ Size: 1.12 GB\\n  â€¢ Architecture: Qwen2\\n```\\n\\n### JSON output\\n\\nGet the list in machine-readable format:\\n```shell\\nlms ps --json\\n```\\n\\n## Operate on a remote LM Studio instance\\n\\n`lms ps` supports the `--host` flag to connect to a remote LM Studio instance:\\n\\n```shell\\nlms ps --host \u003chost\u003e\\n```\\n\\nFor this to work, the remote LM Studio instance must be running and accessible from your local machine, e.g. be accessible on the same subnet.\\n\",\"pageRelUrl\":\"3_cli/0_local-models/ps.md\",\"sectionKey\":\"local-models\",\"sectionPrettyName\":\"Local Models\"},\"import\":{\"metadata\":{\"title\":\"`lms import`\",\"description\":\"Import a local model file into your LM Studio models directory.\",\"index\":6},\"prettyName\":\"`lms import`\",\"content\":\"$6a\",\"pageRelUrl\":\"3_cli/0_local-models/import.md\",\"sectionKey\":\"local-models\",\"sectionPrettyName\":\"Local Models\"}},\"serve\":{\"server-start\":{\"metadata\":{\"title\":\"`lms server start`\",\"description\":\"Start the LM Studio local server with customizable port and logging options.\",\"index\":1},\"prettyName\":\"`lms server start`\",\"content\":\"$6b\",\"pageRelUrl\":\"3_cli/1_serve/server-start.md\",\"sectionKey\":\"serve\",\"sectionPrettyName\":\"serve\"},\"server-status\":{\"metadata\":{\"title\":\"`lms server status`\",\"description\":\"Check the status of your running LM Studio server instance.\",\"index\":2},\"prettyName\":\"`lms server status`\",\"content\":\"$6c\",\"pageRelUrl\":\"3_cli/1_serve/server-status.md\",\"sectionKey\":\"serve\",\"sectionPrettyName\":\"serve\"},\"server-stop\":{\"metadata\":{\"title\":\"`lms server stop`\",\"description\":\"Stop the running LM Studio server instance.\",\"index\":3},\"prettyName\":\"`lms server stop`\",\"content\":\"\\nThe `lms server stop` command gracefully stops the running LM Studio server.\\n\\n```shell\\nlms server stop\\n```\\n\\nExample output:\\n```\\nStopped the server on port 1234.\\n```\\n\\nAny active request will be terminated when the server is stopped. You can restart the server using [`lms server start`](/docs/cli/serve/server-start).\\n\",\"pageRelUrl\":\"3_cli/1_serve/server-stop.md\",\"sectionKey\":\"serve\",\"sectionPrettyName\":\"serve\"},\"log-stream\":{\"metadata\":{\"title\":\"`lms log stream`\",\"description\":\"Stream logs from LM Studio. Useful for debugging prompts sent to the model.\",\"index\":4},\"prettyName\":\"`lms log stream`\",\"content\":\"$6d\",\"pageRelUrl\":\"3_cli/1_serve/log-stream.md\",\"sectionKey\":\"serve\",\"sectionPrettyName\":\"serve\"}},\"runtime\":{\"runtime\":{\"metadata\":{\"title\":\"`lms runtime`\",\"description\":\"Manage LM Studio inference runtimes from the CLI.\",\"index\":1},\"prettyName\":\"`lms runtime`\",\"content\":\"\\nUse `lms runtime` to list, download, switch, or remove inference runtimes without opening the app.\\n\\n### Commands\\n\\n- `lms runtime ls` â€” list installed runtimes.\\n- `lms runtime get` â€” download a runtime.\\n- `lms runtime select` â€” set the active runtime.\\n- `lms runtime remove` â€” uninstall a runtime.\\n- `lms runtime update` â€” update an installed runtime.\\n\\n### List installed runtimes\\n\\n```shell\\nlms runtime ls\\n```\\n\\n### Download a runtime\\n\\n```shell\\nlms runtime get\\n```\\n\\n### Switch to a runtime\\n\\n```shell\\nlms runtime select\\n```\\n\\nFollow the interactive prompts to choose the version you want.\\n\",\"pageRelUrl\":\"3_cli/2_runtime/runtime.md\",\"sectionKey\":\"runtime\",\"sectionPrettyName\":\"runtime\"}},\"develop-and-publish\":{\"clone\":{\"metadata\":{\"title\":\"`lms clone`\",\"description\":\"Clone an artifact from LM Studio Hub to a local folder (beta).\",\"index\":1},\"prettyName\":\"`lms clone`\",\"content\":\"\\nUse `lms clone` to copy an artifact from LM Studio Hub onto your machine.\\n\\n### Flags\\n```lms_params\\n- name: \\\"\u003cartifact\u003e\\\"\\n  type: \\\"string\\\"\\n  optional: false\\n  description: \\\"Artifact identifier in the form owner/name\\\"\\n- name: \\\"[path]\\\"\\n  type: \\\"string\\\"\\n  optional: true\\n  description: \\\"Destination folder. Defaults to a new folder named after the artifact.\\\"\\n```\\n\\nIf no path is provided, `lms clone owner/name` creates a folder called `name` in the current directory. The command exits if the target path already exists.\\n\\n### Clone the latest revision\\n\\n```shell\\nlms clone alice/sample-plugin\\n```\\n\\n### Clone into a specific directory\\n\\n```shell\\nlms clone alice/sample-plugin ./my-folder\\n```\\n\",\"pageRelUrl\":\"3_cli/3_develop-and-publish/clone.md\",\"sectionKey\":\"develop-and-publish\",\"sectionPrettyName\":\"Develop and Publish (Beta)\"},\"push\":{\"metadata\":{\"title\":\"`lms push` (Beta)\",\"description\":\"Upload the current folder's artifact to LM Studio Hub (beta).\",\"index\":2},\"prettyName\":\"`lms push`\",\"content\":\"$6e\",\"pageRelUrl\":\"3_cli/3_develop-and-publish/push.md\",\"sectionKey\":\"develop-and-publish\",\"sectionPrettyName\":\"Develop and Publish (Beta)\"},\"dev\":{\"metadata\":{\"title\":\"`lms dev` (Beta)\",\"description\":\"Start a plugin dev server or install a local plugin (beta).\",\"index\":3},\"prettyName\":\"`lms dev`\",\"content\":\"\\nUse `lms dev` inside a plugin project to run a local dev server that rebuilds and reloads on file changes.\\n\\nThis feature is a part of LM Studio [Plugins](/docs/typescript/plugins), currently in private beta.\\n\\n### Run the dev plugin server\\n\\n```shell\\nlms dev\\n```\\n\\nThis verifies `manifest.json`, installs dependencies if needed, and starts a watcher that rebuilds the plugin on changes. Supported runners: Node/ECMAScript and Deno.\\n\\n### Install the plugin instead of running dev\\n\\n```shell\\nlms dev --install\\n```\\n\\n### Flags\\n```lms_params\\n- name: \\\"-i, --install\\\"\\n  type: \\\"flag\\\"\\n  optional: true\\n  description: \\\"Install the plugin into LM Studio instead of running the dev server\\\"\\n- name: \\\"--no-notify\\\"\\n  type: \\\"flag\\\"\\n  optional: true\\n  description: \\\"Do not show the \\\\\\\"Plugin started\\\\\\\" notification in LM Studio\\\"\\n```\",\"pageRelUrl\":\"3_cli/3_develop-and-publish/dev.md\",\"sectionKey\":\"develop-and-publish\",\"sectionPrettyName\":\"Develop and Publish (Beta)\"},\"login\":{\"metadata\":{\"title\":\"`lms login`\",\"description\":\"Authenticate with LM Studio Hub (beta).\",\"index\":4},\"prettyName\":\"`lms login`\",\"content\":\"$6f\",\"pageRelUrl\":\"3_cli/3_develop-and-publish/login.md\",\"sectionKey\":\"develop-and-publish\",\"sectionPrettyName\":\"Develop and Publish (Beta)\"}}}}}],[\"$\",\"div\",null,{\"className\":\"relative flex h-full flex-grow bg-documentation docs-no-line-numbers\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex h-full w-full items-center justify-center\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex h-full w-full flex-row items-start justify-start gap-0 overflow-hidden flex-wrap lg:flex-nowrap max-w-full\",\"children\":[[\"$\",\"$L70\",null,{\"documentationArticles\":\"$71\"}],[\"$\",\"$L1ee\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(static)\",\"children\",\"docs\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L1ef\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]]}]}]}]]}]],null],null]},[[null,[\"$\",\"$L1f0\",null,{\"userProfile\":null,\"profileStatus\":\"none\",\"children\":[\"$\",\"$L1f1\",null,{\"organizations\":[],\"children\":[[\"$\",\"$L1f2\",null,{\"theme\":\"system\",\"richColors\":true,\"position\":\"top-center\"}],[\"$\",\"$L1f3\",null,{}],[\"$\",\"$L1f4\",null,{\"features\":{\"privateArtifactQuota\":0,\"tailscaleAlpha\":false,\"projectsEnabled\":false,\"showUmbrellaModels\":false,\"organizationMemberJoinsPlot\":false,\"discussions\":false,\"artifactSearching\":false,\"freeTrialTeams\":false,\"freeTrialEnterprise\":false,\"enterpriseBookMeeting\":false},\"children\":[\"$\",\"$L1f5\",null,{\"isDynamic\":false,\"children\":[\"$\",\"$L1ee\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(static)\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L1ef\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]}]}]]}]}]],null],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5558a0fd21778b44.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],\"$L1f6\"],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$L1f7\"],\"globalErrorComponent\":\"$1f8\",\"missingSlots\":\"$W1f9\"}]\n"])</script><script>self.__next_f.push([1,"1fa:I[18076,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"1779\",\"static/chunks/0e762574-26ab547f20f731a1.js\",\"5706\",\"static/chunks/9c4e2130-5b8fdd2a06a75632.js\",\"6305\",\"static/chunks/eec3d76d-692d954fdc526554.js\",\"614\",\"static/chunks/3d47b92a-50c49e0880f014d1.js\",\"522\",\"static/chunks/94730671-6f37d42c2422b345.js\",\"3217\",\"static/chunks/578c2090-ecc970c206c04bd8.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"6890\",\"static/chunks/6890-680ccec6f8ba7871.js\",\"7772\",\"static/chunks/7772-31f69a9dcf45ceee.js\",\"1597\",\"static/chunks/1597-04f9481876dbbe2a.js\",\"1687\",\"static/chunks/1687-bad5337c161b89bf.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"1346\",\"static/chunks/1346-7cdf57d39b4f88ef.js\",\"2130\",\"static/chunks/2130-a91b687142678a30.js\",\"9962\",\"static/chunks/9962-4fbb58d4dc9d3e27.js\",\"6234\",\"static/chunks/6234-46ca85d7201402af.js\",\"5742\",\"static/chunks/app/(static)/docs/%5B...slug%5D/layout-147aeca92859f018.js\"],\"H1\"]\n1fb:I[18076,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"1779\",\"static/chunks/0e762574-26ab547f20f731a1.js\",\"5706\",\"static/chunks/9c4e2130-5b8fdd2a06a75632.js\",\"6305\",\"static/chunks/eec3d76d-692d954fdc526554.js\",\"614\",\"static/chunks/3d47b92a-50c49e0880f014d1.js\",\"522\",\"static/chunks/94730671-6f37d42c2422b345.js\",\"3217\",\"static/chunks/578c2090-ecc970c206c04bd8.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8"])</script><script>self.__next_f.push([1,".js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"6890\",\"static/chunks/6890-680ccec6f8ba7871.js\",\"7772\",\"static/chunks/7772-31f69a9dcf45ceee.js\",\"1597\",\"static/chunks/1597-04f9481876dbbe2a.js\",\"1687\",\"static/chunks/1687-bad5337c161b89bf.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"1346\",\"static/chunks/1346-7cdf57d39b4f88ef.js\",\"2130\",\"static/chunks/2130-a91b687142678a30.js\",\"9962\",\"static/chunks/9962-4fbb58d4dc9d3e27.js\",\"6234\",\"static/chunks/6234-46ca85d7201402af.js\",\"5742\",\"static/chunks/app/(static)/docs/%5B...slug%5D/layout-147aeca92859f018.js\"],\"A\"]\n1fc:I[18076,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"1779\",\"static/chunks/0e762574-26ab547f20f731a1.js\",\"5706\",\"static/chunks/9c4e2130-5b8fdd2a06a75632.js\",\"6305\",\"static/chunks/eec3d76d-692d954fdc526554.js\",\"614\",\"static/chunks/3d47b92a-50c49e0880f014d1.js\",\"522\",\"static/chunks/94730671-6f37d42c2422b345.js\",\"3217\",\"static/chunks/578c2090-ecc970c206c04bd8.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"6890\",\"static/chunks/6890-680ccec6f8ba7871.js\",\"7772\",\"static/chunks/7772-31f69a9dcf45ceee.js\",\"1597\",\"static/chunks/1597-04f9481876dbbe2a.js\",\"1687\",\"static/chunks/1687-bad5337c161b89bf.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"1346\",\"static/chunks/1346-7cdf57d39b4f88ef.js\",\"2130\",\"static/chunks/2130-a91b687142678a30.js\",\"9962\",\"static/chunks/9962-4fbb58d4dc9d3e27.js\",\"6234\",\"static/chunks/6234-46ca85d7201402af.js\",\"5742\",\"static/chunks/app/(static)/docs/%5B...slug%5D/layout-147aeca92859f018.js\"],\"H2\"]\n211:I[88003,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"9930\",\"static/chunks/9930-a52"])</script><script>self.__next_f.push([1,"251cc1096c3b1.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"3809\",\"static/chunks/3809-cb26ddec5be1b9d8.js\",\"4662\",\"static/chunks/4662-5275219a57c51235.js\",\"8801\",\"static/chunks/8801-cdec8854fe39743a.js\",\"9133\",\"static/chunks/9133-789d0b90d9d9c0ea.js\",\"3185\",\"static/chunks/app/layout-dba5af340b8e8926.js\"],\"\"]\n212:I[25922,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"3809\",\"static/chunks/3809-cb26ddec5be1b9d8.js\",\"4662\",\"static/chunks/4662-5275219a57c51235.js\",\"8801\",\"static/chunks/8801-cdec8854fe39743a.js\",\"9133\",\"static/chunks/9133-789d0b90d9d9c0ea.js\",\"3185\",\"static/chunks/app/layout-dba5af340b8e8926.js\"],\"ThemeProvider\"]\n213:I[98801,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"3809\",\"static/chunks/3809-cb26ddec5be1b9d8.js\",\"4662\",\"static/chunks/4662-5275219a57c51235.js\",\"8801\",\"static/chunks/8801-cdec8854fe39743a.js\",\"9133\",\"static/chunks/9133-789d0b90d9d9c0ea.js\",\"3185\",\"static/chunks/app/layout-dba5af340b8e8926.js\"],\"NModalProvider\"]\n214:I[59600,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"3809\",\"static/chunks/3809-cb26ddec5be1b9d8.js\",\"4662\",\"static/chunks/4662-5275219a57c51235.js\",\"8801\",\"static/chunks/8801-cdec8854fe39743a.js\",\"9133\",\"static/chunks/9133-789d0b90d9d9c0ea.js\",\"3185\",\"static/chunks/app/layout-dba5af340b8e8926.js\"],\"OSProvider\"]\n215:I[90020,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"3809\",\"static/chunks/3809-cb26ddec5be1b9d8.js\",\"4662\",\"s"])</script><script>self.__next_f.push([1,"tatic/chunks/4662-5275219a57c51235.js\",\"8801\",\"static/chunks/8801-cdec8854fe39743a.js\",\"9133\",\"static/chunks/9133-789d0b90d9d9c0ea.js\",\"3185\",\"static/chunks/app/layout-dba5af340b8e8926.js\"],\"UserSelectionsProvider\"]\n216:I[60525,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"3809\",\"static/chunks/3809-cb26ddec5be1b9d8.js\",\"4662\",\"static/chunks/4662-5275219a57c51235.js\",\"8801\",\"static/chunks/8801-cdec8854fe39743a.js\",\"9133\",\"static/chunks/9133-789d0b90d9d9c0ea.js\",\"3185\",\"static/chunks/app/layout-dba5af340b8e8926.js\"],\"RootScrollLockProvider\"]\n217:I[87198,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"1779\",\"static/chunks/0e762574-26ab547f20f731a1.js\",\"5706\",\"static/chunks/9c4e2130-5b8fdd2a06a75632.js\",\"6305\",\"static/chunks/eec3d76d-692d954fdc526554.js\",\"614\",\"static/chunks/3d47b92a-50c49e0880f014d1.js\",\"522\",\"static/chunks/94730671-6f37d42c2422b345.js\",\"3217\",\"static/chunks/578c2090-ecc970c206c04bd8.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"6890\",\"static/chunks/6890-680ccec6f8ba7871.js\",\"7772\",\"static/chunks/7772-31f69a9dcf45ceee.js\",\"1597\",\"static/chunks/1597-04f9481876dbbe2a.js\",\"1687\",\"static/chunks/1687-bad5337c161b89bf.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"1346\",\"static/chunks/1346-7cdf57d39b4f88ef.js\",\"6097\",\"static/chunks/6097-a44cd9b7466c245f.js\",\"9962\",\"static/chunks/9962-4fbb58d4dc9d3e27.js\",\"6234\",\"static/chunks/6234-46ca85d7201402af.js\",\"9160\",\"static/chunks/app/not-found-d596f4f02073e882.js\"],\"default\"]\n218:\"$Sreact.suspense\"\n219:I[52182,["])</script><script>self.__next_f.push([1,"\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"3809\",\"static/chunks/3809-cb26ddec5be1b9d8.js\",\"4662\",\"static/chunks/4662-5275219a57c51235.js\",\"8801\",\"static/chunks/8801-cdec8854fe39743a.js\",\"9133\",\"static/chunks/9133-789d0b90d9d9c0ea.js\",\"3185\",\"static/chunks/app/layout-dba5af340b8e8926.js\"],\"NavigationEvents\"]\n21b:I[13437,[\"6361\",\"static/chunks/ee560e2c-b093271a256731e1.js\",\"7699\",\"static/chunks/8e1d74a4-ddce50761baba1e8.js\",\"6950\",\"static/chunks/f8025e75-e20309addf7bf5f4.js\",\"1779\",\"static/chunks/0e762574-26ab547f20f731a1.js\",\"5706\",\"static/chunks/9c4e2130-5b8fdd2a06a75632.js\",\"6305\",\"static/chunks/eec3d76d-692d954fdc526554.js\",\"614\",\"static/chunks/3d47b92a-50c49e0880f014d1.js\",\"522\",\"static/chunks/94730671-6f37d42c2422b345.js\",\"3217\",\"static/chunks/578c2090-ecc970c206c04bd8.js\",\"9930\",\"static/chunks/9930-a52251cc1096c3b1.js\",\"2813\",\"static/chunks/2813-6d096989a4051682.js\",\"1716\",\"static/chunks/1716-15257501f3402ab6.js\",\"2972\",\"static/chunks/2972-6f5f7e95fba3c98d.js\",\"1229\",\"static/chunks/1229-bfdd3210479325c8.js\",\"5878\",\"static/chunks/5878-ba606658793f4e54.js\",\"602\",\"static/chunks/602-f8d92996d4060178.js\",\"6890\",\"static/chunks/6890-680ccec6f8ba7871.js\",\"7772\",\"static/chunks/7772-31f69a9dcf45ceee.js\",\"1597\",\"static/chunks/1597-04f9481876dbbe2a.js\",\"1687\",\"static/chunks/1687-bad5337c161b89bf.js\",\"1762\",\"static/chunks/1762-6246519a44fd2148.js\",\"1346\",\"static/chunks/1346-7cdf57d39b4f88ef.js\",\"2130\",\"static/chunks/2130-a91b687142678a30.js\",\"9962\",\"static/chunks/9962-4fbb58d4dc9d3e27.js\",\"6234\",\"static/chunks/6234-46ca85d7201402af.js\",\"5742\",\"static/chunks/app/(static)/docs/%5B...slug%5D/layout-147aeca92859f018.js\"],\"OnThisPageSidebarSection\"]\n8:[\"$\",\"div\",null,{\"className\":\"markdown-body text-foreground [\u0026\u003eh1]:font-medium\",\"children\":[\"$\",\"$L1fa\",null,{\"id\":\"welcome-to-lm-studio-docs\",\"node\":{\"type\":\"element\",\"tagName\":\"h1\",\"properties\":{\"id\":\"welcome-to-lm-studio-doc"])</script><script>self.__next_f.push([1,"s\"},\"children\":[{\"type\":\"text\",\"value\":\"Welcome to LM Studio Docs!\",\"position\":{\"start\":{\"line\":1,\"column\":3,\"offset\":2},\"end\":{\"line\":1,\"column\":29,\"offset\":28}}}],\"position\":{\"start\":{\"line\":1,\"column\":1,\"offset\":0},\"end\":{\"line\":1,\"column\":29,\"offset\":28}}},\"children\":\"Welcome to LM Studio Docs!\"}]}]\nb:[\"$\",\"div\",null,{\"className\":\"markdown-body text-xl font-[400] opacity-80\",\"children\":[\"$\",\"p\",null,{\"children\":\"Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM Studio.\",\"className\":\"text-foreground/90 text-xl font-[400] opacity-80\"}]}]\n1fe:{\"href\":\"api/openai-api\"}\n202:{\"line\":53,\"column\":4,\"offset\":2138}\n203:{\"line\":53,\"column\":28,\"offset\":2162}\n201:{\"start\":\"$202\",\"end\":\"$203\"}\n200:{\"type\":\"text\",\"value\":\"OpenAI Compatibility API\",\"position\":\"$201\"}\n1ff:[\"$200\"]\n205:{\"line\":53,\"column\":3,\"offset\":2137}\n206:{\"line\":53,\"column\":45,\"offset\":2179}\n204:{\"start\":\"$205\",\"end\":\"$206\"}\n1fd:{\"type\":\"element\",\"tagName\":\"a\",\"properties\":\"$1fe\",\"children\":\"$1ff\",\"position\":\"$204\"}\n208:{\"href\":\"api/rest-api\"}\n20c:{\"line\":54,\"column\":4,\"offset\":2183}\n20d:{\"line\":54,\"column\":29,\"offset\":2208}\n20b:{\"start\":\"$20c\",\"end\":\"$20d\"}\n20a:{\"type\":\"text\",\"value\":\"LM Studio REST API (beta)\",\"position\":\"$20b\"}\n209:[\"$20a\"]\n20f:{\"line\":54,\"column\":3,\"offset\":2182}\n210:{\"line\":54,\"column\":44,\"offset\":2223}\n20e:{\"start\":\"$20f\",\"end\":\"$210\"}\n207:{\"type\":\"element\",\"tagName\":\"a\",\"properties\":\"$208\",\"children\":\"$209\",\"position\":\"$20e\"}\n"])</script><script>self.__next_f.push([1,"c:[\"$\",\"div\",null,{\"className\":\"markdown-body \",\"children\":[[\"$\",\"p\",null,{\"children\":[\"To get LM Studio, head over to the \",[\"$\",\"$L1fb\",null,{\"href\":\"/download\",\"node\":{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"/download\"},\"children\":[{\"type\":\"text\",\"value\":\"Downloads page\",\"position\":{\"start\":{\"line\":2,\"column\":37,\"offset\":37},\"end\":{\"line\":2,\"column\":51,\"offset\":51}}}],\"position\":{\"start\":{\"line\":2,\"column\":36,\"offset\":36},\"end\":{\"line\":2,\"column\":63,\"offset\":63}}},\"children\":\"Downloads page\",\"className\":\"\"}],\" and download an installer for your operating system.\"],\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"LM Studio is available for macOS, Windows, and Linux.\",\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"$L1fc\",null,{\"id\":\"what-can-i-do-with-lm-studio\",\"node\":{\"type\":\"element\",\"tagName\":\"h2\",\"properties\":{\"id\":\"what-can-i-do-with-lm-studio\"},\"children\":[{\"type\":\"text\",\"value\":\"What can I do with LM Studio?\",\"position\":{\"start\":{\"line\":6,\"column\":4,\"offset\":176},\"end\":{\"line\":6,\"column\":33,\"offset\":205}}}],\"position\":{\"start\":{\"line\":6,\"column\":1,\"offset\":173},\"end\":{\"line\":6,\"column\":33,\"offset\":205}}},\"children\":\"What can I do with LM Studio?\"}],\"\\n\",[\"$\",\"ul\",null,{\"node\":{\"type\":\"element\",\"tagName\":\"ol\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"\\n\"},{\"type\":\"element\",\"tagName\":\"li\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"Download and run local LLMs like gpt-oss or Llama, Qwen\",\"position\":{\"start\":{\"line\":8,\"column\":4,\"offset\":210},\"end\":{\"line\":8,\"column\":59,\"offset\":265}}}],\"position\":{\"start\":{\"line\":8,\"column\":1,\"offset\":207},\"end\":{\"line\":8,\"column\":59,\"offset\":265}}},{\"type\":\"text\",\"value\":\"\\n\"},{\"type\":\"element\",\"tagName\":\"li\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"Use a simple and flexible chat interface\",\"position\":{\"start\":{\"line\":9,\"column\":4,\"offset\":269},\"end\":{\"line\":9,\"column\":44,\"offset\":309}}}],\"position\":{\"start\":{\"line\":9,\"column\":1,\"offset\":266},\"end\":{\"line\":9,\"column\":44,\"offset\":309}}},{\"type\":\"text\",\"value\":\"\\n\"},{\"type\":\"element\",\"tagName\":\"li\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"Connect MCP servers and use them with local models\",\"position\":{\"start\":{\"line\":10,\"column\":4,\"offset\":313},\"end\":{\"line\":10,\"column\":54,\"offset\":363}}}],\"position\":{\"start\":{\"line\":10,\"column\":1,\"offset\":310},\"end\":{\"line\":10,\"column\":54,\"offset\":363}}},{\"type\":\"text\",\"value\":\"\\n\"},{\"type\":\"element\",\"tagName\":\"li\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"Search \u0026 download functionality (via Hugging Face ðŸ¤—)\",\"position\":{\"start\":{\"line\":11,\"column\":4,\"offset\":367},\"end\":{\"line\":11,\"column\":57,\"offset\":420}}}],\"position\":{\"start\":{\"line\":11,\"column\":1,\"offset\":364},\"end\":{\"line\":11,\"column\":57,\"offset\":420}}},{\"type\":\"text\",\"value\":\"\\n\"},{\"type\":\"element\",\"tagName\":\"li\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"Serve local models on OpenAI-like endpoints, locally and on the network\",\"position\":{\"start\":{\"line\":12,\"column\":4,\"offset\":424},\"end\":{\"line\":12,\"column\":75,\"offset\":495}}}],\"position\":{\"start\":{\"line\":12,\"column\":1,\"offset\":421},\"end\":{\"line\":12,\"column\":75,\"offset\":495}}},{\"type\":\"text\",\"value\":\"\\n\"},{\"type\":\"element\",\"tagName\":\"li\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"Manage your local models, prompts, and configurations\",\"position\":{\"start\":{\"line\":13,\"column\":4,\"offset\":499},\"end\":{\"line\":13,\"column\":57,\"offset\":552}}}],\"position\":{\"start\":{\"line\":13,\"column\":1,\"offset\":496},\"end\":{\"line\":13,\"column\":57,\"offset\":552}}},{\"type\":\"text\",\"value\":\"\\n\"}],\"position\":{\"start\":{\"line\":8,\"column\":1,\"offset\":207},\"end\":{\"line\":13,\"column\":57,\"offset\":552}}},\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Download and run local LLMs like gpt-oss or Llama, Qwen\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Use a simple and flexible chat interface\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"Connect MCP servers and use them with local models\"}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":\"Search \u0026 download functionality (via Hugging Face ðŸ¤—)\"}],\"\\n\",[\"$\",\"li\",\"li-4\",{\"children\":\"Serve local models on OpenAI-like endpoints, locally and on the network\"}],\"\\n\",[\"$\",\"li\",\"li-5\",{\"children\":\"Manage your local models, prompts, and configurations\"}],\"\\n\"],\"className\":\"list-decimal\"}],\"\\n\",[\"$\",\"$L1fc\",null,{\"id\":\"system-requirements\",\"node\":{\"type\":\"element\",\"tagName\":\"h2\",\"properties\":{\"id\":\"system-requirements\"},\"children\":[{\"type\":\"text\",\"value\":\"System requirements\",\"position\":{\"start\":{\"line\":15,\"column\":4,\"offset\":557},\"end\":{\"line\":15,\"column\":23,\"offset\":576}}}],\"position\":{\"start\":{\"line\":15,\"column\":1,\"offset\":554},\"end\":{\"line\":15,\"column\":23,\"offset\":576}}},\"children\":\"System requirements\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"LM Studio generally supports Apple Silicon Macs, x64/ARM64 Windows PCs, and x64 Linux PCs.\",\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Consult the \",[\"$\",\"$L1fb\",null,{\"href\":\"app/system-requirements\",\"node\":{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"app/system-requirements\"},\"children\":[{\"type\":\"text\",\"value\":\"System Requirements\",\"position\":{\"start\":{\"line\":19,\"column\":14,\"offset\":683},\"end\":{\"line\":19,\"column\":33,\"offset\":702}}}],\"position\":{\"start\":{\"line\":19,\"column\":13,\"offset\":682},\"end\":{\"line\":19,\"column\":59,\"offset\":728}}},\"children\":\"System Requirements\",\"className\":\"\"}],\" page for more detailed information.\"],\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"$L1fc\",null,{\"id\":\"run-llamacpp-gguf-or-mlx-models\",\"node\":{\"type\":\"element\",\"tagName\":\"h2\",\"properties\":{\"id\":\"run-llamacpp-gguf-or-mlx-models\"},\"children\":[{\"type\":\"text\",\"value\":\"Run llama.cpp (GGUF) or MLX models\",\"position\":{\"start\":{\"line\":21,\"column\":4,\"offset\":769},\"end\":{\"line\":21,\"column\":38,\"offset\":803}}}],\"position\":{\"start\":{\"line\":21,\"column\":1,\"offset\":766},\"end\":{\"line\":21,\"column\":38,\"offset\":803}}},\"children\":\"Run llama.cpp (GGUF) or MLX models\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"LM Studio supports running LLMs on Mac, Windows, and Linux using \",[\"$\",\"$L1fb\",null,{\"href\":\"https://github.com/ggerganov/llama.cpp\",\"node\":{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"https://github.com/ggerganov/llama.cpp\"},\"children\":[{\"type\":\"element\",\"tagName\":\"code\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"llama.cpp\",\"position\":{\"start\":{\"line\":23,\"column\":67,\"offset\":871},\"end\":{\"line\":23,\"column\":78,\"offset\":882}}}],\"position\":{\"start\":{\"line\":23,\"column\":67,\"offset\":871},\"end\":{\"line\":23,\"column\":78,\"offset\":882}}}],\"position\":{\"start\":{\"line\":23,\"column\":66,\"offset\":870},\"end\":{\"line\":23,\"column\":119,\"offset\":923}}},\"children\":[\"$\",\"code\",null,{\"className\":\"whitespace-nowrap\",\"dir\":\"ltr\",\"style\":{\"whiteSpace\":\"pre-wrap\",\"wordBreak\":\"break-word\"},\"children\":\"llama.cpp\"}],\"className\":\"\"}],\".\"],\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"On Apple Silicon Macs, LM Studio also supports running LLMs using Apple's \",[\"$\",\"$L1fb\",null,{\"href\":\"https://github.com/ml-explore/mlx\",\"node\":{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"https://github.com/ml-explore/mlx\"},\"children\":[{\"type\":\"element\",\"tagName\":\"code\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"MLX\",\"position\":{\"start\":{\"line\":25,\"column\":76,\"offset\":1001},\"end\":{\"line\":25,\"column\":81,\"offset\":1006}}}],\"position\":{\"start\":{\"line\":25,\"column\":76,\"offset\":1001},\"end\":{\"line\":25,\"column\":81,\"offset\":1006}}}],\"position\":{\"start\":{\"line\":25,\"column\":75,\"offset\":1000},\"end\":{\"line\":25,\"column\":117,\"offset\":1042}}},\"children\":[\"$\",\"code\",null,{\"className\":\"whitespace-nowrap\",\"dir\":\"ltr\",\"style\":{\"whiteSpace\":\"pre-wrap\",\"wordBreak\":\"break-word\"},\"children\":\"MLX\"}],\"className\":\"\"}],\".\"],\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"To install or manage LM Runtimes, press \",[\"$\",\"code\",null,{\"className\":\"whitespace-nowrap\",\"dir\":\"ltr\",\"style\":{\"whiteSpace\":\"pre-wrap\",\"wordBreak\":\"break-word\"},\"children\":\"âŒ˜\"}],\" \",[\"$\",\"code\",null,{\"className\":\"whitespace-nowrap\",\"dir\":\"ltr\",\"style\":{\"whiteSpace\":\"pre-wrap\",\"wordBreak\":\"break-word\"},\"children\":\"Shift\"}],\" \",[\"$\",\"code\",null,{\"className\":\"whitespace-nowrap\",\"dir\":\"ltr\",\"style\":{\"whiteSpace\":\"pre-wrap\",\"wordBreak\":\"break-word\"},\"children\":\"R\"}],\" on Mac or \",[\"$\",\"code\",null,{\"className\":\"whitespace-nowrap\",\"dir\":\"ltr\",\"style\":{\"whiteSpace\":\"pre-wrap\",\"wordBreak\":\"break-word\"},\"children\":\"Ctrl\"}],\" \",[\"$\",\"code\",null,{\"className\":\"whitespace-nowrap\",\"dir\":\"ltr\",\"style\":{\"whiteSpace\":\"pre-wrap\",\"wordBreak\":\"break-word\"},\"children\":\"Shift\"}],\" \",[\"$\",\"code\",null,{\"className\":\"whitespace-nowrap\",\"dir\":\"ltr\",\"style\":{\"whiteSpace\":\"pre-wrap\",\"wordBreak\":\"break-word\"},\"children\":\"R\"}],\" on Windows/Linux.\"],\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"$L1fc\",null,{\"id\":\"lm-studio-as-an-mcp-client\",\"node\":{\"type\":\"element\",\"tagName\":\"h2\",\"properties\":{\"id\":\"lm-studio-as-an-mcp-client\"},\"children\":[{\"type\":\"text\",\"value\":\"LM Studio as an MCP client\",\"position\":{\"start\":{\"line\":29,\"column\":4,\"offset\":1152},\"end\":{\"line\":29,\"column\":30,\"offset\":1178}}}],\"position\":{\"start\":{\"line\":29,\"column\":1,\"offset\":1149},\"end\":{\"line\":29,\"column\":30,\"offset\":1178}}},\"children\":\"LM Studio as an MCP client\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"You can install MCP servers in LM Studio and use them with your local models.\",\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"See the docs for more: \",[\"$\",\"$L1fb\",null,{\"href\":\"/docs/app/plugins/mcp\",\"node\":{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"/docs/app/plugins/mcp\"},\"children\":[{\"type\":\"text\",\"value\":\"Use MCP server\",\"position\":{\"start\":{\"line\":33,\"column\":25,\"offset\":1283},\"end\":{\"line\":33,\"column\":39,\"offset\":1297}}}],\"position\":{\"start\":{\"line\":33,\"column\":24,\"offset\":1282},\"end\":{\"line\":33,\"column\":63,\"offset\":1321}}},\"children\":\"Use MCP server\",\"className\":\"\"}],\".\"],\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"If you're develping an MCP server, check out \",[\"$\",\"$L1fb\",null,{\"href\":\"/docs/app/plugins/mcp/deeplink\",\"node\":{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"/docs/app/plugins/mcp/deeplink\"},\"children\":[{\"type\":\"text\",\"value\":\"Add to LM Studio Button\",\"position\":{\"start\":{\"line\":35,\"column\":47,\"offset\":1370},\"end\":{\"line\":35,\"column\":70,\"offset\":1393}}}],\"position\":{\"start\":{\"line\":35,\"column\":46,\"offset\":1369},\"end\":{\"line\":35,\"column\":103,\"offset\":1426}}},\"children\":\"Add to LM Studio Button\",\"className\":\"\"}],\".\"],\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"$L1fc\",null,{\"id\":\"run-an-llm-like-gpt-oss-llama-qwen-mistral-or-deepseek-r1-on-your-computer\",\"node\":{\"type\":\"element\",\"tagName\":\"h2\",\"properties\":{\"id\":\"run-an-llm-like-gpt-oss-llama-qwen-mistral-or-deepseek-r1-on-your-computer\"},\"children\":[{\"type\":\"text\",\"value\":\"Run an LLM like \",\"position\":{\"start\":{\"line\":37,\"column\":4,\"offset\":1432},\"end\":{\"line\":37,\"column\":20,\"offset\":1448}}},{\"type\":\"element\",\"tagName\":\"code\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"gpt-oss\",\"position\":{\"start\":{\"line\":37,\"column\":20,\"offset\":1448},\"end\":{\"line\":37,\"column\":29,\"offset\":1457}}}],\"position\":{\"start\":{\"line\":37,\"column\":20,\"offset\":1448},\"end\":{\"line\":37,\"column\":29,\"offset\":1457}}},{\"type\":\"text\",\"value\":\", \",\"position\":{\"start\":{\"line\":37,\"column\":29,\"offset\":1457},\"end\":{\"line\":37,\"column\":31,\"offset\":1459}}},{\"type\":\"element\",\"tagName\":\"code\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"Llama\",\"position\":{\"start\":{\"line\":37,\"column\":31,\"offset\":1459},\"end\":{\"line\":37,\"column\":38,\"offset\":1466}}}],\"position\":{\"start\":{\"line\":37,\"column\":31,\"offset\":1459},\"end\":{\"line\":37,\"column\":38,\"offset\":1466}}},{\"type\":\"text\",\"value\":\", \",\"position\":{\"start\":{\"line\":37,\"column\":38,\"offset\":1466},\"end\":{\"line\":37,\"column\":40,\"offset\":1468}}},{\"type\":\"element\",\"tagName\":\"code\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"Qwen\",\"position\":{\"start\":{\"line\":37,\"column\":40,\"offset\":1468},\"end\":{\"line\":37,\"column\":46,\"offset\":1474}}}],\"position\":{\"start\":{\"line\":37,\"column\":40,\"offset\":1468},\"end\":{\"line\":37,\"column\":46,\"offset\":1474}}},{\"type\":\"text\",\"value\":\", \",\"position\":{\"start\":{\"line\":37,\"column\":46,\"offset\":1474},\"end\":{\"line\":37,\"column\":48,\"offset\":1476}}},{\"type\":\"element\",\"tagName\":\"code\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"Mistral\",\"position\":{\"start\":{\"line\":37,\"column\":48,\"offset\":1476},\"end\":{\"line\":37,\"column\":57,\"offset\":1485}}}],\"position\":{\"start\":{\"line\":37,\"column\":48,\"offset\":1476},\"end\":{\"line\":37,\"column\":57,\"offset\":1485}}},{\"type\":\"text\",\"value\":\", or \",\"position\":{\"start\":{\"line\":37,\"column\":57,\"offset\":1485},\"end\":{\"line\":37,\"column\":62,\"offset\":1490}}},{\"type\":\"element\",\"tagName\":\"code\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"DeepSeek R1\",\"position\":{\"start\":{\"line\":37,\"column\":62,\"offset\":1490},\"end\":{\"line\":37,\"column\":75,\"offset\":1503}}}],\"position\":{\"start\":{\"line\":37,\"column\":62,\"offset\":1490},\"end\":{\"line\":37,\"column\":75,\"offset\":1503}}},{\"type\":\"text\",\"value\":\" on your computer\",\"position\":{\"start\":{\"line\":37,\"column\":75,\"offset\":1503},\"end\":{\"line\":37,\"column\":92,\"offset\":1520}}}],\"position\":{\"start\":{\"line\":37,\"column\":1,\"offset\":1429},\"end\":{\"line\":37,\"column\":92,\"offset\":1520}}},\"children\":[\"Run an LLM like \",[\"$\",\"code\",null,{\"className\":\"whitespace-nowrap\",\"dir\":\"ltr\",\"style\":{\"whiteSpace\":\"pre-wrap\",\"wordBreak\":\"break-word\"},\"children\":\"gpt-oss\"}],\", \",[\"$\",\"code\",null,{\"className\":\"whitespace-nowrap\",\"dir\":\"ltr\",\"style\":{\"whiteSpace\":\"pre-wrap\",\"wordBreak\":\"break-word\"},\"children\":\"Llama\"}],\", \",[\"$\",\"code\",null,{\"className\":\"whitespace-nowrap\",\"dir\":\"ltr\",\"style\":{\"whiteSpace\":\"pre-wrap\",\"wordBreak\":\"break-word\"},\"children\":\"Qwen\"}],\", \",[\"$\",\"code\",null,{\"className\":\"whitespace-nowrap\",\"dir\":\"ltr\",\"style\":{\"whiteSpace\":\"pre-wrap\",\"wordBreak\":\"break-word\"},\"children\":\"Mistral\"}],\", or \",[\"$\",\"code\",null,{\"className\":\"whitespace-nowrap\",\"dir\":\"ltr\",\"style\":{\"whiteSpace\":\"pre-wrap\",\"wordBreak\":\"break-word\"},\"children\":\"DeepSeek R1\"}],\" on your computer\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"To run an LLM on your computer you first need to download the model weights.\",\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"You can do this right within LM Studio! See \",[\"$\",\"$L1fb\",null,{\"href\":\"app/basics/download-model\",\"node\":{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"app/basics/download-model\"},\"children\":[{\"type\":\"text\",\"value\":\"Download an LLM\",\"position\":{\"start\":{\"line\":41,\"column\":46,\"offset\":1645},\"end\":{\"line\":41,\"column\":61,\"offset\":1660}}}],\"position\":{\"start\":{\"line\":41,\"column\":45,\"offset\":1644},\"end\":{\"line\":41,\"column\":89,\"offset\":1688}}},\"children\":\"Download an LLM\",\"className\":\"\"}],\" for guidance.\"],\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"$L1fc\",null,{\"id\":\"chat-with-documents-entirely-offline-on-your-computer\",\"node\":{\"type\":\"element\",\"tagName\":\"h2\",\"properties\":{\"id\":\"chat-with-documents-entirely-offline-on-your-computer\"},\"children\":[{\"type\":\"text\",\"value\":\"Chat with documents entirely offline on your computer\",\"position\":{\"start\":{\"line\":43,\"column\":4,\"offset\":1707},\"end\":{\"line\":43,\"column\":57,\"offset\":1760}}}],\"position\":{\"start\":{\"line\":43,\"column\":1,\"offset\":1704},\"end\":{\"line\":43,\"column\":57,\"offset\":1760}}},\"children\":\"Chat with documents entirely offline on your computer\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"You can attach documents to your chat messages and interact with them entirely offline, also known as \\\"RAG\\\".\",\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Read more about how to use this feature in the \",[\"$\",\"$L1fb\",null,{\"href\":\"app/basics/rag\",\"node\":{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"app/basics/rag\"},\"children\":[{\"type\":\"text\",\"value\":\"Chat with Documents\",\"position\":{\"start\":{\"line\":47,\"column\":49,\"offset\":1920},\"end\":{\"line\":47,\"column\":68,\"offset\":1939}}}],\"position\":{\"start\":{\"line\":47,\"column\":48,\"offset\":1919},\"end\":{\"line\":47,\"column\":85,\"offset\":1956}}},\"children\":\"Chat with Documents\",\"className\":\"\"}],\" guide.\"],\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"$L1fc\",null,{\"id\":\"use-lm-studios-api-from-your-own-apps-and-scripts\",\"node\":{\"type\":\"element\",\"tagName\":\"h2\",\"properties\":{\"id\":\"use-lm-studios-api-from-your-own-apps-and-scripts\"},\"children\":[{\"type\":\"text\",\"value\":\"Use LM Studio's API from your own apps and scripts\",\"position\":{\"start\":{\"line\":49,\"column\":4,\"offset\":1968},\"end\":{\"line\":49,\"column\":54,\"offset\":2018}}}],\"position\":{\"start\":{\"line\":49,\"column\":1,\"offset\":1965},\"end\":{\"line\":49,\"column\":54,\"offset\":2018}}},\"children\":\"Use LM Studio's API from your own apps and scripts\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"LM Studio provides a REST API that you can use to interact with your local models from your own apps and scripts.\",\"className\":\"text-foreground/90\"}],\"\\n\",[\"$\",\"ul\",null,{\"node\":{\"type\":\"element\",\"tagName\":\"ul\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"\\n\"},{\"type\":\"element\",\"tagName\":\"li\",\"properties\":{},\"children\":[{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"api/openai-api\"},\"children\":[{\"type\":\"text\",\"value\":\"OpenAI Compatibility API\",\"position\":{\"start\":{\"line\":53,\"column\":4,\"offset\":2138},\"end\":{\"line\":53,\"column\":28,\"offset\":2162}}}],\"position\":{\"start\":{\"line\":53,\"column\":3,\"offset\":2137},\"end\":{\"line\":53,\"column\":45,\"offset\":2179}}}],\"position\":{\"start\":{\"line\":53,\"column\":1,\"offset\":2135},\"end\":{\"line\":53,\"column\":45,\"offset\":2179}}},{\"type\":\"text\",\"value\":\"\\n\"},{\"type\":\"element\",\"tagName\":\"li\",\"properties\":{},\"children\":[{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"api/rest-api\"},\"children\":[{\"type\":\"text\",\"value\":\"LM Studio REST API (beta)\",\"position\":{\"start\":{\"line\":54,\"column\":4,\"offset\":2183},\"end\":{\"line\":54,\"column\":29,\"offset\":2208}}}],\"position\":{\"start\":{\"line\":54,\"column\":3,\"offset\":2182},\"end\":{\"line\":54,\"column\":44,\"offset\":2223}}}],\"position\":{\"start\":{\"line\":54,\"column\":1,\"offset\":2180},\"end\":{\"line\":54,\"column\":44,\"offset\":2223}}},{\"type\":\"text\",\"value\":\"\\n\"}],\"position\":{\"start\":{\"line\":53,\"column\":1,\"offset\":2135},\"end\":{\"line\":54,\"column\":44,\"offset\":2223}}},\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[\"$\",\"$L1fb\",null,{\"href\":\"api/openai-api\",\"node\":\"$1fd\",\"children\":\"OpenAI Compatibility API\",\"className\":\"\"}]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[\"$\",\"$L1fb\",null,{\"href\":\"api/rest-api\",\"node\":\"$207\",\"children\":\"LM Studio REST API (beta)\",\"className\":\"\"}]}],\"\\n\"],\"className\":\"list-disc\"}],\"\\n\",[\"$\",\"br\",\"br-0\",{}],\"\\n\",[\"$\",\"$L1fc\",null,{\"id\":\"community\",\"node\":{\"type\":\"element\",\"tagName\":\"h2\",\"properties\":{\"id\":\"community\"},\"children\":[{\"type\":\"text\",\"value\":\"Community\",\"position\":{\"start\":{\"line\":58,\"column\":4,\"offset\":2236},\"end\":{\"line\":58,\"column\":13,\"offset\":2245}}}],\"position\":{\"start\":{\"line\":58,\"column\":1,\"offset\":2233},\"end\":{\"line\":58,\"column\":13,\"offset\":2245}}},\"children\":\"Community\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Join the LM Studio community on \",[\"$\",\"$L1fb\",null,{\"href\":\"https://discord.gg/aPQfnNkxGC\",\"node\":{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"https://discord.gg/aPQfnNkxGC\"},\"children\":[{\"type\":\"text\",\"value\":\"Discord\",\"position\":{\"start\":{\"line\":60,\"column\":34,\"offset\":2280},\"end\":{\"line\":60,\"column\":41,\"offset\":2287}}}],\"position\":{\"start\":{\"line\":60,\"column\":33,\"offset\":2279},\"end\":{\"line\":60,\"column\":73,\"offset\":2319}}},\"children\":\"Discord\",\"className\":\"\"}],\" to ask questions, share knowledge, and get help from other users and the LM Studio team.\"],\"className\":\"text-foreground/90\"}]]}]\n"])</script><script>self.__next_f.push([1,"d:[\"$\",\"div\",null,{\"className\":\"markdown-body text-xs\",\"children\":[\"$\",\"p\",null,{\"children\":[\"This page's source is available on \",[\"$\",\"$L1fb\",null,{\"href\":\"https://github.com/lmstudio-ai/docs/blob/main/0_app/0_root/index.md\",\"node\":{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"https://github.com/lmstudio-ai/docs/blob/main/0_app/0_root/index.md\"},\"children\":[{\"type\":\"text\",\"value\":\"GitHub\",\"position\":{\"start\":{\"line\":1,\"column\":37,\"offset\":36},\"end\":{\"line\":1,\"column\":43,\"offset\":42}}}],\"position\":{\"start\":{\"line\":1,\"column\":36,\"offset\":35},\"end\":{\"line\":1,\"column\":113,\"offset\":112}}},\"children\":\"GitHub\",\"className\":\"\"}]],\"className\":\"text-foreground/90 text-xs\"}]}]\n"])</script><script>self.__next_f.push([1,"1f6:[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://plausible.io\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://static.cloudflareinsights.com\"}],[\"$\",\"script\",null,{\"id\":\"ld-org\",\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Organization\\\",\\\"@id\\\":\\\"https://lmstudio.ai/#organization\\\",\\\"name\\\":\\\"LM Studio\\\",\\\"url\\\":\\\"https://lmstudio.ai\\\",\\\"logo\\\":{\\\"@type\\\":\\\"ImageObject\\\",\\\"url\\\":\\\"https://lmstudio.ai/assets/android-chrome-192x192.png\\\",\\\"width\\\":192,\\\"height\\\":192},\\\"sameAs\\\":[\\\"https://twitter.com/lmstudio\\\",\\\"https://github.com/lmstudio-ai\\\"],\\\"foundingDate\\\":\\\"2023\\\",\\\"contactPoint\\\":[{\\\"@type\\\":\\\"ContactPoint\\\",\\\"contactType\\\":\\\"customer support\\\",\\\"email\\\":\\\"support@lmstudio.ai\\\",\\\"url\\\":\\\"https://lmstudio.ai/support\\\",\\\"availableLanguage\\\":[\\\"en\\\"]}]}\"}}],[\"$\",\"script\",null,{\"id\":\"ld-website\",\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"@id\\\":\\\"https://lmstudio.ai/#website\\\",\\\"url\\\":\\\"https://lmstudio.ai\\\",\\\"name\\\":\\\"LM Studio\\\",\\\"publisher\\\":{\\\"@id\\\":\\\"https://lmstudio.ai/#organization\\\"},\\\"potentialAction\\\":{\\\"@type\\\":\\\"SearchAction\\\",\\\"target\\\":\\\"https://lmstudio.ai/search?keyword={search_term_string}\\\",\\\"query-input\\\":\\\"required name=search_term_string\\\"}}\"}}],[\"$\",\"script\",null,{\"id\":\"ld-software\",\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"SoftwareApplication\\\",\\\"@id\\\":\\\"https://lmstudio.ai/#software\\\",\\\"name\\\":\\\"LM Studio\\\",\\\"applicationCategory\\\":\\\"DeveloperApplication\\\",\\\"operatingSystem\\\":\\\"macOS, Windows, Linux\\\",\\\"downloadUrl\\\":\\\"https://lmstudio.ai/download\\\",\\\"publisher\\\":{\\\"@id\\\":\\\"https://lmstudio.ai/#organization\\\"},\\\"offers\\\":{\\\"@type\\\":\\\"Offer\\\",\\\"price\\\":\\\"0\\\",\\\"priceCurrency\\\":\\\"USD\\\"}}\"}}]]}],[\"$\",\"$L211\",null,{\"src\":\"https://plausible.io/js/script.file-downloads.outbound-links.tagged-events.js\",\"defer\":true,\"strategy\":\"afterInteractive\",\"data-domain\":\"lmstudio.ai\",\"data-file-types\":\"dmg,zip,exe,AppImage\"}],[\"$\",\"$L211\",null,{\"id\":\"plausible-custom-events-script\",\"strategy\":\"afterInteractive\",\"children\":\"\\n          window.plausible = window.plausible || function() { \\n            (window.plausible.q = window.plausible.q || []).push(arguments) \\n          }\\n        \"}],[\"$\",\"body\",null,{\"className\":\"\",\"children\":[\"$\",\"$L212\",null,{\"attribute\":\"class\",\"defaultTheme\":\"system\",\"enableSystem\":true,\"disableTransitionOnChange\":true,\"children\":[\"$\",\"$L213\",null,{\"children\":[\"$\",\"$L214\",null,{\"children\":[\"$\",\"$L215\",null,{\"children\":[[\"$\",\"$L216\",null,{\"children\":[\"$\",\"$L1ee\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L1ef\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"$L217\",null,{}],\"notFoundStyles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/77904bbb8e5c2b67.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]}]}],[\"$\",\"$218\",null,{\"fallback\":null,\"children\":[\"$\",\"$L219\",null,{}]}]]}]}]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"21a:[\"slug\",\"app\",\"c\"]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"TechArticle\\\",\\\"@id\\\":\\\"https://lmstudio.ai/docs/app#article\\\",\\\"mainEntityOfPage\\\":\\\"https://lmstudio.ai/docs/app\\\",\\\"headline\\\":\\\"Welcome to LM Studio Docs!\\\",\\\"description\\\":\\\"Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM Studio.\\\",\\\"image\\\":[\\\"https://lmstudio.ai/api/og?title=Welcome%20to%20LM%20Studio%20Docs!\u0026description=Learn%20how%20to%20run%20Llama%2C%20DeepSeek%2C%20Qwen%2C%20Phi%2C%20and%20other%20LLMs%20locally%20with%20LM%20Studio.\u0026from=docs/app\\\"],\\\"inLanguage\\\":\\\"en\\\",\\\"publisher\\\":{\\\"@id\\\":\\\"https://lmstudio.ai/#organization\\\"}}\"}}],[\"$\",\"div\",null,{\"className\":\"flex w-full flex-row items-center justify-center lg:min-w-0 lg:flex-1\",\"children\":[\"$\",\"$L1ee\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(static)\",\"children\",\"docs\",\"children\",\"$21a\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L1ef\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]}],[\"$\",\"$L21b\",null,{\"headersTree\":[{\"title\":\"What can I do with LM Studio?\",\"depth\":2,\"id\":\"what-can-i-do-with-lm-studio\",\"children\":[]},{\"title\":\"System requirements\",\"depth\":2,\"id\":\"system-requirements\",\"children\":[]},{\"title\":\"Run llama.cpp (GGUF) or MLX models\",\"depth\":2,\"id\":\"run-llamacpp-gguf-or-mlx-models\",\"children\":[]},{\"title\":\"LM Studio as an MCP client\",\"depth\":2,\"id\":\"lm-studio-as-an-mcp-client\",\"children\":[]},{\"title\":\"Run an LLM like gpt-oss, Llama, Qwen, Mistral, or DeepSeek R1 on your computer\",\"depth\":2,\"id\":\"run-an-llm-like-gpt-oss-llama-qwen-mistral-or-deepseek-r1-on-your-computer\",\"children\":[]},{\"title\":\"Chat with documents entirely offline on your computer\",\"depth\":2,\"id\":\"chat-with-documents-entirely-offline-on-your-computer\",\"children\":[]},{\"title\":\"Use LM Studio's API from your own apps and scripts\",\"depth\":2,\"id\":\"use-lm-studios-api-from-your-own-apps-and-scripts\",\"children\":[]},{\"title\":\"Community\",\"depth\":2,\"id\":\"community\",\"children\":[]}],\"pageRelUrl\":\"0_app/0_root/index.md\"}],false]\n"])</script><script>self.__next_f.push([1,"1f7:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Welcome to LM Studio Docs! | LM Studio Docs\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM Studio.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"keywords\",\"content\":\"local ai,local llm,gpt-oss,on-device ai,run local ai,LM Studio,Llama,Gemma,Qwen,DeepSeek,llama.cpp,mlx\"}],[\"$\",\"meta\",\"5\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"link\",\"6\",{\"rel\":\"canonical\",\"href\":\"https://lmstudio.ai/docs/app\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:title\",\"content\":\"Welcome to LM Studio Docs! | LM Studio Docs\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:description\",\"content\":\"Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM Studio.\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:url\",\"content\":\"https://lmstudio.ai/docs/app\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:site_name\",\"content\":\"LM Studio - Docs\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:image\",\"content\":\"https://lmstudio.ai/api/og?title=Welcome%20to%20LM%20Studio%20Docs!\u0026from=docs/app\u0026description=Learn%20how%20to%20run%20Llama,%20DeepSeek,%20Qwen,%20Phi,%20and%20other%20LLMs%20locally%20with%20LM%20Studio.\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:image:type\",\"content\":\"image/png\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:image:height\",\"content\":\"630\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:image:alt\",\"content\":\"LM Studio: Welcome to LM Studio Docs!\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:creator\",\"content\":\"@lmstudio\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:title\",\"content\":\"Welcome to LM Studio Docs! | LM Studio Docs\"}],[\"$\",\"meta\",\"20\",{\"name\":\"twitter:description\",\"content\":\"Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM Studio.\"}],[\"$\",\"meta\",\"21\",{\"name\":\"twitter:image\",\"content\":\"https://lmstudio.ai/api/og?title=Welcome%20to%20LM%20Studio%20Docs!\u0026description=Learn%20how%20to%20run%20Llama,%20DeepSeek,%20Qwen,%20Phi,%20and%20other%20LLMs%20locally%20with%20LM%20Studio.\u0026from=docs/app\"}],[\"$\",\"link\",\"22\",{\"rel\":\"icon\",\"href\":\"/_next/static/media/android-chrome-192x192.3a60873f.png\"}],[\"$\",\"meta\",\"23\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"7:null\n"])</script><!-- Cloudflare Pages Analytics --><script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "e422095eb58144da8265862da0dd8f74"}'></script><!-- Cloudflare Pages Analytics --></body></html>